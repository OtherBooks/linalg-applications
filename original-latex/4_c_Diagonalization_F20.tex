\achapter{19}{Diagonalization} \label{sec:diagonalization}

\vspace*{-17 pt}
\framebox{
\parbox{\dimexpr\linewidth-3\fboxsep-3\fboxrule}
{\begin{fqs}
\item What is a diagonal matrix?
\item What does it mean to diagonalize a matrix?
\item What does it mean for two matrices to be similar?
\item What important properties do similar matrices share?
\item Under what conditions is a matrix diagonalizable?
\item When a matrix $A$ is diagonalizable, what is the structure of a matrix $P$ that diagonalizes $A$?
\item Why is diagonalization useful?
\end{fqs}}}% \hspace*{3 pt}}

\csection{Application: The Fibonacci Numbers}

In 1202 Leonardo of Pisa (better known as Fibonacci) published \emph{Liber Abaci} (roughly translated as \emph{The Book of Calculation}), in which he constructed a mathematical model of the growth of a rabbit population. The problem Fibonacci considered is that of determining the number of pairs of rabbits produced in a given time period beginning with an initial pair of rabbits. Fibonacci made the assumptions that each pair of rabbits more than one month old produces a new pair of rabbits each month, and that no rabbits die. (We ignore any issues about that might arise concerning the gender of the offspring.) If we let $F_n$ represent the number of rabbits in month $n$, Fibonacci produced the model
\begin{equation}
F_{n+2} = F_{n+1} + F_{n}, \label{eq:Fibonacci}
\end{equation}
for $n \geq 0$ where $F_0 = 0$ and $F_1 = 1$. The resulting sequence 
\[1,1,2,3,5,8,13,21, \ldots\]
is a very famous sequence in mathematics and is called the Fibonacci sequence\index{Fibonacci sequence}. This sequence is thought to model many natural phenomena such as number of seeds in a sunflower and anything which grows in a spiral form. It is so famous in fact that it has a journal devoted entirely to it. As a note, while Fibonacci's work \emph{Liber Abaci} introduced this sequence to the western world, it had been described earlier Sanskrit texts going back as early as the sixth century. 

By definition, the Fibonacci numbers are calculated by recursion. This is a vey ineffective way to determine entries $F_n$ for large $n$. Later in this section we will derive a fascinating and unexpected formula for the Fibonacci numbers using the idea of diagonalization. 


\csection{Introduction}

As we have seen when studying Markov processes, each state is dependent on the previous state. If $\vx_0$ is the initial state and $A$ is the transition matrix, then the $n$th state is found by $A^n \vx_0$. In these situations, and others, it is valuable to be able to quickly and easily calculate powers of a matrix. We explore a way to do that in this section. 

\begin{pa} \label{pa:4_c} Consider a very simplified weather forecast. Let us assume there are two possible states for the weather: rainy ($R$) or sunny($S$). Let us also assume that the weather patterns are stable enough that we can reasonably predict the weather tomorrow based on the weather today. If is is sunny today, then there is a 70\% chance that it will be sunny tomorrow, and if it is rainy today then there is a 40\% chance that it will be rainy tomorrow. If $\vx_0 = \left[ \begin{array}{c} s \\ r \end{array} \right]$ is a state vector that indicates a probability $s$ that it is sunny and probability $r$ that it is rainy on day $0$, then 
\[\vx_1 = \left[ \begin{array}{cc} 0.70&0.40 \\ 0.30&0.60 \end{array} \right] \vx_0\]
tells us the likelihood of it being sunny or rainy on day 1. Let $A = \left[ \begin{array}{cc} 0.70&0.40 \\ 0.30&0.60 \end{array} \right]$.  
\be
\item Suppose it is sunny today, that is $\vx_0 = \left[ \begin{array}{c} 1 \\ 0 \end{array} \right]$. Calculate $\vx_1 = A \vx_0$ and explain how this matrix-vector product tells us the probability that it will be sunny tomorrow.

\item Calculate $\vx_2 = A\vx_1$ and interpret the meaning of each component of the product.  

\item  Explain why $\vx_2 = A^2 \vx_0$. Then explain in general why $\vx_n = A^n \vx_0$. 

\item The previous result demonstrates that to determine the long-term probability of a sunny or rainy day, we want to be able to easily calculate powers of the matrix $A$. Use a computer algebra system (e.g., Maple, Mathematica, Wolfram$|$Alpha) to calculate the entries of $\vx_{10}$, $\vx_{20}$, and $\vx_{30}$.  Based on this data, what do you expect the long term probability of any day being a sunny one? 

\ee

\end{pa}


\csection{Diagonalization}


In Preview Activity \ref{pa:4_c} we saw how if we can powers of a matrix we can make predictions about the long-term behavior of some systems. In general, calculating powers of a matrix can be a very difficult thing, but there are times when the process is straightforward. 

\begin{activity} \label{act:4_c_0} Let $D = \left[ \begin{array}{cc} 2&0 \\ 0&3 \end{array} \right]$. 
	\ba
	\item Show that $D^2 = \left[ \begin{array}{cc} 2^2&0 \\ 0&3^2 \end{array} \right]$.

	\item Show that $D^3 = \left[ \begin{array}{cc} 2^3&0 \\ 0&3^3 \end{array} \right]$. (Hint: $D^3 = DD^2$.)

	\item Explain in general why $D^n = \left[ \begin{array}{cc} 2^n&0 \\ 0&3^n \end{array} \right]$ for any positive integer $n$.

	\ea
\end{activity}

Activity \ref{act:4_c_0} illustrates that calculating powers of square matrices whose only nonzero entries are along the diagonal is rather simple. In general, if 
\[D = \left[ \begin{array}{cccccc}
d_{11} 	&0 			&0				&\cdots    	&0		&0 \\
0 			& d_{22} 	&0				&\cdots    	&0		&0 \\
 \vdots  & 0      		&0				&\ddots    	&      &\vdots \\
0 			& 0 			&0    			& \cdots		&0  	&d_{nn}
\end{array} \right],\]
then 
\[D^k = \left[ \begin{array}{cccccc}
d_{11}^k 	&0 			&0				&\cdots    	&0		&0 \\
0 			& d_{22}^k 	&0				&\cdots    	&0		&0 \\
 \vdots  & 0      			&0				&\ddots    	&      &\vdots \\
0 			& 0 				&0    			& \cdots		&0  	&d_{nn}^k
\end{array} \right]\]
for any positive integer $k$. Recall that a diagonal matrix is a matrix whose only nonzero elements are along the diagonal (see Definition \ref{def:special_matrices}).  In this section we will see that matrices that are similar to diagonal matrices have some very nice properties, and that diagonal matrices are useful in calculations of powers of matrices. 

We can utilize the method of calculating powers of diagonal matrices to also easily calculate powers of other types of matrices. 

\begin{activity} \label{act:4_c_0.5} Let $D$ be any matrix, $P$ an invertible matrix, and let $A = P^{-1}DP$. 
	\ba
	\item Show that $A^2 = P^{-1}D^2P$.

	\item Show that $A^3 = P^{-1}D^3P$.

	\item Explain in general why $A^n = P^{-1}D^nP$ for positive integers $n$.

	\ea

\end{activity}

As Activity \ref{act:4_c_0.5} illustrates, to calculate the powers of a matrix of the form $P^{-1}DP$ we only need determine the powers of the matrix $D$. If $D$ is a diagonal matrix, this is especially straightforward. 

 
\csection{Similar Matrices}

Similar matrices play an important role in certain calculations. For example, Activity  \ref{act:4_c_0.5} showed that if we can write a square matrix $A$ in the form $A = P^{-1}DP$ for some invertible matrix $P$ and diagonal matrix $D$, then finding the powers of $A$ is straightforward. As we will see, the relation $A = P^{-1}DP$ will imply that the matrices $A$ and $D$ share many properties. 



\begin{definition} The $n \times n$ matrix $A$ is \textbf{similar}\index{similar matrices} to the $n \times n$ matrix $B$ if there is an invertible matrix $P$ such that $A = P^{-1}BP$.
\end{definition}



\begin{activity} \label{act:4_c_1} Let $A = \left[ \begin{array}{cc} 1&1\\2&0 \end{array} \right]$ and $B = \left[ \begin{array}{cr} 2&2\\0&-1 \end{array} \right]$. Assume that $A$ is similar to $B$ via the matrix $P = \left[ \begin{array}{cc} 2&1\\2&2 \end{array} \right]$.
    \ba
    \item Calculate $\det(A)$ and $\det(B)$. What do you notice?



    \item Find the characteristic polynomials of $A$ and $B$. What do you notice?



    \item What can you say about the eigenvalues of $A$ and $B$? Explain.



    \item Explain why $\vx = \left[ \begin{array}{c} 1\\1 \end{array} \right]$ is an eigenvector for $A$ with eigenvalue 2. Is $\vx$ an eigenvector for $B$ with eigenvalue 2? Why or why not?



    \ea

\end{activity}



Activity \ref{act:4_c_1} suggests that similar matrices share some, but not all, properties. Note that if $A = P^{-1}BP$, then $B = Q^{-1}AQ$ with $Q = P^{-1}$. So if $A$ is similar to $B$, then $B$ is similar to $A$. Similarly (no pun intended), since $A = I^{-1}AI$ (where $I$ is the identity matrix), then any square matrix is similar to itself. Also, if $A = P^{-1}BP$ and $B = M^{-1}CM$, then $A = (MP)^{-1}C(MP)$. So if $A$ is similar to $B$ and $B$ is similar to $C$, then $A$ is similar to $C$. If you have studied relations, these three properties show that similarity is an equivalence relation on the set of all $n \times n$ matrices. This is one reason why similar matrices share many important traits, as the next activity highlights.



\begin{activity} \label{act:4_c_2} Let $A$ and $B$ be similar matrices with $A = P^{-1}BP$.
\ba
\item Use the multiplicative property of the determinant to explain why $\det(A) = \det(B)$. So similar matrices have the same determinants.



%Using the multiplicative property of the determinant we see that
%\[\det(A) = \det(P^{-1}BP) = \det(P^{-1}) \det(B) \det(P) = \det(B).\]

\item Use the fact that $P^{-1}IP = I$ to show that $A-\lambda I$ is similar to $B - \lambda I$.



%\[A - \lambda I = P^{-1}BP - \lambda I = P^{-1}BP - \lambda P^{-1}IP = P^{-1}(B - \lambda I)P.\]
%So $A-\lambda I$ is similar to $B - \lambda I$.

\item Explain why it follows from (a) and (b) that 
\[\det(A - \lambda I) = \det(B - \lambda I).\]
So similar matrices have the same characteristic polynomial, and the same eigenvalues.
%Since similar matrices have the same determinant, it follows that



\ea

\end{activity}



We summarize some properties of similar matrices in the following theorem.



\begin{theorem} Let $A$ and $B$ be similar $n \times n$ matrices and $I$ the $n \times n$ identity matrix. Then
\begin{enumerate}
\item $\det(A) = \det(B)$,
\item $A-\lambda I$ is similar to $B - \lambda I$,
\item $A$ and $B$ have the same characteristic polynomial,
\item $A$ and $B$ have the same eigenvalues.
\end{enumerate}
\end{theorem}

\csection{Similarity and Matrix Transformations}

When a matrix is similar to a diagonal matrix, we can gain insight into the action of the corresponding matrix transformation. As an example,  consider the matrix transformation $T$ from $\R^2$ to $\R^2$ defined by $T(\vx) = A \vx$, where
\begin{equation} \label{eq:4_c_1}
A = \left[ \begin{array}{cc} 3&1\\1&3 \end{array} \right].
\end{equation}
We are interested in understanding what this matrix transformation does to vectors in $\R^2$. First we note that $A$ has eigenvalues $\lambda_1 = 2$ and $\lambda_2 = 4$ with corresponding eigenvectors $\vv_1 = \left[ \begin{array}{r} -1\\1 \end{array} \right]$ and $\vv_2 = \left[ \begin{array}{c} 1\\1 \end{array} \right]$. If we let $P = [\vv_1 \ \vv_2]$, then you can check that 
\[P^{-1}AP = D\]
and
\[A = PDP^{-1},\]
where 
\[D = \left[ \begin{array}{cc} 2 & 0 \\ 0 & 4 \end{array} \right].\]
Thus,
\[T(\vx) = PDP^{-1}\vx.\]

A simple calculation shows that 
\[P^{-1} = \frac{1}{2}\left[ \begin{array}{rc} -1&1 \\ 1&1 \end{array} \right].\]
Let us apply $T$ to the unit square whose sides are formed by the vectors $\ve_1 = \left[ \begin{array}{c} 1 \\ 0 \end{array} \right]$ and $\ve_2 = \left[ \begin{array}{c} 0 \\ 1 \end{array} \right]$ as shown in the first picture in Figure \ref{F:4_c_1}. 

To apply $T$ we first multiply $\ve_1$ and $\ve_2$ by $P^{-1}$. This gives us 
\[P^{-1}\ve_1 = \frac{1}{2}\left[ \begin{array}{r} -1\\1 \end{array} \right] \text{ and } \ P^{-1}\ve_2 = \frac{1}{2}\left[ \begin{array}{c} 1\\1 \end{array} \right]  .\]
So $P^{-1}$ transforms the standard coordinate system into a coordinate system in which columns of $P^{-1}$ determine the axes, as illustrated in the second picture in Figure \ref{F:4_c_1}. Applying $D$ to the output scales by 2 in the first component and by $4$ in the second component as depicted in the third picture in Figure \ref{F:4_c_1}. Finally, we apply $P$ to translate back into the standard $xy$ coordinate system as shown in the last picture in Figure \ref{F:4_c_1}.  In this case, we can visualize that when we apply the transformation $T$ to a vector in this system it is just scaled in the $P^{-1}\ve_1-P^{-1}\ve_2$ system  by the matrix $D$. Then the matrix $P$ translates everything back to the standard $xy$ coordinate system. 
%\begin{figure}[ht]
%\begin{center}
%\begin{tabular}{ccc}
%\resizebox{!}{2.5in}{\includegraphics{4_c_diag}} 
%\resizebox{!}{1.5in}{\includegraphics{4_c_Transformation_1}} &\hspace{0.1in}  &\resizebox{!}{1.5in}{\includegraphics{4_c_Transformation_2}} \\
%\resizebox{!}{1.5in}{\includegraphics{4_c_Transformation_3}} &\hspace{0.1in} &\resizebox{!}{1.5in}{\includegraphics{4_c_Transformation_4}} 
%\end{tabular}
%\end{center}
%\caption{The matrix transformation.}
%\label{F:4_c_1}
%\end{figure}

\begin{figure}[ht]
\begin{center}
%\begin{tabular}{ccc}
%\resizebox{!}{2.5in}{\includegraphics{4_c_diag}} 
\resizebox{!}{1.25in}{\includegraphics{4_c_Transformation_1}} \hspace{0.1in}  \resizebox{!}{1.25in}{\includegraphics{4_c_Transformation_2}} 
\resizebox{!}{1.25in}{\includegraphics{4_c_Transformation_3}} \hspace{0.1in} \resizebox{!}{1.25in}{\includegraphics{4_c_Transformation_4}} 
%\end{tabular}
\end{center}
\caption{The matrix transformation.}
\label{F:4_c_1}
\end{figure}

This geometric perspective provides another example of how having a matrix similar to a diagonal matrix informs us about the situation. In what follows we determine the conditions that determine when a matrix is similar to a diagonal matrix. 

\csection{Diagonalization in General}

In Preview Activity \ref{pa:4_c} and in the matrix transformation example we found that a matrix $A$ was similar to a diagonal matrix whose columns were eigenvectors of $A$. This will work for a general $n \times n$ matrix $A$ as long as we can find an invertible matrix $P$ whose columns are eigenvectors of $A$. More specifically, suppose $A$ is an $n \times n$ matrix with $n$ linearly independent eigenvectors $\vv_1$, $\vv_1$, $\ldots$, $\vv_n$ with corresponding eigenvalues $\lambda_1$, $\lambda_1$, $\ldots$, $\lambda_n$ (not necessarily distinct). Let
\[P = [ \vv_1 \  \vv_2  \ \vv_3  \ \cdots  \ \vv_n].\]
Then
\begin{align*}
AP &= [ A\vv_1  \ A\vv_2  \ A\vv_3  \ \cdots  \ A\vv_n] \\
    &= [ \lambda_1\vv_1  \ \lambda_2\vv_2  \ \lambda_3\vv_3  \ \cdots  \ \lambda_n\vv_n] \\
    &= [ \vv_1 \  \vv_2  \ \vv_3  \ \cdots  \ \vv_n]\left[ \begin{array}{cccccc} \lambda_1&0&0&\cdots&0&0 \\ 0&\lambda_2&0&\cdots&0&0 \\ \vdots&\vdots&& \cdots&\vdots&\vdots \\ 0&0&0&\cdots&\lambda_{n-1}&0 \\  0&0&0&\cdots&0&\lambda_{n} \end{array} \right] \\
    &= P D.
\end{align*}
where 
\[D = \left[ \begin{array}{cccccc} \lambda_1&0&0&\cdots&0&0 \\ 0&\lambda_2&0&\cdots&0&0 \\ \vdots&\vdots&& \cdots&\vdots&\vdots \\ 0&0&0&\cdots&\lambda_{n-1}&0 \\  0&0&0&\cdots&0&\lambda_{n} \end{array} \right].\]
Since the columns of $P$ are linearly independent, we know $P$ is invertible, and so
\[P^{-1}AP = D.\]



\begin{definition} An $n \times n$ matrix $A$ is \textbf{diagonalizable}\index{matrix!diagonalizable} if there is an invertible $n \times n$ matrix $P$ so that $P^{-1}AP$ is a diagonal matrix.
\end{definition}



In other words, a matrix $A$ is diagonalizable if $A$ is similar to a diagonal matrix. 



\noindent \textbf{IMPORTANT NOTE:} The key notion to the process described above is that in order to diagonalize an $n\times n$ matrix $A$, we have to find $n$ linearly independent eigenvectors for $A$. When $A$ is diagonalizable, a matrix $P$ so that $P^{-1}AP$ is diagonal is said to \emph{diagonalize} $A$.



\begin{activity} \label{act:4_c_3} Find an invertible matrix $P$ that diagonalizes $A$.
    \ba
    \item $A = \left[ \begin{array}{cc} 1&1 \\ 0&2 \end{array} \right]$

    

    \item $A = \left[ \begin{array}{ccc} 3&2&4 \\ 2&0&2 \\ 4&2&3 \end{array} \right]$. (Hint: The eigenvalues of $A$ are 8 and $-1$.)

    

    \ea
\end{activity}


It should be noted that there are square matrices that are not diagonalizable. For example, the matrix $A = \left[ \begin{array}{cc} 1&1 \\ 0&1 \end{array} \right]$ has 1 as its only eigenvalue and the dimension of the eigenspace of $A$ corresponding to the eigenvalue is one. Therefore, it will be impossible to find two linearly independent eigenvectors for $A$.

We showed previously that eigenvectors corresponding to distinct eigenvalue are always linearly independent, so if an $n \times n$ matrix $A$ has $n$ distinct eigenvalues then $A$ is diagonalizable. Activity \ref{act:4_c_3} (b) shows that it is possible to diagonalize an $n \times n$ matrix even if the matrix does not have $n$ distinct eigenvalues. In general, we can diagonalize a matrix as long as the dimension of each eigenspace is equal to the multiplicity of the corresponding eigenvalue. In other words, a matrix is diagonalizable if the geometric multiplicity is the same is the algebraic multiplicity for each eigenvalue. 

At this point we might ask one final question. We argued that if an $n \times n$ matrix $A$ has $n$ linearly independent eigenvectors, then $A$ is diagonalizable. It is reasonable to wonder if the converse is true -- that is, if $A$ is diagonalizable, must $A$ have $n$ linearly independent eigenvectors? The answer is yes, and you are asked to show this in Exercise \ref{ex:4_c_diagonal_converse}. We summarize the result in the following theorem.

\begin{theorem}[The Diagonalization Theorem] An $n \times n$ matrix $A$ is diagonalizable if and only if $A$ has $n$ linearly independent eigenvectors. If $A$ is diagonalizable and has linearly independent eigenvectors $\vv_1$, $\vv_2$, $\ldots$, $\vv_n$ with $A\vv_i = \lambda_i \vv_i$ for each $i$, then $n \times n$ matrix $P [ \vv_1 \ \vv_2 \ \cdots \ \vv_n]$ whose columns are linearly independent eigenvectors of $A$ satisfies $P^{-1}AP = D$, where $D = [d_{ij}]$ is the diagonal matrix  with diagonal entries $d_{ii} = \lambda_i$ for each $i$.
\end{theorem}


\csection{Examples}

\ExampleIntro

\begin{example} Let $A = \left[ \begin{array}{crr} 1&-2&1 \\ 0&3&-1 \\ 0&-2&2 \end{array} \right]$ and $B = \left[ \begin{array}{ccc} 1&2&0 \\ 0&1&0 \\ 0&0&4 \end{array} \right]$. You should use appropriate technology to calculate determinants, perform any row reductions, or solve any polynomial equations. 
\ba
\item Determine if $A$ is diagonalizable. If diagonalizable, find a matrix $P$ that diagonalizes $A$.

\item Determine if $B$ is diagonalizable. If diagonalizable, find a matrix $Q$ that diagonalizes $B$.

\item Is it possible for two matrices $R$ and $S$ to have the same eigenvalues with the same algebraic multiplicities, but one matrix is diagonalizable and the other is not? Explain.

\ea

\ExampleSolution
\ba
\item Technology shows that the characteristic polynomial of $A$ is
\[p(\lambda) = \det(A - \lambda I_3) = (4-\lambda)(1-\lambda)^2.\]
The eigenvalues of $A$ are the solutions to the characteristic equation $p(\lambda) = 0$. Thus, the eigenvalues of $A$ are $1$ and $4$. 

To find a basis for the eigenspace of $A$ corresponding to the eigenvalue $1$, we find the general solution to the homogeneous system $(A - I_3)\vx = \vzero$. Using technology we see that the reduced row echelon form of $A - I_3 =  \left[ \begin{array}{crr} 0&-2&1 \\ 0&2&-1 \\ 0&-2&1 \end{array} \right]$ is $\left[ \renewcommand{\arraystretch}{1.4} \begin{array}{ccr} 0&1&-\frac{1}{2} \\ 0&0&0 \\ 0&0&0 \end{array} \right]$. So if $\vx = \left[ \begin{array}{c} x_1 \\ x_2 \\ x_3 \end{array} \right]$, then the general solution to $(A-I_3)\vx = \vzero$ is
\begin{align*}
\vx &= \left[ \renewcommand{\arraystretch}{1.3}  \begin{array}{c} x_1 \\ \frac{1}{2}x_3 \\ x_3 \end{array} \right]\\
	&= x_1 \left[\begin{array}{c}  1 \\ 0 \\ 0 \end{array} \right] + x_3 \left[ \renewcommand{\arraystretch}{1.3}  \begin{array}{c} 0 \\ \frac{1}{2} \\ 1 \end{array} \right].
\end{align*}
So a basis for the eigenspace of $A$ corresponding to the eigenvalue $1$ is 
\[\left\{ \left[ 1 \ 0 \ 0 \right]^{\tr}, \left[  0 \ 1 \ 2 \right]^{\tr} \right\}.\]

To find a basis for the eigenspace of $A$ corresponding to the eigenvalue $4$, we find the general solution to the homogeneous system $(A - 4I_3)\vx = \vzero$. Using technology we see that the reduced row echelon form of $A - 4I_3 =  \left[ \begin{array}{rrr} -3&-2&1 \\ 0&-1&-1 \\ 0&-2&-2 \end{array} \right]$ is $\left[ \begin{array}{ccr} 0&1&-1 \\ 0&1&1 \\ 0&0&0 \end{array} \right]$. So if $\vx = \left[ \begin{array}{c} x_1 \\ x_2 \\ x_3 \end{array} \right]$, then the general solution to $(A-4I_3)\vx = \vzero$ is
\begin{align*}
\vx &= \left[ x_1 \ x_2 \ x_3 \right]^{\tr} \\
	&= \left[ x_3 \ -x_3 \ x_3 \right]^{\tr} \\
	&= x_3 \left[  1\ -1 \ 1 \right]^{\tr}.
\end{align*}
So a basis for the eigenspace of $A$ corresponding to the eigenvalue $4$ is 
\[\left\{ \left[ 1 \ -1 \ 0 \right]^{\tr}\right\}.\]
Eigenvectors corresponding to different eigenvalues are linearly independent, so the set 
\[\left\{ \left[ 1 \ 0 \ 0 \right]^{\tr}, \left[  0 \ 1 \ 2  \right]^{\tr},  \left[ 1 \ -1 \ 0 \right]^{\tr} \right\}\]
is a basis for $\R^3$. Since we can find a basis for $\R^3$ consisting of eigenvectors of $A$, we conclude that $A$ is diagonalizable. Letting 
\[P =  \left[ \begin{array}{ccr} 1&0&1 \\ 0&1&-1 \\ 0&2&1 \end{array} \right]\]
gives us
\[P^{-1}AP = \left[ \begin{array}{ccc} 1&0&0 \\ 0&1&0 \\ 0&0&4 \end{array} \right]\]


\item Technology shows that the characteristic polynomial of $B$ is
\[p(\lambda) = \det(B - \lambda I_3) = (4-\lambda)(1-\lambda)^2.\]
The eigenvalues of $B$ are the solutions to the characteristic equation $p(\lambda) = 0$. Thus, the eigenvalues of $B$ are $1$ and $4$. 

To find a basis for the eigenspace of $B$ corresponding to the eigenvalue $1$, we find the general solution to the homogeneous system $(B - I_3)\vx = \vzero$. Using technology we see that the reduced row echelon form of $B - I_3 =  \left[ \begin{array}{ccc} 0&2&0 \\ 0&0&0 \\ 0&0&3 \end{array} \right]$ is $\left[ \begin{array}{ccc} 0&1&0 \\ 0&0&1 \\ 0&0&0 \end{array} \right]$. So if $\vx = \left[ \begin{array}{c} x_1 \\ x_2 \\ x_3 \end{array} \right]$, then the general solution to $(B-I_3)\vx = \vzero$ is
\begin{align*}
\vx &= \left[ x_1 \ x_2 \ x_3\right]^{\tr} \\
	&= \left[  x_1 \ 0 \ 0  \right]^{\tr} \\
	&= x_1 \left[ 1 \ 0 \ 0 \right]^{\tr}.
\end{align*}
So a basis for the eigenspace of $B$ corresponding to the eigenvalue $1$ is 
\[\left\{ \left[ 1 \ 0 \ 0 \right]^{\tr}\right\}.\]

To find a basis for the eigenspace of $B$ corresponding to the eigenvalue $4$, we find the general solution to the homogeneous system $(B - 4I_3)\vx = \vzero$. Using technology we see that the reduced row echelon form of $B - 4I_3 =  \left[ \begin{array}{rrc} -3&2&0 \\ 0&-3&0 \\ 0&0&0 \end{array} \right]$ is $\left[ \begin{array}{ccc} 1&0&0 \\ 0&1&0 \\ 0&0&0 \end{array} \right]$. So if $\vx = \left[ \begin{array}{c} x_1 \\ x_2 \\ x_3 \end{array} \right]$, then the general solution to $(B-4I_3)\vx = \vzero$ is
\begin{align*}
\vx &= \left[ x_1 \ x_2 \ x_3  \right]^{\tr} \\
	&= \left[  0 \ 0 \ x_3  \right]^{\tr} \\
	&= x_3 \left[  0\ 0 \ 1  \right]^{\tr}.
\end{align*}
So a basis for the eigenspace of $B$ corresponding to the eigenvalue $4$ is 
\[\left\{ \left[ 0 \ 0 \ 1  \right]^{\tr}\right\}.\]
Since each eigenspace is one-dimensional, we cannot find a basis for $\R^3$ consisting of eigenvectors of $B$. We conclude that $B$ is not diagonalizable. 


\item Yes it is possible for two matrices $R$ and $S$ to have the same eigenvalues with the same multiplicities, but one matrix is diagonalizable and the other is not. An example is given by the matrices $A$ and $B$ in this problem. 

\ea

\end{example}


\begin{example} ~
\ba
\item Is it possible to find diagonalizable matrices $A$ and $B$ such that $AB$ is not diagonalizable? If yes, provide an example. If no, explain why.

\item Is it possible to find diagonalizable matrices $A$ and $B$ such that $A+B$ is not diagonalizable? If yes, provide an example. If no, explain why.

\item Is it possible to find a diagonalizable matrix $A$ such that $A^{\tr}$ is not diagonalizable? If yes, provide an example. If no, explain why.

\item Is it possible to find an invertible diagonalizable matrix $A$ such that $A^{-1}$ is not diagonalizable? If yes, provide an example. If no, explain why.

\ea

\ExampleSolution
\ba
\item Let $A = \left[ \begin{array}{cc} 1&1\\0&2 \end{array} \right]$ and $B = \left[ \begin{array}{cr} 2&-2 \\ 0&1 \end{array} \right]$. Since $A$ and $B$ are both diagonal matrices, their eigenvalues are their diagonal entries. With $2$ distinct eigenvalues, both $A$ and $B$ are diagonalizable. In this case we have $AB = \left[ \begin{array}{cr} 2&-1\\0&2 \end{array} \right]$, whose only eigenvector is $2$. The reduced row echelon form of $AB - 2I_2$ is $\left[ \begin{array}{cr} 0&1\\0&0 \end{array} \right]$. So a basis for the eigenspace of $AB$ is $\{[1 \ 0]^{\tr}\}$. Since there is no basis for $\R^2$ consisting of eigenvectors of $AB$, we conclude that $AB$ is not diagonalizable.

\item Let $A = \left[ \begin{array}{cc} 1&3\\0&2 \end{array} \right]$ and $B = \left[ \begin{array}{rr} 2&0 \\ 0&1 \end{array} \right]$. Since $A$ and $B$ are both diagonal matrices, their eigenvalues are their diagonal entries. With $2$ distinct eigenvalues, both $A$ and $B$ are diagonalizable. In this case we have $A+B = \left[ \begin{array}{cc} 3&3\\0&3 \end{array} \right]$, whose only eigenvector is $3$. The reduced row echelon form of $(A+B) - 3I_2$ is $\left[ \begin{array}{cc} 0&1\\0&0 \end{array} \right]$. So a basis for the eigenspace of $A+B$ is $\{[1 \ 0]^{\tr}\}$. Since there is no basis for $\R^2$ consisting of eigenvectors of $A+B$, we conclude that $A+B$ is not diagonalizable.

\item It is not possible to find a diagonalizable matrix $A$ such that $A^{\tr}$ is not diagonalizable. To see why, suppose that matrix $A$ is diagonalizable. That is, there exists a matrix $P$ such that $P^{-1}AP = D$, where $D$ is a diagonal matrix. Recall that $\left(P^{-1}\right)^{\tr} = \left(P^{\tr}\right)^{-1}$. So 
\begin{align*}
D &= D^{\tr} \\
	&= \left(P^{-1}AP\right)^{\tr} \\
	&= P^{\tr}A^{\tr}\left(P^{-1}\right)^{\tr}  \\
	&= P^{\tr}A^{\tr}\left(P^{\tr}\right)^{-1}.
\end{align*}
Letting $A = \left(P^{\tr}\right)^{-1}$, we conclude that 
\[Q^{-1}A^{\tr}Q = D.\]
Therefore, $Q$ diagonalizes $A^{\tr}$. 

\item It is not possible to find an invertible diagonalizable matrix $A$ such that $A^{-1}$ is not diagonalizable. To see why, suppose that matrix $A$ is diagonalizable. That is, there exists a matrix $P$ such that $P^{-1}AP = D$, where $D$ is a diagonal matrix. Thus, $A = PDP^{-1}$. Since $A$ is invertible, $\det(A) \neq 0$. It follows that $\det(D) \neq 0$. So none of the diagonal entries of $D$ can be $0$. Thus, $D$ is invertible and $D^{-1}$ is a diagonal matrix. Then
\[D^{-1} = \left(P^{-1}AP\right)^{-1} = PA^{-1}P^{-1}\]
and so $P^{-1}$ diagonalizes $A^{-1}$. 
\ea

\end{example}

\csection{Summary}
\begin{itemize}
\item A matrix $D = [d_{ij}]$ is a diagonal matrix if $d_{ij} = 0$ whenever $i \neq j$.
\item A matrix $A$ is diagonalizable if there is an invertible matrix $P$ so that $P^{-1}AP$ is a diagonal matrix.
\item Two matrices $A$ and $B$ are similar if there is an invertible matrix $P$ so that 
\[B = P^{-1}AP.\]
\item Similar matrices have the same determinants, same characteristic polynomials, and same eigenvalues. Note that similar matrices do not necessarily have the same eigenvectors corresponding to the same eigenvalues. 
\item An $n \times n$ matrix $A$ is diagonalizable if and only if $A$ has $n$ linearly independent eigenvectors.
\item When an $n \times n$ matrix $A$ is diagonalizable, then $P = [ \vv_1  \ \vv_2  \ \vv_3  \ \cdots  \ \vv_n]$ is invertible and $P^{-1}AP$ is diagonal, where $\vv_1$, $\vv_2$, $\ldots$, $\vv_n$ are $n$ linearly independent eigenvectors for $A$.
\item One use for diagonalization is that once we have diagonalized a matrix $A$ we can quickly and easily compute powers of $A$. Diagonalization can also help us understand the actions of matrix transformations. 
\end{itemize}



\csection{Exercises}
\be
\item Determine if each of the following matrices is diagonalizable or not. For diagonalizable matrices, clearly identify a matrix $P$ which diagonalizes the matrix, and what the resulting diagonal matrix is.
\ba
\item $A=\left[ \begin{array}{cr} 2&-1\\ 1&4 \end{array} \right]$

\item $A=\left[ \begin{array}{rcr} -1 & 4 & -2 \\ -3 & 4 & 0 \\ -3 & 1 & 3 \end{array} \right]$
\ea

\item The $3\times 3$ matrix $A$ has two eigenvalues $\lambda_1=2$ and $\lambda_2=3$. The vectors $\left[ \begin{array}{c} 1\\2\\1 \end{array} \right]$, $\left[ \begin{array}{r} 1\\-1\\2 \end{array} \right]$, and $\left[ \begin{array}{c} 2\\4\\2\end{array} \right]$ are eigenvectors for $\lambda_1=2$, while the vectors $\left[ \begin{array}{c} 1\\1\\1\end{array} \right], \left[ \begin{array}{c} 2\\2\\2\end{array} \right]$ are eigenvectors for $\lambda_2=3$. Find the matrix $A$.

\item Find a $2\times 2$ non-diagonal matrix $A$ and two different pairs of $P$ and $D$ matrices for which $A=PDP^{-1}$.

\item Find a $2\times 2$ non-diagonal matrix $A$ and two different $P$ matrices for which $A=PDP^{-1}$ with the same $D$.

\item Suppose a $4\times 4$ matrix $A$ has eigenvalues 2, 3 and 5 and the eigenspace for the eigenvalue 3 has dimension 2. Do we have enough information to determine if $A$ is diagonalizable? Explain.

\item \label{ex:4_c_diagonal_converse} Let $A$ be a diagonalizable $n \times n$ matrix. Show that $A$ has $n$ linearly independent eigenvectors. 

\item ~
	\ba
	\item Let $A = \left[ \begin{array}{cc} 1&1\\0&1 \end{array} \right]$ and $B = \left[ \begin{array}{cc} 1&2\\0&1 \end{array} \right]$. Find the eigenvalues and eigenvectors of $A$ and $B$. Conclude that it is possible for two different $n \times n$ matrices $A$ and $B$ to have exactly the same eigenvectors and corresponding eigenvalues. 

	\item A natural question to ask is if there are any conditions under which $n \times n$ matrices that have exactly the same eigenvectors and corresponding eigenvalues must be equal. Determine the answer to this question if $A$ and $B$ are both diagonalizable. 

	\ea

\item ~
	\ba
	\item Show that if $D$ and $D'$ are $n \times n$ diagonal matrices, then $DD' = D'D$. 

	\item Show that if $A$ and $B$ are $n \times n$ matrices and $P$ is an invertible $n \times n$ matrix such that $P^{-1}AP = D$ and $P^{-1}BP = D'$ with $D$ and $D'$ diagonal matrices, then $AB = BA$. 

	\ea


\item \label{ex:trace_eigenvalues} Exercise \ref{ex:determinant_eigenvalues} in Section \ref{sec:characteristic_equation} shows that the determinant of a matrix is the product of its eigenvalues. In this exercise we show that the trace of a diagonalizable matrix is the sum of its eigenvalues.\footnote{This result is true for any matrix, but the argument is more complicated.} First we define the trace of a matrix. 

\begin{definition} \label{def:trace} The \textbf{trace}\index{trace} of an $n \times n$ matrix $A = [a_{ij}]$ is the sum of the diagonal entries of $A$. That is,
\[\trace(A) = a_{11} + a_{22} + \cdots + a_{nn} = \sum_{i=1}^n a_{ii}.\]
\end{definition}

\ba
\item Show that if $R = [r_{ij}]$ and $S = [s_{ij}]$ are $n \times n$ matrices, then $\trace(RS) = \trace(SR)$. 

\item Let $A$ be a diagonalizable $n \times n$ matrix, and let $p(\lambda) = \det(A - \lambda I_n)$ be the characteristic polynomial of $A$. Let $P$ be an invertible matrix such that $P^{-1}AP = D$, where $D$ is the diagonal matrix whose diagonal entries are $\lambda_1$, $\lambda_2$, $\ldots$, $\lambda_n$,  the eigenvalues of $A$ (note that these eigenvalues may not all be distinct). 
	\begin{enumerate}[i.]
	\item Explain why $\trace(A) = \trace(D)$. 
	
	\item Show that the trace of an $n \times n$ diagonalizable matrix is the sum of the eigenvalues of the matrix. 
	\end{enumerate}
	
\ea

\item  \label{ex:4_c_matrix_exponential} In this exercise we generalize the result of Exercise \ref{ex:2_a_matrix_exponential} in Section \ref{sec:matrix_operations} to arbitrary diagonalizable matrices. 
\ba
\item Show that if 
\[D=\left[ \begin{array}{ccccc} \lambda_1& 0 &0& \cdots &0 \\ 0 & \lambda_2& 0 &\cdots & 0 \\ \vdots &\vdots &\vdots & \ddots & \vdots \\ 0 & 0& 0 &\cdots & \lambda_n \end{array} \right],\]
then
\[e^D =  \left[ \begin{array}{ccccc} e^{\lambda_1}& 0 &0& \cdots &0 \\ 0 & e^{\lambda_2}& 0 &\cdots & 0 \\ \vdots &\vdots &\vdots & \ddots & \vdots \\ 0 & 0& 0 &\cdots & e^{\lambda_n} \end{array} \right].\]

\item Now suppose that an $n \times n$ matrix $A$ is diagonalizable, with $P^{-1}AP$ equal to a diagonal matrix $D$. Show that $e^A = Pe^DP^{-1}$. 

\ea

\item  \label{ex:4_c_matrix_exponential_examples} Let $A = \left[ \begin{array}{cc} 1&1\\0&0 \end{array} \right]$ and let $B = \left[ \begin{array}{cr} 0&-1 \\ 0&0 \end{array} \right]$.
\ba
\item Use the result of Exercise \ref{ex:4_c_matrix_exponential} to calculate $e^A$.

\item Calculate $e^B$. (Hint: Explain why $B$ is not diagonalizable.)

\item Use the result of Exercise \ref{ex:4_c_matrix_exponential} to calculate $e^{A+B}$.

\item The real exponential function satisfies some familiar properties. For example, $e^xe^y = e^ye^x$ and $e^{x+y} = e^x e^y$ for any real numbers $x$ and $y$. Does the matrix exponential satisfy the corresponding properties. That is, if $X$ and $Y$ are $n \times n$ matrices, must $e^Xe^Y = e^Ye^X$ and $e^{X+Y} = e^X e^Y$? Explain.

\ea

\item In Exercise \ref{ex:4_c_matrix_exponential_examples} we see that we cannot conclude that $e^{X+Y} = e^X e^Y$ for $n \times n$ matrices $X$ and $Y$. However, a more limited property is true. 
\ba
\item  Follow the steps indicated to show that if $A$ is an $n \times n$ matrix and $s$ and $t$ are any scalars, then $e^{As} e^{At} = e^{A(s+t)}$. (Although we will not use it, you may assume that the series for $e^A$ converges for any square matrix $A$.) 
	\begin{enumerate}[i.]
	\item Use the definition to show that 
	\[e^{As}e^{At} = \sum_{k \geq 0} \sum_{m \geq 0} \frac{s^kt^m}{k!}m! A^{k+m}.\]
	\item Relabel and reorder terms with $n = k+m$ to show that
	\[e^{As}e^{At} = sum_{n \geq 0} \frac{1}{n!} A^n \sum_{m = 0}^n \frac{n!}{(n-m)!m!} s^{n-m}t^m.\]
	\item Complete the problem using the Binomial Theorem that says \[(s+t)^n =  \sum_{m = 0}^n \frac{n!} {(n-m)!m!} s^{n-m}t^m.\]
	\end{enumerate}

\item Use the result of part (a) to show that $e^A$ is an invertible matrix for any $n \times n$ matrix $A$. 

\ea


\item There is an interesting connection between the determinant of a matrix exponential and the trace of the matrix. 
Let $A$ be a diagonalizable $n \times n$ matrix with real entries. Let $D = P^{-1}AP$ for some invertible matrix $P$, where $D$ is the diagonal matrix with entries $\lambda_1$, $\lambda_2$, $\ldots$, $\lambda_n$ the eigenvalues of $A$.
\ba
\item  Show that $e^A = Pe^DP^{-1}$.

\item Use Exercise \ref{ex:trace_eigenvalues} to show that 
\[\det\left(e^A\right) = e^{\trace(A)}.\]

\ea

\item \label{ex:Cayley-Hamilton} There is interesting relationship between a matrix and its characteristic equation that we explore in this exercise.  
	\ba
	\item We first illustrate with an example. Let $B = \left[ \begin{array}{cr} 1&2\\ 1&-2 \end{array} \right]$.
		\begin{enumerate}[i.]
		\item Show that $\lambda^2 + \lambda - 4$ is the characteristic polynomial for $B$.
		\item Calculate $B^2$. Then compute $B^2 + B - 4I_2$. What do you get?
		\end{enumerate}
	\item The first part of this exercise presents an example of a matrix that satisfies its own characteristic equation. Show that if $A$ is an $n \times n$ \emph{diagonalizable} matrix with characteristic polynomial $p(x)$, then $p(A) = 0$.\footnote{This result is known as the Cayley-Hamilton Theorem and is one of the fascinating results in linear algebra. This result is true for any square matrix.} That is, if $p(x) = a_nx^n+a_{n-1}x^{n-1} + \cdots + a_1x + a_0$, then $p(A) = a_nA^n+a_{n-1}A^{n-1} + \cdots + a_1A + a_0 = 0$. (Hint: If $A = PDP^{-1}$ for some diagonal matrix $D$, show that $p(A) = Pp(D)P^{-1}$. Then determine $p(D)$.)
	\ea

\item Label each of the following statements as True or False. Provide justification for your response.
\ba
\item \textbf{True/False} If matrix $A$ is diagonalizable, then so is $A^T$.

\item \textbf{True/False} If matrix $A$ is diagonalizable, then $A$ is invertible.

\item \textbf{True/False} If an $n\times n$ matrix $A$ is diagonalizable, then $A$ has $n$ distinct eigenvalues.

\item \textbf{True/False} If matrix $A$ is invertible and diagonalizable, then so is $A^{-1}$.

\item \textbf{True/False} If an $n \times n$ matrix $C$ is diagonalizable, then there exists a basis of $\R^n$ consisting of the eigenvectors of $C$.

\item \textbf{True/False} An $n\times n$ matrix with $n$ distinct eigenvalues is diagonalizable.

\item \textbf{True/False} If $A$ is an $n\times n$ diagonalizable matrix, then there is a unique diagonal matrix such that $P^{-1}AP = D$ for some invertible matrix $P$.  

\item \textbf{True/False} If $A$ is an $n\times n$ matrix with eigenvalue $\lambda$, then the dimension of the eigenspace of $A$ corresponding to the eigenvalue $\lambda$ is $n - \rank(A - \lambda I_n)$. 

\item \textbf{True/False} If $\lambda$ is an eigenvalue of an $n \times n$ matrix $A$, then $e^\lambda$ is an eigenvalue of $e^A$. (See Exercise  \ref{ex:2_a_matrix_exponential} in Section \ref{sec:matrix_operations} for information on the matrix exponential.)

\ea
\ee

\csection{Project: Binet's Formula for the Fibonacci Numbers}

We return to the Fibonacci sequence $F_n$ where $F_{n+2} = F_{n+1} + F_{n}$, for $n \geq 0$, $F_0 = 0$, and $F_1=1$. Since $F_{n+2}$ is determined by previous values $F_{n+1}$ and $F_n$, the relation $F_{n+2} = F_{n+1} + F_{n}$ is called a \emph{recurrence relation}. The recurrence relation $F_{n+2} = F_{n+1} + F_{n}$ is very time consuming to use to compute $F_n$ for large values of $n$. It turns out that there is a fascinating formula that gives the $n$th term of the Fibonacci sequence directly, without using the relation $F_{n+2} = F_{n+1} + F_{n}$.


\begin{pactivity} The recurrence relation$F_{n+2} = F_{n+1} + F_{n}$ gives the equations
\begin{align}
F_{n+1} &= F_{n} + F_{n-1} \label{eq:Fib_1} \\
F_{n} &= F_{n}. \label{eq:Fib_2} 
\end{align} 
Let $\vx_{n} = \left[ \begin{array}{c} F_{n+1} \\ F_{n} \end{array} \right]$ for $n \geq 0$. Explain how the equations (\ref{eq:Fib_1}) and (\ref{eq:Fib_1}) can be described with the matrix equation
\begin{equation} \label{eq:Fib_matrix}
\vx_n = A \vx_{n-1},
%\left[ \begin{array}{c} F_{n+1} \\ F_{n} \end{array} \right] = A \left[ \begin{array}{c} F_{n} \\ F_{n-1} \end{array} \right] 
\end{equation}
where $A = \left[ \begin{array}{cc} 1&1 \\ 1&0 \end{array} \right]$. 

\end{pactivity}


The matrix equation (\ref{eq:Fib_matrix}) shows us how to find the vectors $\vx_n$ using powers of the matrix $A$:
\begin{align*}
\vx_1 &= A\vx_0 \\
\vx_2 &= A\vx_1 = A(A\vx_0) = A^2\vx_0 \\
\vx_3 &= A\vx_2 = A(A^2\vx_0) = A^3\vx_0 \\
\ \vdots & \qquad \vdots \\
\vx_n &= A^n\vx_0.
\end{align*}
So if we can somehow easily find the powers of the matrix $A$, then we can find a convenient formula for $F_n$. As we have seen, we know how to do this if $A$ is diagonalizable


\begin{pactivity} Let $A = \left[ \begin{array}{cc} 1&1 \\ 1&0 \end{array} \right]$. 
\ba
\item Show that the eigenvalues of $A$ are $\gr = \frac{1 + \sqrt{5}}{2}$ and $\grc = \frac{1 - \sqrt{5}}{2}$.

\item Find bases for each eigenspace of $A$.

\ea

\end{pactivity}

Now that we have the eigenvalues and know corresponding eigenvectors for $A$, we can return  to the problem of diagonalizing $A$. 

\begin{pactivity} \hfill
\ba
\item Why do we know that $A$ is diagonalizable?

\item Find a matrix $P$ such that $P^{-1}AP$ is a diagonal matrix. What is the diagonal matrix?

\ea

\end{pactivity}

Now we can find a formula for the $n$th Fibonacci number.

\begin{pactivity} Since $P^{-1}AP = D$, where $D$ is a diagonal matrix, we also have $A = PDP^{-1}$. Recall that when $A = PDP^{-1}$, it follows that $A^n = PD^nP^{-1}$. Use the equation $A^n = PD^nP^{-1}$ to show that 
\begin{equation} \label{eq:Binet}
F_n = \frac{\gr^n - \grc^n}{\sqrt{5}}.
\end{equation}
(Hint: We just need to calculate the second component of $A^n \vx_0$.)

\end{pactivity}

Formula (\ref{eq:Binet}) is called \emph{Binet's formula}\index{Binet's formula}. It is a very surprising formula in the fact that the expression on the right hand side of (\ref{eq:Binet}) is an integer for each positive integer $n$. Note that with Binet's formula we can quickly compute $F_n$ for very large values of $n$. For example, 
\[F_{150} = 9969216677189303386214405760200.\]

The number $\gr  = \frac{1+\sqrt{5}}{2}$, called the  \emph{golden mean} or \emph{golden ratio} is intimately related to the Fibonacci sequence. Binet's formula provides a fascinating relationship between the Fibonacci numbers and the golden ratio. The golden ratio also occurs often in other areas of mathematics. It was an important number to the ancient Greek mathematicians who felt that the most aesthetically pleasing rectangles had sides in the ratio of $\gr:1$. 

\begin{pactivity} You might wonder what happens if we use negative integer exponents in Binet's formula. In other words, are there negatively indexed Fibonacci numbers? For any integer $n$, including negative integers, let 
\[F_n = \frac{\gr^n - \grc^n}{\sqrt{5}}\]
There is a specific relationship between $F_{-n}$ and $F_n$. Find it and verify it. 


\end{pactivity}


