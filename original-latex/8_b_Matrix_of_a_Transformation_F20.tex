\achapter{38}{The Matrix of a Linear Transformation} \label{chap:transformation_matrix}

\vspace*{-17 pt}
\framebox{
\parbox{\dimexpr\linewidth-3\fboxsep-3\fboxrule}
{\begin{fqs}
\item How do we represent a linear transformation from $\R^n$ to $\R^m$ as a matrix transformation? 
\item How can we represent any linear transformation from a finite dimensional vector space $V$ to a finite dimensional vector space $W$ as a matrix transformation? 
\item In what ways is representing a linear transformation as a matrix transformation useful? 
\end{fqs}}}% \hspace*{3 pt}}

\vspace*{13 pt}

\csection{Application: Secret Sharing Algorithms} 
\label{sec:appl_secret}

Suppose we have a secret that we want or need to share with several individuals. For example, a bank manager might want to share pieces of the code for the bank vault with several employees so that if the manager is not available, some subset of these employees could open the vault if they work together. This allows for a significant amount of security while still making the code available if needed. As another example, in order to keep passwords secure (as they can be hard to remember), a person could implement a secret sharing scheme. The person would generate a set of shares for a given password and store them in several different places. If the person forgets the password, it can be reconstructed by collecting some set of these shares. Since the shares can be stored in many different places, the password is relatively secure.  

The idea behind secret sharing algorithms is to break a secret into a number ($n$) of pieces and give each person one piece. A code is then created in such a way that any $k$ individuals could combine their information and learn the secret, but no group of fewer than $k$ individuals could. This is called a $(k,n)$ threshold scheme. One secret sharing algorithm is Shamir's Secret Sharing, which, as we will see later in this section, involves Lagrange polynomials. In order to implement this algorithm, we will use matrices of linear transformations from polynomials spaces to $\R^n$. 


\csection{Introduction}
\label{sec:mtxof_trans_intro}

A matrix transformation $T$ from $\R^n$ to $\R^m$ is a linear transformation defined by $T(\vx) = A\vx$ for some $m \times n$ matrix $A$. We can use the matrix to quickly determine if $T$ is one-to-one or onto -- if every column of $A$ contains a pivot, then $T$ is one-to-one, and if every row of $A$ contains a pivot, then $T$ is onto. As we will see, we can represent any linear transformation between finite dimensional vector spaces as a matrix transformation, which will allow us to use all of the tools we have developed for matrices to study linear transformations. We will begin with linear transformations from $\R^n$ to $\R^m$. 



\begin{pa} \label{pa:8_b} Let $T$ be a linear transformation from $\R^2$ into $\R^3$. Let's also say that we know the following information about $T$:
\[T\left(\left[ \begin{array}{c} 1 \\ 0 \end{array} \right] \right) = \left[ \begin{array}{r} 0 \\ 2 \\ -1 \end{array} \right] \text{ and }
T\left(\left[ \begin{array}{c} 0 \\ 1 \end{array} \right] \right) = \left[ \begin{array}{r} 1 \\ 3 \\ 2 \end{array} \right].\]
	\be
	\item Find  $T\left(\left[ \begin{array}{c} 2 \\ 0 \end{array} \right] \right)$. Hint: Use the fact that $T$ is a linear transformation.

	\item Find $T\left(\left[ \begin{array}{c} 1 \\ 1 \end{array} \right] \right)$.

	
	\item Find $T\left(\left[ \begin{array}{c} 2 \\ 3 \end{array} \right] \right)$.

	\item Find $T\left(\left[ \begin{array}{c} a \\ b \end{array} \right] \right)$ for any real numbers $a$ and $b$.


	\item Is it possible to find a matrix $A$ so that $T\left(\left[ \begin{array}{c} a \\ b \end{array} \right] \right) = A \left[ \begin{array}{c} a \\ b \end{array} \right]$ for any real numbers $a$ and $b$? If so, what is this matrix? If not, why not?


	\ee

\end{pa}

\csection{Linear Transformations from $\R^n$ to $\R^m$}
\label{sec:trans_rn_rm}

Preview Activity \ref{pa:8_b} illustrates the method for representing a linear transformation from $\R^n$ to $\R^m$ as a matrix transformation. We now consider the general context. 


\begin{activity} \label{act_8_b_matrix_of_a_linear_transformation} Let $T$ be a linear transformation from $\R^n$ to $\R^m$.  Let $\ve_i$ be the $i$th column of the $n \times n$ identity matrix. In other words,
\[\ve_1 = \left[ \begin{array}{c} 1 \\ 0 \\ 0 \\ 0 \\ \vdots \\ 0 \end{array} \right], \ve_2 = \left[ \begin{array}{c} 0 \\ 1 \\ 0 \\ 0  \\ \vdots \\ 0 \end{array} \right], \ve_3 = \left[ \begin{array}{c} 0 \\ 0 \\ 1 \\ 0 \\ \vdots \\ 0 \end{array} \right], \ldots , \ve_n = \left[ \begin{array}{c} 0 \\ 0 \\ 0 \\ \vdots \\ 1 \end{array} \right]\]
in $\R^n$. The set $\{\ve_1, \ve_2, \ldots, \ve_n\}$ is the standard basis for $\R^n$. Let $\vx = \left[ \begin{array}{c} x_1 \\ x_2 \\ \vdots \ \\x_n \end{array} \right]$ in $\R^n$. Explain why 
$T(\vx) = A \vx$ for every $\vx$ in $\R^n$, where
\[A = [\ T(\ve_1) \; \;  T(\ve_2)  \; \; \cdots \; \;  T(\ve_n) \ ]\]
is the matrix with columns $T(\ve_1)$, $T(\ve_2)$, $\ldots$, $T(\ve_n)$. 

\end{activity}

As we discovered in Activity \ref{act_8_b_matrix_of_a_linear_transformation}, a linear transformation from $\R^n$ to $\R^m$ can be expressed as a matrix transformation where the columns of the matrix are given by the images of $\ve_i$, the columns of the $n\times n$ identity matrix. This matrix is called the \emph{standard matrix} of the linear transformation.

\begin{definition} If $T$ is a linear transformation from $\R^n$ to $\R^m$, the \textbf{standard matrix}\index{linear transformation!standard matrix} for $T$ is the matrix  
\[A= [ \ T(\ve_1) \;\;  T(\ve_2)  \;\; \cdots \;\;  T(\ve_n) \ ]\]
with columns $T(\ve_1)$, $T(\ve_2)$, $\ldots$, $T(\ve_n)$, where $\{\ve_1, \ve_2, \ldots, \ve_n\}$ is the standard basis for $\R^n$. More specifically, $T(\vx)=A\vx$ for every $\vx$ in $\R^n$. 
 \end{definition}


\csection{The Matrix of a Linear Transformation}
\label{sec:mtx_lin_trans}

We saw in the previous section that any linear transformation $T$ from $\R^n$ to $\R^m$ is in fact a matrix transformation. In this section, we turn to the general question -- can any linear transformation $T: V \to W$ between an $n$-dimensional vector space $V$ and an $m$-dimensional vector space $W$ be represented in some way as a matrix transformation? This will allow us to extend ideas like eigenvalues and eigenvectors to arbitrary linear transformations. We begin by investigating an example.



The general process for representing a linear transformation as a matrix transformation is as described in Activity \ref{act:8_b_general_matrix_transformation}. 


\begin{activity} \label{act:8_b_general_matrix_transformation} Let $V$ be an $n$ dimensional vector space and $W$ an $m$ dimensional vector space and suppose $T : V \to W$ is a linear transformation. Let $\CB = \{\vv_1, \vv_2, \ldots, \vv_n\}$ be a basis for $V$ and $\CC = \{\vw_1, \vw_2, \ldots, \vw_m\}$ a basis for $W$. Let $\vv$ be in $V$ so that 
\[\vv = c_1\vv_1 + c_2 \vv_2 + \cdots + c_n \vv_n\]
for some scalars $c_1$, $c_2$, $\ldots$, $c_n$. 
	\ba
	\item What is $[\vv]_\CB$?
	
	\item Note that for $\vv$ in $V$, $T(\vv)$ is an element in $W$, so we can consider the coordinate vector for $T(\vv)$ with respect to $\CC$. Explain why 
\begin{equation} \label{eq:Matrix_linear_transformation}
[T(\vv)]_{\CC} = c_1[T(\vv_1)]_{\CC} + c_2 [T(\vv_2)]_{\CC} + \cdots + c_n [T(\vv_n)]_{\CC}.
\end{equation}


	\item Now find a matrix $A$ so that Equation (\ref{eq:Matrix_linear_transformation}) has the form $[T(\vv)]_{\CC} = A [\vv]_{\CB}$. We call the matrix $A$ to be the \emph{matrix for $T$ relative to the bases $\CB$ and $\CC$} and denote this matrix as $[T]_{\CB}^{\CC}$ so that
\[[T(\vv)]_{\CC} = [T]_{\CB}^{\CC} [\vv]_{\CB}.\]

	\ea

\end{activity}


Activity \ref{act:8_b_general_matrix_transformation} shows us how to represent a linear transformation as a matrix transformation.

\begin{definition} Let $T$ be a linear transformation from a vector space $V$ with basis $\CB = \{\vv_1$, $\vv_2$, $\ldots$, $\vv_n\}$ to a vector space $W$ with basis $\CC = \{\vw_1, \vw_2, \ldots, \vw_m\}$. The \textbf{matrix for $T$ with respect to the bases $\CB$ and $\CC$}\index{linear transformation!matrix of} is the matrix  
\[[T]_{\CB}^{\CC} = \left[  [T(\vv_1)]_{\CC}  \ [T(\vv_2)]_{\CC}  \ [T(\vv_3)]_{\CC}  \ \cdots  \ [T(\vv_n)]_{\CC} \right].\] 
\end{definition}

If $V=W$ and $\CC = \CB$, then we use the notation $[T]_{\CB}$ as a shorthand for $[T]_{\CB}^{\CC}$. The matrix $[T]_{\CB}^{\CC} $ has the property that
\[[T(\vv)]_{\CC} = A [\vv]_{\CB}\]
for any $\vv$ in $V$. Recall that we can find the unique vector in $W$ whose coordinate vector is $[T(\vv)]_{\CC}$, so we have completely realized the transformation $T$ as a matrix transformation. In essence, we are viewing $T$ as a composite of linear transformations, first from $V$ to $\R^n$, then from $\R^n$ to $\R^m$, then from $\R^m$ to $W$ as illustrated in Figure \ref{F:matrix_transformation}.

\begin{figure}[ht]
\begin{center}
\setlength{\unitlength}{1.25cm}
\begin{picture}(2.55,3.35)
%rt angle
\put(1,0.5){$V$}
\put(3,0.5){$W$}
\put(1,2.5){$\R^n$}
\put(3,2.5){$\R^m$}
\put(1.4,0.6){\vector(1,0){1.5}}
\put(1.1,0.9){\vector(0,1){1.5}}
\put(1.4,2.6){\vector(1,0){1.5}}
\put(3.1,0.9){\vector(0,1){1.5}}
\put(2.01,0.2){$T$}
\put(1.98,2.8){$[T]_{\CB}^{\CC}$}
\put(0.5,1.6){$[ \ ]_{\CB}$}
\put(3.3,1.6){$[ \ ]_{\CC}$}
\end{picture} 
\caption{Visualizing the matrix of a linear transformation.}
\label{F:matrix_transformation}
\end{center}
\end{figure}

\begin{activity} Let $T : \pol_2 \to \pol_1$ be defined by $T(p(t))= p'(t)$. Let $\CB = \{1+t,1-t,t+t^2\}$ be a basis for $\pol_2$ and $\CC = \{1, t\}$ be a basis for $\pol_1$. 
	\ba
	\item Find the matrix $[T]_{\CB}^{\CC}$.
	

	\item Find $[1+t+t^2]_{\CB}$. Then use the matrix $[T]_{\CB}^{\CC}$ to calculate $[T(1+t+t^2)]_{\CC}$. (Hint: Use the fact that $1+t+t^2 =\frac{1}{2}(1+t) + \frac{1}{2}(1-t) + (t+t^2)$.) 


	\item Calculate $T(1+t+t^2)$ directly from the definition of $T$ and compare to your result from part (b).

	\item Find the matrix $[T]_{\CB}^{\CC'}$ where $\CC'=\{1+t, 1-t\}$. Use the facts that 
	\[-1 = -\frac{1}{2}(1+t) - \frac{1}{2}(1-t) \ \text{ and } \ 1+2t = \frac{3}{2}(1+t) - \frac{1}{2}(1-t).\]
	
	\ea 
\end{activity}

\csection{A Connection between $\Ker(T)$ and a Matrix Representation of $T$}
\label{sec:ker_mtx}

Recall that we defined the kernel of a matrix transformation $T: \R^n \to \R^m$ to be the set of vectors that $T$ maps to the zero vector. If $T$ is the matrix transformation defined by $T(\vx) = A \vx$, we also saw that the kernel of $T$ is the same as the null space of $A$. We can make a similar connection between a linear transformation and  a matrix that defines the transformation. 

\begin{activity} \label{act:8_b_kernel} Let $T : V \to W$ be a linear transformation from an $n$-dimensional vector space $V$ to an $m$-dimensional vector space $W$ with additive identity $\vzero_W$. Let $\CB$ be a basis for $V$ and $\CC$ a basis for $W$. Let $A = [T]_{\CB}^{\CC}$. Let $T'$ be the matrix transformation from $\R^n$ to $\R^m$ defined by $T'(\vx) = A\vx$. 
\ba
\item Show that if $\vv$ is in $\Ker(T)$, then $[\vv]_{\CB}$ is in $\Nul A$. (Hint: Apply an appropriate coordinate transformation.) 


\item Let $\CB = \{\vv_1, \vv_2, \ldots, \vv_n\}$ and $\CC = \{\vw_1, \vw_2, \ldots, \vw_m\}$. Show that if the vector $\vx = [x_1 \ x_2 \ \cdots \ x_n]$ is in $\Nul A$, then the vector $\vv = x_1\vv_1 + x_2 \vv_2 + \cdots x_n \vv_n$ is in $\Ker(T)$. (Hint: Note that $[\vv]_{\CB} = \vx$.)

\ea

\end{activity}

Activity \ref{act:8_b_kernel} shows that if $T: V \to W$ is a linear transformation from an $n$-dimensional vector space $V$ with basis $\CB$ to an $m$-dimensional vector space $W$ with basis $\CC$, then there is a one-to-one correspondence between vectors in $\Ker(T)$ and vectors in $\Nul [T]_{\CB}^{\CC}$. Recall that $T$ is one-to-one if and only if $\Ker(T) = \{\vzero_V\}$, where $\vzero_V$ is the additive identity of $V$. So $T$ will be one-to-one if and only if $\Nul [T]_{\CB}^{\CC} = \{\vzero\}$. In other words, $T$ is one-to-one if and only if every column of $\Nul [T]_{\CB}^{\CC}$ is a pivot column. Note that this does not depend on the basis $\CB$ and $\CC$. 

A similar argument shows that $T$ is onto if and only if every row of $[T]_{\CB}^{\CC}$ contains a pivot. This is left to the exercises (see Exercise \ref{ex:8_b_range_matrix}).

\csection{Examples}
\label{sec:mtxof_trans_exam}

\ExampleIntro

\begin{example} Let $T: \pol_1 \to \pol_2$ be defined by $T(a+bt) = b+at+2at^2$. 
\ba
\item Show that $T$ is a linear transformation.

\item Let $\CS_1 = \{1,t\}$ and $\CS_2 = \{1,t,t^2\}$ be the standard bases for $\pol_1$ and $\pol_2$, respectively. Find the matrix $[T]_{\CS_2}^{\CS_1}$ for $T$ with respect to $\CS_2$ and $\CS_1$. Use the matrix $[T]_{\CS_2}^{\CS_1}$ to determine if $T$ is one-to-one and/or onto. Explain your reasoning. Use technology as appropriate. 

\item Use the matrix $[T]_{\CS_2}^{\CS_1}$ to find $\Ker(T)$ and $\Range(T)$. If possible, find a basis for each. 

\item Let $\CB = \{1+t,1-t\}$ be a basis for $\pol_1$ and $\CC = \{t, 1-t^2, t+t^2\}$ a basis for $\pol_2$. Find the matrix $[T]_{\CB}^{\CC}$ for $T$ with respect to $\CC$ and $\CB$. 

\item Find $T(1-2t)$ using the matrix $[T]_{\CB}^{\CC}$. 


\ea


\ExampleSolution

\ba
\item Let $p(t)=a+bt$ and $q(t)=c+dt$ be in $\pol_1$. Then
\begin{align*}
T(p(t)+q(t)) &= T((a+c)+(b+d)t)\\
	&= (b+d) + (a+c)t + 2(a+c)t^2 \\
	&= (b+at+2at^2) + (d+ct+2ct^2) \\
	&= T(p(t)) + T(q(t)).
\end{align*}
Now let $k$ be a scalar. Then
\begin{align*}
T(kp(t)) &= T((ka)+(kb)t) \\
	&= (kb)+(ka)t + (2ka)t^2 \\
	&= k(b+at+2at^2) \\
	&=kT(p(t)).
\end{align*}
We conclude that $T$ is a linear transformation. 

\item Recall that 
\[[T]_{\CS_1}^{\CS_2} = \left[ [T(1)]_{\CS_2} \ [T(t)]_{\CS_2} \right] = \left[ [t+2t^2]_{\CS_2} \ [1]_{\CS_2} \right] = \left[ \begin{array}{cc} 0&1\\1&0\\2&0 \end{array} \right].\]
The linear transformation $T$ is one-to-one and/or onto if and only if the matrix transformation defined by $[T]_{\CS_1}^{\CS_2}$ is one-to-one and/or onto. The reduced row echelon form of $[T]_{\CS_1}^{\CS_2}$ is $\left[ \begin{array}{cc} 1&0\\0&1\\0&0 \end{array} \right]$. Since $[T]_{\CS_1}^{\CS_2}$ has a pivot in every column, $T$ is one-to-one. However, $[T]_{\CS_1}^{\CS_2}$ does not have a pivot in every row, so $T$ is not onto. 

\item From part (b) we know that $T$ is one-to-one, so $\Ker(T) = \{0\}$. Let $q(t)$ be a polynomial in $\Range(T)$. Then there is a polynomial $p(t)$ in $\pol_1$ such that $T(p(t)) = q(t)$. It follows that 
\[ [q(t)]_{\CS_2} = [T(p(t))]_{\CS_2} = [T]_{\CS_1}^{\CS_2}[p(t)]_{\CS_1}.\]
If $p(t) = a_0+a_1t$, then 
\begin{align*}
 [q(t)]_{\CS_2}  &= \left[ \begin{array}{cc} 0&1\\1&0\\2&0 \end{array} \right] \left[ \begin{array}{c} a_0\\a_1 \end{array} \right] \\
 	&= \left[ \begin{array}{c} a_1\\a_0\\2a_0 \end{array} \right] \\
	&= a_0\left[ \begin{array}{c} 0\\1\\2 \end{array} \right] + a_1\left[ \begin{array}{c} 1\\0\\0\end{array} \right].
\end{align*}
Thus, $q(t)$ is in the span of $t+2t^2$ and $1$. So $\Range(T) = \Span\{1, t+2t^2\}$.  Since neither $1$ nor $t+2t^2$ is a scalar multiple of the other, the vectors $1$ and $t+2t^2$ are linearly independent. Thus, $\{1, t+2t^2\}$ is a basis for $\Range(T)$. 

\item Recall that 
\[[T]_{\CB}^{\CC} = \left[ [T(1+t)]_{\CC} \ [T(1-t)]_{\CC} \right] = \left[ [1+2t+t^2]_{\CC} \ [1-t^2]_{\CC} \right] = \left[ \begin{array}{cc} 0&0\\1&1\\2&0 \end{array} \right].\]

\item To find $T(1-2t)$ using the matrix $[T]_{\CB}^{\CC}$, recall that $[T(p(t))]_{\CC} = [T]_{\CB}^{\CC}[p(t)]_{\CB}$. First note that $[1-2t]_{\CB} = \left[\renewcommand{\arraystretch}{1.4} \begin{array}{r} -\frac{1}{2} \\ \frac{3}{2} \end{array} \right]$. So 
\[[T(p(t))]_{\CC} =  [T]_{\CB}^{\CC}[p(t)]_{\CB} = \left[ \begin{array}{cc} 1&0\\0&1\\0&0 \end{array} \right] \left[\renewcommand{\arraystretch}{1.4} \begin{array}{r} -\frac{1}{2} \\ \frac{3}{2} \end{array} \right] = \left[ \begin{array}{r} 0 \\ 1 \\ -1 \end{array} \right].\]
This makes 
\[T(1-2t) = 0(t) + (1)\left(1-t^2\right) + (-1)\left(t+t^2\right) = 1-t-2t^2.\]
To check, using the definition of $T$ we have 
\[T(1-2t) = t(1-2t) + (1-2t) = 1-t-2t^2.\]

\ea

\end{example}


\begin{example} Let $T: V \to W$ be a linear transformation. Let $\CB = \{\vv_1, \vv_2, \ldots, \vv_n\}$ be a basis for $V$ and $\CC = \{\vw_1, \vw_2, \ldots, \vw_m\}$ a basis for $W$. Let $S: \R^n \to \R^m$ be the matrix transformation defined by $S(\vx) = [T]_{\CB}^{\CC} \vx$. 
\ba
\item Let $\vw$ be a vector in $\Range(T)$. Show that $[\vw]_{\CC}$ is in $\Col \ [T]_{\CB}^{\CC}$. 

\item If $\vw \in \Range(T)$, part (a) shows that $[\vw]_{\CC}$ is in $\Col \ [T]_{\CB}^{\CC}$.  So the coordinate transformation $[ \ ]_{\CC}$ maps $\Range(T)$ into $\Col \ [T]_{\CB}^{\CC}$. Define $R: \Range(T) \to \Col \ [T]_{\CB}^{\CC}$ to be this coordinate transformation. That is, $R(\vw) = [\vw]_{\CC}$. (As a coordinate transformation, $R$ is a linear transformation.) We know that coordinate transformations are one-to-one. Show that $R$ is also an onto transformation. 

\item What can we conclude about the relationship between the vector spaces $\Range(T)$ and $\Col \ [T]_{\CB}^{\CC}$? What must then be true about $\dim(\Range(T))$ and $\dim(\Col \ [T]_{\CB}^{\CC})$? 

\ea

When we connect the results of this example with the result of Exercise \ref{ex:8_b_Ker_Nul} in this section, we obtain a linear transformation analog of the Rank-Nullity Theorem. That is,  if $T$ is a linear transformation from a vector space $V$ of dimension $n$ with basis $\CB$  to a vector space $W$ of dimension $m$ with basis $\CC$, then
\[\dim(\Ker(T)) + \dim(\Range(T)) = \dim(\Nul \ [T]_{\CB}^{\CC}) + \dim(\Col \ [T]_{\CB}^{\CC}) = n.\]

\ExampleSolution

\ba
\item Let $\vw$ be a vector in $\Range(T)$. Then there exists a vector $\vv$ in $V$ such that $T(\vv) = \vw$. It follows that 
\[[\vw]_{\CC} = [T(\vv)]_{\CC} = [T]_{\CB}^{\CC} [\vv]_{\CC}.\]
Recall that the vectors of the form $A \vz$ all linear combinations of the columns of $A$ with weights from $\vz$, so  $[T]_{\CB}^{\CC} [\vv]_{\CC}$ (or $[\vw]_{\CC}$) is in $\Col \ [T]_{\CB}^{\CC}$. 

 \item Let $R: \Range(T) \to \Col \ [T]_{\CB}^{\CC}$ be the coordinate transformation $R(\vw) = [\vw]_{\CC}$ for each $\vw \in \Range(T)$. 
To show that $R$ is an onto transformation, let $\vy$ be in $\Col [T]_{\CB}^{\CC}$. Then there exists $\vx = [x_1 \ x_2 \ \ldots \ x_n]^{\tr}$ in $\R^n$ such that $[T]_{\CB}^{\CC}\vx=\vy$. Let $\vv = x_1\vv_1 + x_2 \vv_2 + \cdots + x_n \vv_n$. Then $\vv$ is in $V$ and $[\vv]_{\CB} = \vx$. Also,
\[[T(\vv)]_{\CC} = [T]_{\CB}^{\CC} [\vv]_{\CB} = [T]_{\CB}^{\CC} \vx = \vy.\]
So if we let $\vw=T(\vv)$ in $\Range(T)$, then $[\vw]_{\CC} = \vy$ and $\vy$ and $R$ is onto. 
 
 
 	\item Since $R$ is a one-to-one and onto linear transformation from $\Range(T)$ to $\Col \ [T]_{\CB}^{\CC}$, it follows that $\Range(T)$ and $\Col \ [T]_{\CB}^{\CC}$ are isomorphic vector spaces, and therefore we also have $\dim(\Range(T)) = \dim(\Col \ [T]_{\CB}^{\CC})$.
	
\ea


\end{example}

 
\csection{Summary}
\label{sec:mtxof_trans_summ}
\begin{itemize}
\item The standard matrix for a linear transformation $T$ from $\R^n$ to $\R^m$ is the matrix 
\[A = [T(\ve_1) \ T(\ve_2) \ \cdots \ T(\ve_n)],\]
where $\{\ve_1, \ve_2, \ldots, \ve_n\}$ is the standard basis for $\R^n$. Then $T$ is the matrix transformation defined by $T(\vx) = A\vx$ for all $\vx$ in $\R^n$. 
\item Let $V$ be an $n$ dimensional vector space and $W$ an $m$ dimensional vector space and suppose $T : V \to W$ is a linear transformation. Let $\CB = \{\vv_1, \vv_2, \ldots, \vv_n\}$ be a basis for $V$ and $\CC = \{\vw_1, \vw_2, \ldots, \vw_m\}$ a basis for $W$. The matrix for $T$ with respect to the bases $\CB$ and $\CC$ is the matrix  
\[[T]_{\CB}^{\CC} = \left[  [T(\vv_1)]_{\CC}  \ [T(\vv_2)]_{\CC}  \ [T(\vv_3)]_{\CC}  \ \cdots  \ [T(\vv_n)]_{\CC} \right].\] 
If $V=W$ and $\CC = \CB$, then we use the notation $[T]_{\CB}$ as a shorthand for $[T]_{\CB}^{\CC}$. The matrix $[T]_{\CB}^{\CC} $ has the property that
\[[T(\vv)]_{\CC} = [T]_{\CB}^{\CC} [\vv]_{\CB}\]
for any $\vv$ in $V$. 
\item The matrix for a linear transformation allows us to represent any linear transformation between finite dimensional vectors spaces as a matrix transformation. When we view a linear transformation $T$ as a matrix transformation, we are able to use the matrix tools we have developed to understand $T$.
\end{itemize}

\csection{Exercises}
\label{sec:mtxof_trans_exer}
\be
\item  \label{ex:8_b_matrix_transformation} Let $T : \pol_2 \to \pol_3$ be defined by $T(a_0+a_1t+a_2t^2) = (a_0+a_1) + (a_1+a_2)t + a_0t^2 + a_2t^3$. You may assume that $T$ is a linear transformation. Let $\CB = \{p_0(t), p_1(t), p_2(t)\}$, where $p_0(t)=1$, $p_1(t)=t$, and $p_2(t) = t^2$, be the standard basis for $\pol_2$ and $\CC=\{q_0(t), q_1(t), q_2(t), q_3(t)\}$, where $q_0(t) = 1$, $q_1(t) = t$, $q_2(t)=t^2$, and $q_3(t)=t^3$, the standard basis for $\pol_3$.
	\ba
	\item Write the polynomial $r(t) = r_0+r_1t+r_2t^2$ as a linear combination $c_0p_0(t)+c_1p_1(t)+c_2p_2(t)$ of the basis vectors in $\CB$. Identify the weights $c_0$, $c_1$, and $c_2$. What is $[r(t)]_{\CB}$?
	

	\item Without doing any calculations, explain why 
\[T(r(t)) = c_0T(p_0(t)) + c_1 T(p_1(t)) + c_2 T(p_2(t)).\]

	\item Without doing any calculations, explain why 
	\[[T(r(t))]_{\CC} = c_0[T(p_0(t))]_{\CC} + c_1 [T(p_1(t))]_{\CC} + c_2 [T(p_2(t))]_{\CC}.\]
	
	
	\item Explicitly determine $[T(p_0(t))]_{\CC}$, $[T(p_1(t))]_{\CC}$, $[T(p_2(t))]_{\CC}$.


	\item Combine the results of parts (c) and (d) to find a matrix $A$ so that 
\[[T(r(t))]_{\CC} = A [r(t)]_{\CB}.\]

	
	\item Use the matrix $A$ to find $[T(1+t-t^2)]_{\CC}$. Then use this vector to calculate $T(1+t-t^2)$. 

	
	\item Calculate $T(1+t-t^2)$ directly from the rule for $T$ and compare to the previous result. 

	
	\ea


\item Let $V$ and $W$ be finite dimensional vector spaces, and let $S$ and $T$ be linear transformations from $V$ to $W$.
\ba
\item The sum $S+T$ is defined as 
\[(S+T)(\vx) = S(\vx) + T(\vx)\]
for all $\vx$ in $V$.   Let $\CB$ be a basis for $V$ and $\CC$ a basis for $W$. Is the statement
\[[S+T]_{\CB}^{\CC} = [S]_{\CB}^{\CC} + [T]_{\CB}^{\CC} \]
true or false? If true, prove the statement. If false, provide a counterexample. 

\item The scalar multiple $cT$, for a scalar $c$, is defined as 
\[(cT)(\vx) = cT(\vx)\]
for all $\vx$ in $V$.   Let $\CB$ be a basis for $V$ and $\CC$ a basis for $W$. Is the statement
\[[cT]_{\CB}^{\CC} = c[T]_{\CB}^{\CC} \]
true or false? If true, prove the statement. If false, provide a counterexample. 

\ea


\item If $V$, $W$, and $U$ are finite dimensional vector spaces and $S:V \to W$ and $T:W \to U$ are linear transformations, the composite $T \circ S$ is defined as 
\[(T \circ S)(\vx) = T(S(\vx))\]
for all $\vx$ in $V$.   
	\ba
	\item Prove that $T \circ S:V\to U$ is a linear transformation.
	\item Let $\CB$ be a basis for $V$, $\CC$ a basis for $W$, and $\CD$ a basis for $U$. Is the statement
\[[T \circ S]_{\CB}^{\CD} = [T]_{\CC}^{\CD}  [S]_{\CB}^{\CC} \]
true or false? If true, prove the statement. If false, provide a counterexample. 
	\ea

\item Let $V$ be a finite dimensional vector space with basis $\CB$, and let $T$ be a one-to-one linear transformation from $V$ to $V$.
	\ba
	\item Use the matrix $[T]_{\CB}$ to explain why $T$ is also onto. (Recall that we use the shorthand notation $[T]_{\CB}$ for $[T]_{\CB}^{\CB}$.)
	\item Since $T$ is one-to-one and onto, as a function $T$ has an inverse defined by 
\[T^{-1}(\vy) = \vx\]
whenever $T(\vx) = \vy$. Show that $T^{-1}$ is a linear transformation from $V$ to $V$.
	\item Is the statement
\[[T^{-1}]_{\CB} = \left([T]_{\CB}\right)^{-1} \]
true or false? If true, prove the statement. If false, provide a counterexample. 
	\ea

\item \label{ex:8_b_Ker_Nul} Let $V$ and $W$ be vector spaces with $\dim(V) = n$ and $\dim(W) = m$, and let $T : V \to W$. Let $\CB$ be a basis for $V$ and $\CC$ a basis for $W$. There is a connection between $\Ker(T)$ and $\Nul [T]_{\CB}^{\CC}$. Find the connection and verify it.

\item \label{ex:8_b_transformation_space} Let $V$ and $W$ be vector spaces and define $L(V,W)$ to be the set of all linear transformations from $V$ to $W$ as in Exercise \ref{ex:8_a_transformation_space} of Section \ref{sec:linear_transformation}. The set $L(V,W)$ is a vector space with the operations as as follows for $S$ and $T$ are in $L(V,W)$ and $c$ a scalar: 
\[(S+T)(\vv) = S(\vv) + T(\vv) \ \ \text{ and } \ \ (cT)(\vv) = cT(\vv)\]
for all $\vv$ in $V$. If $\dim(V) = n$ and $\dim(W) = m$, find the dimension of $L(V,W)$. (Hint: Let $\CB$ be a basis for $V$ and $\CC$ a basis for $W$. What can be said about the mapping that sends $T$ in $L(V,W)$ to $[T]_{\CB}^{\CC}$? Then use Exercise \ref{ex:8_a_isomorphic_dimension} in Section \ref{sec:linear_transformation}.) 

\item \label{ex:8_b_range_matrix} Let $T : V \to W$ be a linear transformation from an $n$-dimensional vector space $V$ to an $m$-dimensional vector space $W$. Let $\CB$ be a basis for $V$ and $\CC$ a basis for $W$. Let $A = [T]_{\CB}^{\CC}$. Let $T'$ be the matrix transformation from $\R^n$ to $\R^m$ defined by $T'(\vx) = A\vx$. 
\ba
\item Show that if $\vw$ is in $\Range(T)$, then $[\vw]_{\CC}$ is in the range of $T'$. 


\item Let $\CB = \{\vv_1, \vv_2, \ldots, \vv_n\}$ and $\CC = \{\vw_1, \vw_2, \ldots, \vw_m\}$. Show that if the vector $\vy = [y_1 \ y_2 \ \cdots \ y_m]$ is in the range of $T'$, then the vector $\vw = y_1\vw_1 + y_2 \vw_2 + \cdots y_m \vw_m$ is in the range of $T$.  


\item Explain why the results of (a) and (b) show that $T$ is onto if and only if every row of $A = [T]_{\CB}^{\CC}$ contains a pivot.


\ea


\item Our entire world is always in a state of motion. Much of this motion is vibration. When the vibrations of one object some into contact with the vibrations of another, the vibrations can be amplified. This is called \emph{resonance}. Resonance appears in our lives every day, e.g., resonance is how food cooks in a microwave oven. A great example of resonance is the collapse of the Tacoma Narrows Bridge (just Google Tacoma Narrows Bridge to find some fascinating videos of this event). Resonance can be seen in mathematical models of vibrations. One such model is the second order differential equation 
\[y'' + y = \cos(t),\]
where $y$ is some unknown function of $t$ and $y''$ is its second derivative. You can think of this system as modeling the oscillatory motion of a spring with mass attached to it. The cosine function in this example exerts an external force on the mass-spring system. (You don't need to know how this model is derived for this project.) Our goal is to find the solutions to this differential equation within the subspace $Y = \Span \ \CB$, where $\CB = \{\cos(t), \sin(t), t\cos(t), t\sin(t)\}$ of $\D^2$, the subspace of $\F$ of functions with second derivatives (you may assume that $\D^2$ is a subspace of $\F$). 

Let $T: \D^2 \to \D^2$ be defined by $T(f) = f'' + f$. 
	\ba
	\item Show that $T$ is a linear transformation.
	
	
	\item Show that $\CB$ is a basis for $Y$. (Hint: You might consider using the Wronskian.)


	\item Find the matrix of $T$ with respect to the basis $\CB$. That is, find $[T]_{\CB}$. (Recall that we use the shorthand notation $[T]_{\CB}$ for $[T]_{\CB}^{\CB}$.) 

	\item Use the matrix to find \emph{all} solutions to the equation $T(f) = \cos(t)$ in $Y$. (Hint: If $T(f) = \cos(t)$, what does that say about the relationship between $[T]_{\CB}^{\CB}$, $[\cos(t)]_{\CB}$, and $[f]_{\CB}$?)  
	
	\item Sketch a few graphs of solutions to $T(f) = \cos(t)$ and explain what they look like and how they are related to resonance. 
	

	\ea

 \item Label each of the following statements as True or False. Provide justification for your response.
\ba	
\item \textbf{True/False} If $T$ is a linear transformation from a vector space $V$ to a vector space $W$, and $[T]_{\CB}^{\CC}$ is a $3 \times 2$ matrix for some bases $\CB$ of $V$ and $\CC$ of $W$, then $T$ cannot be one-to-one.  

\item \textbf{True/False} If $T$ is a linear transformation from a vector space $V$ to a vector space $W$, and $[T]_{\CB}^{\CC}$ is a $2 \times 3$ matrix for some bases $\CB$ of $V$ and $\CC$ of $W$, then $T$ cannot be onto.  

\item \textbf{True/False} If $T$ is a linear transformation from a vector space $V$ to a vector space $W$, and $[T]_{\CB}^{\CC}$ is a $2 \times 3$ matrix for some bases $\CB$ of $V$ and $\CC$ of $W$, then $T$ cannot be one-to-one.  

\item \textbf{True/False} If $T$ is a linear transformation from a vector space $V$ to a vector space $W$, and $[T]_{\CB}^{\CC}$ is a $3 \times 2$ matrix for some bases $\CB$ of $V$ and $\CC$ of $W$, then $T$ cannot be onto.  

\item \textbf{True/False} Let $S$ be a linear transformation from a vector space $Y$ to a vector space $V$ and let $T$ be a linear transformation from a vector space $V$ to a vector space $W$. Suppose that $[S]_{\CS}^{\CB}$ is a $3 \times 4$ matrix and $[T]_{\CB}^{\CC}$ is a $4 \times 2$ matrix for some bases $\CS$ of $Y$, $\CB$ of $V$ and $\CC$ of $W$. Then $[T \circ S]_{\CS}^{\CC}$ is a $3 \times 2$ matrix. 

\item \textbf{True/False} Let $V$ be a finite dimensional vector space of dimension $n$ with basis $\CB$ and $W$ a finite dimensional vector space of dimension $m$ with basis $\CC$. Let $T$ be a linear transformation from $V$ to $W$. If the columns of the matrix $[T]_{\CB}^{\CC}$ are linearly independent, then the transformation $T$ is onto.

\item \textbf{True/False}  Let $V$ be a finite dimensional vector space of dimension $n$ with basis $\CB$ and $W$ a finite dimensional vector space of dimension $m$ with basis $\CC$. Let $T$ be a linear transformation from $V$ to $W$. If the columns of the matrix $[T]_{\CB}^{\CC}$ are linearly independent, then the transformation $T$ is one-to-one.

\item \textbf{True/False}  Let $V$ be a finite dimensional vector space of dimension $n$ with basis $\CB$ and $W$ a finite dimensional vector space of dimension $m$ with basis $\CC$. Let $T$ be a linear transformation from $V$ to $W$. If the matrix $[T]_{\CB}^{\CC}$ has $n$ pivots, then the transformation $T$ is onto.

\item \textbf{True/False} Let $V$ be a finite dimensional vector space of dimension $n$ with basis $\CB$ and $W$ a finite dimensional vector space of dimension $m$ with basis $\CC$. Let $T$ be a linear transformation from $V$ to $W$. If the matrix $[T]_{\CB}^{\CC}$ has $n$ pivots, then the transformation $T$ is one-to-one.

	\ea


\ee

\csection{Project: Shamir's Secret Sharing and Lagrange Polynomials}
\label{sec:proj_secret}

Shamir's Secret Sharing is a secret sharing algorithm developed by the Israeli cryptographer Adi Shamir, who also contributed to the invention of RSA algorithm. The idea is to develop shares related to a secret code that can be distributed in such a way that the secret code can only be discovered if a fixed number of shareholders combine their information. 

The general algorithm for Shamir's Secret Sharing works by creating a polynomial with the secret code as one coefficient and random remaining coefficients. More specifically, to create a $(k,n)$ threshold scheme, let $a_0$ be the secret code. Then choose at random $k-1$ positive integers $a_1$, $a_2$, $\ldots$, $a_{k-1}$ and create the polynomial
\[p(t) = a_0 + a_1t + a_2 t^2 + \cdots + a_{k-1}t^{k-1},\]
where $a_0$ is the secret. Evaluate $p(t)$ at $n$ different inputs $t_1$, $t_2$, $\ldots$, $t_n$ to create $n$ points $P_1 = (t_1,p(t_1))$, $P_2 = (t_2,p(t_2))$, $\ldots$, $P_n = (t_n, p(t_n))$. Each participant is given one of these points. Since any collection of $k$ distinct points on the graph of $p(t)$ uniquely determines $p(t)$, any combination of $k$ of the participants could reconstruct $p(t)$. The secret is then $p(0)$. (Note that, except under very restrictive circumstances, there is no polynomial of degree less than $k$ that passes through the given $k$ points, so it would be impossible for fewer than $k$ participants to reconstruct the message.) 

As an example, let our secret code be $a_0=1234$. Let $n = 6$ and $k = 3$. Choose two positive integers at random, say $a_1 = 100$ and $a_2 = 38$. Then
\begin{equation} \label{eq:SSS_polynomial}
p(t) = 1234 + 100t + 38t^2.
\end{equation}
Now we construct $6$ points by selecting $6$ positive integers, say $t_1 = 1$, $t_2 = 2$, $t_3 = 3$, $t_4 = 4$, $t_5 = 5$, and $t_6 = 6$. Evaluating $p(t)$ at $t=1$ through $t=6$ gives us the points $P_1 = (1,1372)$, $P_2 = (2,1586)$, $P_3 = (3,1876)$, $P_4 = (4,2242)$, $P_5 = (5,2684)$, and $P_6 = (6,3202)$. Our goal is to find the polynomial that passes through any three of these points. The next activity will show us how to find this polynomial.

\begin{pactivity} \label{act:SSS_1}  It will be instructive to work in the most general setting. We restrict ourselves to the degree $2$ case for now. Given three scalars $t_1$, $t_2$, and $t_3$, define $L: \pol_2 \to \R^3$ by 
\[L(q(t)) = \left[ \begin{array}{c} q(t_1) \\  q(t_2) \\ q(t_3) \end{array} \right].\] 
%, where $L: \pol_2 \to \R^3$ is defined by 
%\[L(q(t)) = \left[ \begin{array}{c} q(t_1) \\  q(t_2) \\ q(t_3) \end{array} \right].\]
	\ba
	\item Show that $L$ is a linear transformation.

	\item Find the matrix for $L$ with respect to the standard bases $\CB = \{\ve_1, \ve_2, \ve_3\}$ for $\R^3$ and $\CS = \{1,t,t^2\}$ for $\pol_2$.  

	\item Recall that the matrix $[L]_{\CS}^{\CB}$ has the property that $[L(q(t))]_{\CB} = [L]_{\CS}^{\CB}[q(t)]_{\CS}$ for any $q(t)$ in $\pol_2$. So $L$ is one-to-one if and only if the matrix transformation defined by $[L]_{\CS}^{\CB}$ is one-to-one. Use this idea to show that $L$ is one-to-one if and only if $t_1$, $t_2$, and $t_3$ are all different. Use appropriate technology where needed.

	
	\item Given three points $(t_1,y_1)$,  $(t_2,y_2)$, and $(t_3,y_3)$ with distinct $t_1$, $t_2$, and $t_3$, our objective is to find a polynomial $p(t)$ such that $p(t_i) = y_i$ for $i$ from $1$ to $3$. We proceed by finding quadratic polynomials $\ell_1(t)$, $\ell_2(t)$, and $\ell_3(t)$ so that $L(\ell_1(t)) =  \ve_1$, $L(\ell_2(t)) = \ve_2$, and $L(\ell_3(t)) = \ve_3$. Explain why, if $\ell_1(t)$, $\ell_2(t)$, and $\ell_3(t)$ are quadratic polynomials so that $L(\ell_1(1)) =  \ve_1$, $L(\ell_2(t)) = \ve_2$, and $L(\ell_3(t)) = \ve_3$, and $t_1$, $t_2$, and $t_3$ are all different, then the polynomial $p(t)$ will satisfy
\begin{equation} \label{eq:Lagrange_2} 
p(t) = y_1 \ell_1(t) + y_2 \ell_2(t) + y_3 \ell_3(t).
\end{equation} 

	
	\ea

\end{pactivity}


The polynomials $\ell_1(t)$, $\ell_2(t)$, and $\ell_3(t)$ in Project Activity \ref{act:SSS_1} are examples of \emph{Lagrange polynomials}\index{Lagrange polynomials}. Project Activity \ref{act:SSS_1} shows  that fitting polynomials to given points can be accomplished easily using linear combinations of Lagrange polynomials. Now we want to better understand the general formulas for the Lagrange polynomials $\ell_i(t)$. 
 
\begin{pactivity} \label{act:Lagrange_polynomials} In this activity we see how to find the quadratic polynomials $\ell_1(t)$, $\ell_2(t)$, and $\ell_3(t)$ so that $L(\ell_1(t)) =  \ve_1$, $L(\ell_2(t)) = \ve_2$, and $L(\ell_3(t)) = \ve_3$. Let $\CB = \{\ve_1, \ve_2, \ve_3\}$ and $\CS = \{1,t,t^2\}$ be the standard bases for $\R^3$ and $\pol_2$, respectively. 
	\ba
	\item Explain why the problem of solving the equations $L(\ell_1(t)) =  \ve_1$, $L(\ell_2(t)) = \ve_2$, and $L(\ell_3(t)) = \ve_3$ is equivalent to solving the equations 
\[ [L]_{\CS}^{\CB}[\ell_1(t)]_{\CS} = \ve_1, \  [L]_{\CS}^{\CB}[\ell_2(t)]_{\CS} =\ve_2, \ \text{ and } \  [L]_{\CS}^{\CB}[\ell_3(t)]_{\CS} =\ve_3.\]

	
	\item Explain why we can solve the equations 
	\[[L]_{\CS}^{\CB}[\ell_1(t)]_{\CS} = \ve_1, \ [L]_{\CS}^{\CB}[\ell_2(t)]_{\CS} = \ve_2, \ \text{ and } \  [L]_{\CS}^{\CB}[\ell_3(t)]_{\CS}= \ve_3\]
all at once by solving the matrix equation 
\[ [L]_{\CS}^{\CB} \left[[\ell_1(t)]_{\CS} \ [\ell_2(t)]_{\CS} \ [\ell_3(t)]_{\CS} \right] =  I_3.\]
 What does this tell us about the relationship between the matrix $\left[[\ell_1(t)]_{\CS} \ [\ell_2(t)]_{\CS} \ [\ell_3(t)]_{\CS} \right]$ and $[L]_{\CS}^{\CB}$?
 
 	\item Technology shows that 
\[\left([L]_{\CS}^{\CB}\right)^{-1} = \left[ \renewcommand{\arraystretch}{1.4} \begin{array}{ccc} \frac{t_2t_3}{(t_1-t_2)(t_1-t_3)} & \frac{t_1t_3}{(t_2-t_1)(t_2-t_3)} & \frac{t_1t_2}{(t_3-t_1)(t_3-t_2)}  \\  -\frac{t_2+t_3}{(t_1-t_2)(t_1-t_3)} & -\frac{t_1+t_3}{(t_2-t_1)(t_2-t_3)} & -\frac{t_1+t_2}{(t_3-t_1)(t_3-t_2)} \\  \frac{1}{(t_1-t_2)(t_1-t_3)} & \frac{1}{(t_2-t_1)(t_2-t_3)} & \frac{1}{(t_3-t_1)(t_3-t_2)} \end{array} \right].\]
Use $\left([L]_{\CS}^{\CB}\right)^{-1}$ to determine the coefficients of $\ell_1(t)$. Then apply some algebra to show that 
\[\ell_1(t) =  \frac{(t-t_2)(t-t_3)}{(t_1-t_2)(t_1-t_3)}.\]

	\item Find similar expressions for $\ell_2(t)$ and $\ell_3(t)$. Explain why these three polynomials satisfy the required conditions that $L(\ell_1(t)) =  \ve_1$, $L(\ell_2(t)) = \ve_2$, and $L(\ell_3(t)) = \ve_3$. 

	\ea
	
\end{pactivity}

Now we return to our secret code with $a_0 = 1234$. 

\begin{pactivity} Pick any three of the points $P_1 = (1,1372)$, $P_2 = (2,1586)$, $P_3 = (3,1876)$, $P_4 = (4,2242)$, $P_5 = (5,2684)$, and $P_6 = (6,3202)$. Use the Lagrange polynomials $\ell_1(t)$, $\ell_2(t)$, and $\ell_3(t)$ and (\ref{eq:Lagrange_2}) to find the polynomial whose graph contains the three chosen points. How does your polynomial compare to the polynomial $p(t)$ in (\ref{eq:SSS_polynomial})? 

\end{pactivity}

The Shamir Secret Sharing algorithm depends on being able to find a unique polynomial $p(t)$ that passes through the created points. We can understand this result with Lagrange polynomials. 

\begin{pactivity} \label{act:SSS_Lagrange_general}  The process described in Project Activity \ref{act:Lagrange_polynomials} for finding the Lagrange polynomials can be applied to any number of points. Let $(t_1,y_1)$, $(t_2,y_2)$, $(t_3,y_3)$, $\ldots$, $(t_n,y_n)$, and $(t_{n+1},y_{n+1})$ be $n+1$ points with distinct $t_i$. Generalizing the results of Project Activity \ref{act:Lagrange_polynomials}, define $\ell_i(t)$ for $i$ from $1$ to $n+1$ as follows:
\begin{align*}
\ell_i(t) &= \frac{(t-t_1)(t-t_2) \cdots (t-t_{i-1})(t-t_{i+1}) \cdots (t-t_n)(t-t_{n+1})}{(t_i-t_1)(t_i-t_2) \cdots (t_i-t_{i-1})(t_i-t_{i+1}) \cdots (t_i-t_n)(t_i-t_{n+1})} \\
	&=\prod_{j \neq i} \frac{t-t_j}{t_i-t_j}. 
\end{align*}
\ba
\item Explain why $\ell_i(t)$ is in $\pol_n$ for each $i$.

    \item Explain why the polynomial $\ell_i(t)$ satisfies $\ell_i(t_i) = 1$ and $\ell_i(t_j) = 0$ if $i \neq j$.
    
    \item Let $p(t) =  \sum_{i=1}^{n+1} y_i \ell_i(t)$. That is, $p(t)$ is the polynomial 
    \[\sum_{i=1}^{n+1} y_i \frac{(t-t_1)(t-t_2) \dots (t-t_{i-1})(t-t_{i+1}) \dots (t-t_n)(t-t_{n+1})}{(t_i-t_1)(t_i-t_2) \dots (t_i-t_{i-1})(t_i-t_{i+1}) \dots (t_i-t_n)(t_i-t_{n+1})}.\]
   Explain why $p(t)$ is a polynomial in $\pol_n$ whose graph contains the points $(t_i,y_i)$ for $i$ from $1$ to $n+1$. 
         

	\item The previous part demonstrates that there is always a polynomial in $\pol_n$ whose graph contains the points $(t_1,y_1)$, $(t_2,y_2)$, $(t_3,y_3)$, $\ldots$, $(t_n,y_n)$, and $(t_{n+1},y_{n+1})$  with distinct $t_i$. The last piece we need is to know that this polynomial is unique. Use the fact that a polynomial of degree $n$ can have at most $n$ roots to show that if $f(t)$ and $g(t)$ are two polynomials in $\pol_n$ whose graphs contain the points $(t_1,y_1)$, $(t_2,y_2)$, $\ldots$, $(t_n,y_n)$, $(t_{n+1},y_{n+1})$ with distinct values of $t_i$, then $f(t) = g(t)$. This completes Shamir's Secret Sharing algorithm. 
  
             
\ea

\end{pactivity}
 

