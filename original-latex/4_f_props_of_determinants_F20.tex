\achapter{22}{Properties of Determinants} \label{sec:det_properties}

\vspace*{-17 pt}
\framebox{
\parbox{\dimexpr\linewidth-3\fboxsep-3\fboxrule}
{\begin{fqs}
\item How do elementary row operations change the determinant?
\item How can we represent elementary row operations via matrix multiplication?
\item How can we use elementary row operations to calculate the determinant more efficiently?
\item What is the Cramer's rule for the explicit formula for the inverse of a matrix?  
\item How can we interpret determinants from a geometric perspective?
\item What is an $LU$ factorization of a matrix and why is such a factorization useful?
\end{fqs}}}% \hspace*{3 pt}}

\vspace*{13 pt}




\csection{Introduction}

This section is different than others in that it contains mainly proofs of previously stated results and only a little new material. Consequently, there is no application attached to this section.

We have seen that an important property of the determinant is that it provides an easy criteria for the invertibility of a matrix. As a result, we obtained an algebraic method for finding the eigenvalues of a matrix, using the characteristic equation. In this section, we will investigate other properties of the determinant related to how elementary row operations change the determinant. These properties of the determinant will help us evaluate the determinant in a more efficient way compared to using the cofactor expansion method, which is computationally intensive for large $n$ values due to it being a recursive method. Finally, we will derive a geometrical interpretation of the determinant.


\begin{pa} \label{pa:4_f} ~
\be
\item We first consider how the determinant changes if we multiply a row of the matrix by a constant.

\ba 
\item Let $A = \left[ \begin{array}{cc} 2&3 \\ 1&4 \end{array} \right]$. Pick a few different values for the constant $k$ and compare the determinant of $A$ and that of $\left[ \begin{array}{cc} 2k& 3k \\ 1&4 \end{array} \right]$. What do you conjecture that the effect of multiplying a row by a constant on the determinant is?


\item If we want to make sure our conjecture is valid for any $2\times 2$ matrix, we need to show that for $A = \left[ \begin{array}{cc} a&b\\c&d \end{array} \right]$, the relationship between $\det(A)$ and the determinant of $\left[ \begin{array}{cc} a\cdot k&b\cdot k\\c&d \end{array} \right]$ follows our conjecture. We should also check that the relationship between $\det(A)$ and the determinant of $\left[ \begin{array}{cc} a&b\\c\cdot k&d\cdot k \end{array} \right]$ follows our conjecture. Verify this.



\item Make a similar conjecture for what happens to the determinant when a row of a $3\times 3$ matrix $A$ is multiplied by a constant $k$, and explain why your conjecture is true using the cofactor expansion definition of the determinant.



\ea

\item The second type of elementary row operation we consider is row swapping. 

\ba \item Take a general $2\times 2$ matrix $A = \left[ \begin{array}{cc} a&b\\c&d \end{array} \right]$ and determine how row swapping effects the determinant.



\item Now choose a few different $3\times 3$ matrices and see how row swapping changes the determinant in these matrices by evaluating the determinant with a calculator or any other appropriate technology. 



\item Based on your results so far, conjecture how row swapping changes the determinant in general.



\ea

\item The last type of elementary row operation is adding a multiple of a row to another. Determine the effect of this operation on a $2\times 2$ matrix by evaluating the determinant of a general $2\times 2$ matrix after a multiple of one row is added to the other row.



\item All of the elementary row operations we discussed above can be achieved by matrix multiplication with \emph{elementary matrices}. For each of the following elementary matrices, determine what elementary operation it corresponds to by calculating the product $EA$, where  $A = \left[ \begin{array}{ccc} a_{11}&a_{12}&a_{13}\\a_{21}&a_{22}&a_{23}\\a_{31}&a_{32}&a_{33} \end{array} \right]$ is a general $3\times 3$ matrix.


\ba 
\begin{minipage}{1.5in} 
\item $E = \left[ \begin{array}{ccc} 0&1&0\\ 1&0&0 \\ 0&0&1 \end{array} \right]$
\end{minipage}
\begin{minipage}{1.5in} 
\item $E = \left[ \begin{array}{ccc} 1&0&0\\ 0&3&0 \\ 0&0&1 \end{array} \right]$
\end{minipage}
\begin{minipage}{1.5in} 
\item $E = \left[ \begin{array}{ccc} 1&0&0\\ 0&1&2 \\ 0&0&1 \end{array} \right]$
\end{minipage}

\ea


\ee

\end{pa}


\csection{Elementary Row Operations and Their Effects on the Determinant}

In Preview Activity \ref{pa:4_f}, we conjectured how elementary row operations affect the determinant of a matrix. In the following activity, we prove how the determinant changes when a row is multiplied by a constant using the cofactor expansion definition of the determinant.


\begin{activity} \label{act:4_f_1} In this activity, assume that the determinant of $A$ can be determined by a cofactor expansion along any row or column. (We will prove this result independently later in this section.) Consider an arbitrary $n \times n$ matrix $A = [a_{ij}]$. 
	\ba
	\item  

		\item Write the expression for $\det(A)$ using the cofactor expansion along the second row.


	
		\item Let $B$ be obtained by multiplying the second row of $A$ by $k$. Write the expression for $\det(B)$ if the cofactor expansion along the second row is used.



		\item Use the expressions you found above, to express $\det(B)$ in terms of $\det(A)$.

 

		\item Explain how this method generalizes to prove the relationship between the determinant of a matrix $A$ and that of the matrix obtained by multiplying a row by a constant $k$.
	

	
	\ea

\end{activity}



Your work in Activity \ref{act:4_f_1} proves the first part of the following theorem on how elementary row operations change the determinant of a matrix. 

\begin{theorem} \label{thm:4_f_1} Let $A$ be a square matrix. 
\begin{enumerate}
\item If $B$ is obtained by multiplying a row of $A$ by a constant $k$, then $\det(B)=k\det(A)$.
\item If $B$ is obtained by swapping two rows of $A$, then $\det(B)=-\det(A)$.
\item If $B$ is obtained by adding a multiple of a row of $A$ to another, then $\det(B)=\det(A)$.
\end{enumerate}
\end{theorem}

In the next section, we will use elementary matrices to prove the last two properties of Theorem \ref{thm:4_f_1}. 

\csection{Elementary Matrices}

As we saw in Preview Activity \ref{pa:4_f}, elementary row operations can be achieved by multiplication by \emph{elementary matrices}.

\begin{definition} An \textbf{elementary matrix}\index{matrix!elementary} is a matrix obtained by performing a single elementary row operation on an identity matrix.
\end{definition}



The following elementary matrices correspond, respectively, to an elementary row operation which swaps rows 2 and 4; an elementary row operation which multiplies the third row by 5; and an elementary row operation which adds four times the third row to the first row on any $4\times 4$ matrix:

\[ E_1 = \left[ \begin{array}{cccc} 1&0&0&0\\0&0&0&1\\0&0&1&0\\ 0&1&0&0 \end{array} \right], \ \ E_2 = \left[ \begin{array}{cccc} 1&0&0&0\\0&1&0&0\\0&0&5&0 \\ 0&0&0&1 \end{array} \right], \ \ \text{ and } \ \ E_3 = \left[ \begin{array}{cccc} 1&0&4&0\\0&1&0&0\\0&0&1&0\\0&0&0&1 \end{array} \right]\,.\]

To obtain an elementary matrix corresponding an elementary row operation, we simply perform the elementary row operation on the identity matrix. For example, $E_1$ above is obtained by swapping rows 2 and 4 of the identity matrix.



With the use of elementary matrices, we can now prove the result about how the determinant is affected by elementary row operations. We first rewrite Theorem \ref{thm:4_f_1} in terms of elementary matrices:

\begin{theorem} \label{thm:4_f_2} Let $A$ be an $n\times n$ matrix. If $E$ is an $n\times n$ elementary matrix, then $\det(EA)=\det(E)\det(A)$ where
\[ \det(E) = \left\{ \begin{array}{rl} r & \text{if $E$ corresponds to multiplying a row by $r$} \\ -1& \text{if $E$ corresponds to swapping two rows} \\ 1 & \text{if $E$ corresponds to adding a multiple of a row to another.} \end{array} \right. \]
\end{theorem}



\noindent \textbf{Notes on Theorem \ref{thm:4_f_2}.} An elementary matrix $E$ obtained by multiplying a row by $r$ is a diagonal matrix with one $r$ along the diagonal and the rest 1s, so $\det(E) = r$. Similarly, an elementary matrix $E$ obtained by adding a multiple of a row to another is a triangular matrix with 1s along the diagonal, so $\det(E) = 1$. The fact that the the determinant of an elementary matrix obtained by swapping two rows is $-1$ is a bit more complicated and is verified independently later in this section. Also, the proof of \ref{thm:4_f_2} depends on the fact that the cofactor expansion of a matrix is the same along any two rows. A proof of this can also be found later in this section.



\begin{proof}[Proof of Theorem \ref{thm:4_f_2}] 
We will prove the result by induction on $n$, the size of the matrix $A$. We verified these results in Preview Activity \ref{pa:4_f} for $n=2$ using elementary row operations. The elementary matrix versions follow immediately.

Now assume the theorem is true for $k\times k$ matrices with $k\geq 2$ and consider an $n\times n$ matrix $A$ where $n=k+1$. If $E$ is an $n\times n$ elementary matrix, we want to show that $\det(EA)=\det(E)\det(A)$. Let $EA=B$. (Although it is an abuse of language, we will refer to both the elementary matrix and the elementary row operation corresponding to it by $E$.)

When finding $\det(B)=\det(EA)$ we will use a cofactor expansion along a row which is not affected by the elementary row operation $E$. Since $E$ affects at most two rows and $A$ has $n\geq 3$ rows, it is possible to find such a row, say row $i$. The cofactor expansion along row $i$ of $B$ is 
\begin{equation}\label{eq:4_f_1}
b_{i1} (-1)^{i+1} \det(B_{i1}) + b_{i2} (-1)^{i+2} \det(B_{i2}) + \cdots + b_{in} (-1)^{i+n} \det(B_{in}) \, . 
\end{equation}

Since we chose a row of $A$ which was not affected by the elementary row operation, it follows that $b_{ij}=a_{ij}$ for $1\leq j\leq n$. Also, the matrix $B_{ij}$ obtained by removing row $i$ and column $j$ from matrix $B=EA$ can be obtained from $A_{ij}$ by an elementary row operation of the same type as $E$. Hence there is an elementary matrix $E_k$ of the same type as $E$ with $B_{ij}=E_k A_{ij}$. Therefore, by induction, $\det(B_{ij})=\det(E_k)\det(A_{ij})$ and $\det(E_k)$ is equal to 1, -1 or $r$ depending on the type of elementary row operation. If we substitute this information into equation \eqref{eq:4_f_1}, we obtain
\begin{equation*}
\begin{split}
\det(B)&= a_{i1} (-1)^{i+1} \det(E_k) \det(A_{i1}) + a_{i2} (-1)^{i+2} \det(E_k) \det(A_{i2}) \\
	&\qquad + \cdots + a_{in} (-1)^{i+n} \det(E_k) \det(A_{in})\\
	&= \det(E_k) \det(A) \, . 
\end{split}
\end{equation*}
This equation proves $\det(EA)=\det(E_k)\det(A)$ for any $n\times n$ matrix $A$ where $E_k$ is the corresponding elementary row operation on the $k\times k$ matrices obtained in the cofactor expansion.

The proof of the inductive step will be finished if we show that $\det(E_k)=\det(E)$. This equality follows if we let $A=I_n$ in $\det(EA)=\det(E_k)\det(A)$. Therefore, $\det(E)$ is equal to $r$, or 1, or $-1$, depending on the type of the elementary row operation $E$ since the same is true of $\det(E_k)$ by inductive hypothesis.

Therefore, by the principle of induction, the claim is true for every $n\geq 2$.
\end{proof}



As a corollary of this theorem, we can prove the multiplicativity of determinants:



\begin{theorem} \label{thm:determinant_product} Let $A$ and $B$ be $n\times n$ matrices. Then
\[ \det(AB)=\det(A)\det(B) \,.\]
\end{theorem}

\begin{proof}
If $A$ is non-invertible, then $AB$ is also non-invertible and both $\det(A)$ and $\det(AB)$ are 0, proving the equality in this case.

Suppose now that $A$ is invertible. By the Invertible Matrix Theorem, we know that $A$ is row equivalent to $I_n$. Expressed in terms of elementary matrices, this means that there are elementary matrices $E_1, E_2, \ldots, E_\ell$ such that 
\begin{equation} \label{eq:4_f_2}  
A= E_1 E_2 \cdots E_\ell I_n = E_1 E_2 \cdots E_\ell \,. 
\end{equation}
Therefore, repeatedly applying Theorem \ref{thm:4_f_2}, we find that 
\begin{equation} \label{eq:4_f_3}  
\det(A) = \det(E_1) \det(E_2) \cdots \det(E_\ell) \, .
\end{equation}
If we multiply equation \eqref{eq:4_f_2} by $B$ on the right, we obtain
\[ AB = E_1 E_2 \cdots E_\ell B \,. \]
Again, by repeatedly applying Theorem \ref{thm:4_f_2} with this product of matrices, we find
\[ \det(AB) = \det(E_1 E_2 \cdots E_\ell B ) = \det(E_1) \det(E_2) \cdots \det(E_\ell) \det(B) \, .\]
From equation \eqref{eq:4_f_3}, the product of $\det(E_i)$'s equals $\det(A)$, so 
\[ \det(AB) = \det(A) \det(B) \]
which finishes the proof of the theorem.
\end{proof}



We can use the multiplicative property of the determinant and the determinants of elementary matrices to calculate the determinant of a matrix in a more efficient way than using the cofactor expansion. The next activity provides an example.



\begin{activity} \label{act:4_f_2} Let $A=\left[ \begin{array}{rcc} 1&1&2\\ 2&2&6\\ -1&2&1\end{array} \right]$.
\ba 
\item Use elementary row operations to reduce $A$ to a row echelon form. Keep track of the elementary row operation you use.


\item Taking into account how elementary row operations affect the determinant, use the row echelon form of $A$ to calculate $\det(A)$.


\ea 
\end{activity}



Your work in Activity \ref{act:4_f_2} provides an efficient method for calculating the determinant. If $A$ is a square matrix, we use row operations given by elementary matrices $E_1$, $E_2$, $\ldots$, $E_k$ to row reduce $A$ to row echelon form $R$. That is
\[R = E_kE_{k-1} \cdots E_2E_1A.\]
We know $\det(E_i)$ for each $i$, and since $R$ is a triangular matrix we can find its determinant. Then
\[\det(A) = \det(E_1)^{-1}\det(E_2)^{-1} \cdots \det(E_2)^{-1}\det(R).\]
In other words, if we keep track of how the row operations affect the determinant, we can calculate the determinant of a matrix $A$ using row operations. 




\begin{activity} \label{act:4_f_2_b} Theorems \ref{thm:4_f_2} and \ref{thm:determinant_product} can be used to prove the following (part c of Theorem \ref{thm:determinant_properties}) that $A$ is invertible if and only if $\det(A) \neq 0$. We see how in this activity. Let $A$ be an $n \times n$ matrix. We can row reduce $A$ to its reduced row echelon form $R$ by elementary matrices $E_1$, $E_2$, $\ldots$, $E_k$ so that 
\[R = E_1E_2 \cdots E_kA.\]
\ba 
\item Suppose $A$ is invertible. What, then, is $R$? What is $\det(R)$? Can the determinant of an elementary matrix ever be $0$? How do we conclude that $\det(A) \neq 0$?
\item Now suppose that $\det(A) \neq 0$. What can we conclude about $\det(R)$? What, then, must $R$ be? How do we conclude that $A$ is invertible? 
\ea 
\end{activity}

\noindent \textbf{Summary: } Let $A$ be an $n\times n$ matrix. Suppose we swap rows $s$ times and divide rows by constants $k_1, k_2, \ldots, k_r$ while computing a row echelon form $\text{REF}(A)$ of $A$. Then $\det(A)=(-1)^s k_1 k_2\cdots k_r \det(\text{REF}(A))$.

\csection{Geometric Interpretation of the Determinant}

Determinants have interesting and useful applications from a geometric perspective. To understand the geometric interpretation of the determinant of an $n\times n$ matrix $A$, we consider the image of the unit square under the transformation $T(\vx)=A\vx$ and see how its area changes based on $A$.



\begin{activity} \label{act:4_f_3}~

\ba
\item Let $A = \left[ \begin{array}{cc} 2&0\\0&3 \end{array} \right]$. Start with the unit square in $\R^2$ with corners at the origin and at $(1,1)$. In other words, the unit square we are considering consists of all vectors $\vv=\left[ \begin{array}{c} x\\y \end{array} \right]$ where $0\leq x\leq 1$ and $0\leq y\leq 1$, visualized as points in the plane. 

\begin{enumerate}[i.]
\item Consider the collection of image vectors $A\vv$ obtained by multiplying $\vv$'s by $A$. Sketch the rectangle formed by these image vectors. 



\item Explain how the area of this image rectangle and the unit square is related via $\det(A)$.



\item Does the relationship you found above generalize to an arbitrary $A = \left[ \begin{array}{cc} a&0\\0&b \end{array} \right]$? If not, modify the relationship to hold for all diagonal matrices.



\end{enumerate}

\item Let $A=\left[ \begin{array}{cc} 2&1\\ 0&3 \end{array} \right]$.

\begin{enumerate}[i.]
\item Sketch the image of the unit square under the transformation $T(\vv)=A\vv$. To make the sketching easier, find the images of the vectors $[0 \  0]^{\tr}, [1 \ 0]^{\tr}, [0 \ 1]^{\tr}, [1 \ 1]^{\tr}$ as points first and then connect these images to find the image of the unit square.
	

		
\item Check that the area of the parallelogram you obtained in the above part is equal to $\det(A)$.



\item Does the relationship between the area and $\det(A)$ still hold if $A=\left[ \begin{array}{rc} -2&1\\ 0&3 \end{array} \right]$? If not, how will you modify the relationship?

 

\end{enumerate}
	
\ea

\end{activity}



It can be shown that for all $2\times 2$ matrices a similar relationship holds.




\begin{theorem}
For a $2\times 2$ matrix $A$, the area of the image of the unit square under the transformation $T(\vx)=A\vx$ is equal to $|\det(A)|$. This is equivalent to saying that $|\det(A)|$ is equal to the area of the parallelogram defined by the columns of $A$. The area of the parallelogram is also equal to the lengths of the column vectors of $A$ multiplied by $|\sin(\theta)|$ where $\theta$ is the angle between the two column vectors.
\end{theorem}



There is a similar geometric interpretation of the determinant of a $3\times 3$ matrix in terms of volume.



\begin{theorem}
For a $3\times 3$ matrix $A$, the volume of the image of the unit cube under the transformation $T(\vx)=A\vx$ is equal to $|\det(A)|$. This is equivalent to saying that $|\det(A)|$ is equal to the volume of the parallelepiped defined by the columns of $A$. 
\end{theorem}



The sign of $\det(A)$ can be interpreted in terms of the orientation of the column vectors of $A$. See the project in Section \ref{sec:determinants} for details. 



\csection{An Explicit Formula for the Inverse and Cramer's Rule}

In Section \ref{sec:matrix_inverse} we found the inverse $A^{-1}$ using row reduction of the matrix obtained by augmenting $A$ with $I_n$. However, in theoretical applications, having an explicit formula for $A^{-1}$ can be handy. Such an explicit formula provides us with an algebraic expression for $A^{-1}$ in terms of the entries of $A$. A consequence of the formula we develop is Cramer's Rule, which can be used to provide formulas that give solutions to certain linear systems.

We begin with an interesting connection between a square matrix and the matrix of its cofactors that we explore in the next activity.



\begin{activity} \label{act:4_f_4} Let $A = \left[ \begin{array}{crc} 2&1&3 \\ 1&4&5 \\ 2&-1&2 \end{array} \right]$. 
\ba
\item Calculate the $(1,1)$, $(1,2)$, and $(1,3)$ cofactors of $A$.



\item If $C_{ij}$ represents the $(i,j)$ cofactor of $A$, then the cofactor matrix $C$ is the matrix $C = [C_{ij}]$. The \emph{adjugate}\index{matrix!adjugate} matrix of $A$ is the transpose of the cofactor matrix. In our example, the adjugate matrix of $A$ is 
\[\adj(A) = \left[ \begin{array}{rrr} 13&-5&-7 \\ 8&-2&-7 \\ -9&4&7 \end{array} \right].\]
Check the entries of this adjugate matrix with your calculations from part (a). Then calculate the matrix product
\[A \ \adj(A).\]



\item What do you notice about the product $A \ \adj(A)$? How is this product related to $\det(A)$?



\ea

\end{activity}



The result of Activity \ref{act:4_f_4} is rather surprising, but it is valid in general. That is, if $A = [a_{ij}]$ is an invertible $n \times n$ matrix and $C_{ij}$ is the $(i,j)$ cofactor of $A$, then $A \ \adj(A)=\det(A) I_n$. In other words, $A \left(\frac{\adj(A)}{\det(A)}\right) = I_n$ and so 
\[A^{-1} = \frac{1}{\det(A)} \adj(A).\]
This gives us another formulation of the inverse of a matrix. To see why $A \ \adj(A) = \det(A) I_n$, we use the row-column version of the matrix product to find the $ij$th entry of $A \ \adj(A)$ as indicated by the shaded row and column 
\[\left[ \begin{array}{cccc}
a_{11} 		& a_{12} 	& \cdots   & a_{1n} \\
a_{21} 		&a_{22}		& \cdots  & a_{2n} \\
\vdots 		& \vdots      &           &\vdots  \\
\rowcolor{Gray}
a_{i1}  		&a_{i2} 		& \cdots   & a_{in} \\
\vdots 		& \vdots      &           &\vdots  \\
a_{n1} 		& a_{n2} 	& \cdots   &a_{nn}
\end{array} \right] \left[ \begin{array}{ccc>{\columncolor{Gray}}ccc}
C_{11} 	& C_{21} & \cdots    &C_{j1} &\cdots & C_{n1} \\
C_{12} 	& C_{22} & \cdots    &C_{j2} &\cdots & C_{n2} \\
\vdots 	&  \vdots  & 		    &           &			 & \vdots \\
C_{1n} 	& C_{2n} & \cdots    &C_{jn} &\cdots & C_{nn} 
\end{array} \right]. \]
Thus the $ij$th entry of $A \ \adj(A)$ is 
\begin{equation} \label{eq:4_f_4}
a_{i1}C_{j1} + a_{i2}C_{j2} + \cdots + a_{in}C_{jn}.
\end{equation}
Notice that if $i=j$, then expression (\ref{eq:4_f_4}) is the cofactor expansion of $A$ along the $i$th row. So the $ii$th entry of $A \ \adj(A)$ is $\det(A)$. It remains to show that the $ij$th entry of $A \ \adj(A)$ is 0 when $i \neq j$. 

When $i \neq j$, the expression (\ref{eq:4_f_4}) is the cofactor expansion of the matrix
\[\left[ \begin{array}{cccc}
a_{11} 		& a_{12} 	& \cdots   & a_{1n} \\
a_{21} 		&a_{22}		& \cdots  & a_{2n} \\
\vdots 		& \vdots      &           &\vdots  \\
a_{i1}  		&a_{i2} 		& \cdots   & a_{in} \\
\vdots 		& \vdots      &           &\vdots  \\
a_{j-11}  		&a_{j-12} 		& \cdots   & a_{j-1n} \\
\rowcolor{Gray}
a_{i1}  		&a_{i2} 		& \cdots   & a_{in} \\
a_{j+11}  		&a_{i+12} 		& \cdots   & a_{j+1n} \\
\vdots 		& \vdots      &           &\vdots  \\
a_{n1} 		& a_{n2} 	& \cdots   &a_{nn}
\end{array} \right]\]
along the $j$th row. This matrix is the one obtained by replacing the $j$th row of $A$ with the $i$th row of $A$. Since this matrix has two identical rows, it is not row equivalent to the identity matrix and is therefore not invertible. Thus, when $i \neq j$ expression (\ref{eq:4_f_4}) is 0. This makes $A \ \adj(A) = \det(A) I_n$.

One consequence of the formula $A^{-1} = \frac{1}{\det(A)} \adj(A)$ is Cramer's rule, which describes the solution to the equation $A \vx = \vb$. 



\begin{activity} \label{act:4_f_5} Let $A = \left[ \begin{array}{cc} 3&1 \\ 4&2 \end{array} \right]$, and let $\vb = \left[ \begin{array}{c}2\\6 \end{array} \right]$. 
\ba
\item Solve the equation $A \vx = \vb$ using the inverse of $A$. 



\item Let $A_1 =  \left[ \begin{array}{cc} 2&1 \\ 6&2 \end{array} \right]$, the matrix obtained by replacing the first column of $A$ with $\vb$. Calculate $\frac{\det(A_1)}{\det(A)}$ and compare to your solution from part (a). What do you notice?



\item Now let $A_2 =  \left[ \begin{array}{cc} 3&2 \\ 4&6 \end{array} \right]$, the matrix obtained by replacing the second column of $A$ with $\vb$. Calculate $\frac{\det(A_2)}{\det(A)}$ and compare to your solution from part (a). What do you notice?



\ea

\end{activity}



The result from Activity \ref{act:4_f_5} may seem a bit strange, but turns out to be true in general. The result is called \emph{Cramer's Rule}.



\begin{theorem}[Cramer's Rule]
Let $A$ be an $n\times n$ invertible matrix. For any $\vb$ in $\R^n$, the solution $\vx$ of $A\vx=\vb$ has entries
\[ x_i =\frac{\det(A_i)}{\det(A)} \]
where $A_i$ represents the matrix formed by replacing $i$th column of $A$ with $\vb$.\index{Cramer's Rule}
\end{theorem}



To see why Cramer's Rule works in general, let $A$ be an $n \times n$ invertible matrix and $\vb = [b_1 \ b_2 \ \cdots \ b_n]^{\tr}$. The solution to $A \vx = \vb$ is 
\[\vx = A^{-1} \vb = \frac{1}{\det(A)} \adj(A) \vb = \frac{1}{\det(A)}\left[ \begin{array}{cccc}
C_{11} 	& C_{21} &\cdots & C_{n1} \\
C_{12} 	& C_{22} &\cdots & C_{n2} \\
\vdots 	&  \vdots  &			 & \vdots \\
C_{1n} 	& C_{2n} &\cdots & C_{nn} 
\end{array} \right] \left[ \begin{array}{c} b_1 \\ b_2 \\ \vdots \\ b_n \end{array} \right].\]
Expanding the product gives us
\[\vx = \frac{1}{\det(A)}\left[ \begin{array}{c} b_1C_{11} + b_2C_{21} + \cdots + b_nC_{n1}  \\ b_1C_{12} + b_2C_{22} + \cdots + b_nC_{n2}   \\ \vdots \\ b_1C_{1n} + b_2C_{2n} + \cdots + b_nC_{nn}   \end{array} \right].\]
The expression 
\[b_1C_{1j} + b_2C_{2j} + \cdots + b_nC_{nj}\]
is the cofactor expansion of the matrix 
\[A_j = \left[ \begin{array}{cccc>{\columncolor{Gray}}cccc}
a_{11} 	& a_{12} & \cdots    &a_{1j-1} &b_1 	&a_{1j+1}		&\cdots 	& a_{1n} \\
a_{21} 	& a_{22} & \cdots    &a_{2j-1} &b_2 	&a_{2j+1}	&\cdots	& a_{2n} \\
\vdots 	&  \vdots  & 		    &           	&		&				&		 	& \vdots \\
a_{n1} 	& a_{n2} & \cdots    &a_{nj-1} &b_n 	&a_{nj+1}	&\cdots	& a_{nn} 
\end{array} \right]\]
along the $j$th column, giving us the formula in Cramer's Rule. 

Cramer's Rule is not a computationally efficient method. To find a solution to a linear system of $n$ equations in $n$ unknowns using Cramer's Rule requires calculating $n+1$ determinants of $n \times n$ matrices -- quite inefficient when $n$ is 3 or greater. Our standard method of solving systems using Gaussian elimination is much more efficient. However, Cramer's Rule does provide a formula for the solution to $A \vx = \vb$ as long as $A$ is invertible. 

\csection{The Determinant of the Transpose}

In this section we establish the fact that the determinant of a square matrix is the same as the determinant of its transpose.  

The result is easily verified for $2 \times 2$ matrices, so we will proceed by induction and assume that the determinant of the transpose of any $(n-1) \times (n-1)$ matrix is the same as the determinant of its transpose. Suppose $A = [a_{ij}]$ is an $n \times n$ matrix. By definition,
\[\det(A) = a_{11}C_{11} + a_{12}C_{12} + a_{13}C_{13} + \cdots + a_{1n}C_{1n}\]
and
\[\det(A^{\tr}) = a_{11}C_{11} + a_{21}C_{21} + a_{31}C_{31} + \cdots + a_{n1}C_{n1}.\]
Note that the only terms in either determinant that contains $a_{11}$ is $a_{11}C_{11}$. This term is the same in both determinants, so we proceed to examine other elements. Let us consider all terms in the cofactor expansion for $\det(A^{\tr})$ that contain $a_{i1}a_{1j}$. The only summand that contains $a_{i1}$ is $a_{i1}C_{i1}$. Letting $A_{ij}$ be the sub-matrix of $A$ obtained by deleting the $i$th row and $j$th column, we see that $a_{i1}C_{i1} = (-1)^{i+1}a_{i1}\det(A_{i1})$. Now let's examine the sub-matrix $A_{i1}$:
\[\left[ \begin{array}{ccc>{\columncolor{Gray}}cccc}
\rowcolor{Gray}
a_{12} & a_{13} & \cdots    &a_{1j} &\cdots & a_{1n-1} & a_{1n} \\
a_{22} & a_{23} & \cdots    &a_{2j} &\cdots & a_{2n-1} & a_{2n} \\
\vdots &       & \ddots    & \vdots          &\ddots & & \\
a_{i-12} & a_{i-13} & \cdots    &a_{i-1j} &\cdots & a_{i-1n-1} & a_{i-1n} \\
a_{i+12} & a_{i+13} & \cdots    &a_{i+1j} &\cdots & a_{i+1n-1} & a_{i+1n} \\
a_{n2} & a_{n3} & \cdots    &a_{nj} &\cdots & a_{nn-1} & a_{nn}
\end{array} \right] \]

When we expand along the first row to calculate $\det(A_{i1})$, the only term that will involve $a_{1j}$ is 
\[(-1)^{j-1+1}a_{1j}\det(A_{i1, 1j}),\]
where $A_{ik,jm}$ denotes the sub-matrix of $A$ obtained by deleting rows $i$ and $k$ and columns $j$ and $m$ from $A$. So the term that contains $a_{i1}a_{1j}$ in the cofactor expansion for $\det(A^{\tr})$ is 
\begin{equation} \label{eq:det_transpose_1}
(-1){i+1}a_{i1}(-1)^{j}a_{1j}\det(A_{i1_{1j}}) = (-1)^{i+j+1} a_{i1}a_{1j}\det(A_{i1, 1j}).
\end{equation}

Now we examine the cofactor expansion for $\det(A)$ to find the terms that contain $a_{i1}a_{1j}$. The quantity $a_{1j}$ only appears in the cofactor expansion as 
\[a_{1j}C_{1j} = (-1)^{1+j}a_{1j}\det(A_{1j}).\]
Now let's examine the sub-matrix $A_{1j}$: 
\[\left[ \begin{array}{>{\columncolor{Gray}}ccccccc}
a_{21} 	& a_{22} 	& \cdots    &a_{2j-1} &a_{2j+1} &\cdots & a_{2n} \\
a_{31} 	& a_{32} 	& \cdots    &a_{3j-1} &a_{3j+1} &\cdots & a_{3n} \\
\vdots 	&       		& \ddots    & \vdots   &\ddots 	 & 		 & \\
\rowcolor{Gray}
a_{i1} 	& a_{i2} 	& \cdots     &a_{ij-1} &a_{ij+1}   &\cdots & a_{in} \\
\vdots 	&       		&          	& \vdots   & 			  &\vdots &\\
a_{n1} 	& a_{n2} 	& \cdots    &a_{nj-1} &a_{nj+1}  &\cdots & a_{nn}
\end{array} \right] \]
Here is where we use the induction hypothesis. Since $A_{1j}$ is an $(n-1) \times (n-1)$ matrix, its determinant can be found with a cofactor expansion down the first column. The only term in this cofactor expansion that will involve $a_{i1}$ is 
\[(-1)^{i-1+1}a_{i1} \det(A_{1i, j1}).\]
So the term that contains $a_{i1}a_{1j}$ in the cofactor expansion for $\det(A)$ is 
\begin{equation} \label{eq:det_transpose_2}
(-1)^{1+j}a_{1j}(-1)^{i-1+1}a_{i1} \det(A_{1j_{i1}}) = (-1)^{i+j+1} a_{i1}a_{1j}\det(A_{1i, j1}).
\end{equation}
Since the quantities in (\ref{eq:det_transpose_1}) and (\ref{eq:det_transpose_2}) are equal, we conclude that the terms in the two cofactor expansions are the same and 
\[\det(A^{\tr}) = \det(A).\]

\csection{Row Swaps and Determinants}

In this section we determine the effect of row swaps to the determinant. Let $E_{rs}$ be the elementary matrix that swaps rows $r$ and $s$ in the $n \times n$ matrix $A=[a_{ij}]$. Applying $E_{12}$ to a $2 \times 2$ matrix $A = \left[ \begin{array}{cc} a & b \\ c & d \end{array} \right]$, we see that 
\[\det(A) = ad - bc = -(ad-bc) = \det\left(\left[ \begin{array}{cc} c & d \\ a & b \end{array} \right]\right) = \det(E_{12}A).\]
So swapping rows in a $2 \times 2$ matrix multiplies the determinant by $-1$. Suppose that row swapping on any $(n-1) \times (n-1)$ matrix multiplies the determinant by $-1$ (in other words, we are proving our statement by mathematical induction). Now suppose $A$ is an $n \times n$ matrix and let $B = [b_{ij}] = E_{rs}A$. We first consider the case that $s = r+1$ -- that we swap adjacent rows. We consider two cases, $r > 1$ and $r = 1$. First let us suppose that $r > 1$. Let $C_{ij}$ be the $(i,j)$ cofactor of $A$ and $C'_{ij}$ the $(i,j)$ cofactor of $B$. We have
\[\det(A) = a_{11}C_{11} + a_{12}C_{12} + \cdots + a_{1n}C_{1n}\]
and
\[\det(B) = b_{11}C'_{11} + b_{12}C'_{12} + \cdots + b_{1n}C'_{1n}.\]
Since $r > 1$,it follows that $a_{1j} = b_{1j}$ for every $j$. For each $j$ the sub-matrix $B_{1j}$ obtained from $B$ by deleting the $i$th row and $j$th column is the same matrix as obtained from $A_{ij}$ by swapping rows $r$ and $s$. So by our induction hypothesis, we have $C'_{1j} = -C_{1j}$ for each $j$. Then
\begin{align*}
\det(B) &= b_{11}C'_{11} + b_{12}C'_{12} + \cdots + b_{1n}C'_{1n} \\
	&= a_{11}(-C_{11}) + a_{12}(-C_{12}) + \cdots + a_{1n}(-C_{1n}) \\
	&= -(a_{11}C_{11} + a_{12}C_{12} + \cdots + a_{1n}C_{1n}) \\
	&= -\det(A).
\end{align*}
Now we consider the case where $r=1$, where $B$ is the matrix obtained from $A$ by swapping the first and second rows. Here we will use the fact that $\det(A) = \det(A^{\tr})$ which allows us to calculate $\det(A)$ and $\det(B)$ with the cofactor expansions down the first column. In this case we have 
\[\det(A) = a_{11}C_{11} + a_{21}C_{21} + \cdots + a_{n1}C_{n1}\]
and
\begin{align*}
\det(B) &= b_{11}C'_{11} + b_{21}C'_{21} + \cdots + b_{n1}C'_{n1} \\
	&= a_{21}C'_{11} + a_{11}C'_{21} + a_{31}C'_{31} + \cdots + a_{n1}C'_{n1}.
\end{align*}
For each $i \geq 3$, the sub-matrix $B_{i1}$ is just $A_{i1}$ with rows 1 and 2 swapped. So we have $C'_{i1} = -C_{i1}$ by our induction hypothesis. Since we swapped rows 1 and 2, we have $B_{21} = A_{11}$ and $B_{11} = A_{21}$. Thus,
\[b_{11}C'_{11} = (-1)^{1+1}b_{11}\det(A_{21}) = a_{21}\det(A_{21}) = -a_{21}C_{21}\]
and
\[b_{21}C'_{21} = (-1)^{2+1}a_{11}\det(A_{11}) = -a_{11}\det(A_{11}) = -a_{11}C_{11}.\]
Putting this all together gives us
\begin{align*}
\det(B) &= b_{11}C'_{11} + b_{21}C'_{21} + \cdots + b_{n1}C'_{n1} \\
	&= -a_{21}C_{21} - a_{11}C_{11} + a_{31}(-C_{31}) + \cdots + a_{n1}(-C_{n1}) \\
	&= -\left(a_{11}C_{11} + a_{21}C_{21} + \cdots + a_{n1}C_{n1}\right) \\
	&= - \det(A).
\end{align*}
So we have shown that if $B$ is obtained from $A$ by interchanging two adjacent rows, then $\det(B) = -\det(A)$. Now we consider the general case. Suppose $B$ is obtained from $A$ by interchanging rows $r$ and $s$, with $r < s$. We can perform this single row interchange through a sequence of adjacent row interchanges. First we swap rows $r$ and $r+1$, then rows $r+1$ and $r+2$, and continue until we swap rows $s-1$ and $s$. This places the original row $r$ into the row $s$ position, and the process involved $s-r$ adjacent row interchanges. Each of these interchanges multiplies the determinant by a factor of $-1$. At the end of this sequence of row swaps, the original row $s$ is now row $s-1$. So it will take one fewer adjacent row interchanges to move this row to be row $r$. This sequence of $(s-r)+(s-r-1) = 2(s-r-1)-1$ row interchanges produces the matrix $B$. Thus,
\[\det(B) = (-1)^{2(s-r)-1}\det(A) = -\det(A),\]
and interchanging any two rows multiplies the determinant by $-1$. 



\csection{Cofactor Expansions}

We have stated that the determinant of a matrix can be calculated by using a cofactor expansion along any row or column. We use the result that swapping rows introduces a factor of $-1$ in the determinant to verify that result in this section. Note that in proving that $\det(A^{\tr}) = \det(A)$, we have already shown that the cofactor expansion along the first column is the same as the cofactor expansion along the first row. If we can prove that the cofactor expansion along any row is the same, then the fact that $\det(A^{\tr}) = \det(A)$ will imply that the cofactor expansion along any column is the same as well.

Now we demonstrate that the cofactor expansions along the first row and the $i$th row are the same. Let $A = [a_{ij}]$ be an $n \times n$ matrix. The cofactor expansion of $A$ along the first row is 
\[a_{11}C_{11} + a_{12}C_{12} + \cdots + a_{1n}C_{1n}\]
and the cofactor expansion along the $i$th row is 
\[a_{i1}C_{i1} + a_{i2}C_{i2} + \cdots + a_{in}C_{in}.\]
Let $B$ be the matrix obtained by swapping row $i$ with previous rows so that row $i$ becomes the first row and the order of the remaining rows is preserved. 
\[B = \left[ \begin{array}{>{\columncolor{Gray}}cccccc}
\rowcolor{Gray}
a_{i1} 		& a_{i2} 	& \cdots    &a_{ij} 	&\cdots & a_{in} \\
a_{11} 		& a_{12} 	& \cdots    &a_{1j} 	&\cdots & a_{1n} \\
a_{21} 		& a_{22} 	& \cdots    &a_{2j} 	&\cdots & a_{2n} \\
a_{i-11} 	& a_{i-12} 	& \cdots    &a_{i-1j} 	&\cdots & a_{i-1n} \\
a_{i+11} 	& a_{i+12} 	& \cdots    &a_{i+1j} 	&\cdots & a_{i+1n} \\
\vdots 		& \ddots    & \vdots   	&\ddots 	 & 		 & \\
a_{n1} 		& a_{n2} 	& \cdots    &a_{nj}  	&\cdots & a_{nn}
\end{array} \right] \]
Then
\[\det(B) = (-1)^{i-1} \det(A).\]
So, letting $C'_{ij}$ be the $(i,j)$ cofactor of $B$ we have 
\[\det(A) = (-1)^{i-1} \det(B) = (-1)^{i-1}\left(a_{i1}C'_{11} + a_{i2}C'_{12} + \cdots + a_{in}C'_{1n}\right).\]
Notice that for each $j$ we have $B_{1j} = A_{ij}$. So 
\begin{align*}
\det(A) &= (-1)^{i-1}\big(a_{i1}C'_{11} + a_{i2}C'_{12} + \cdots + a_{in}C'_{1n}\big) \\
	&= (-1)^{i-1}\Big(a_{i1}(-1)^(1+1)\det(B_{11}) + a_{i2}(-1)^{1+2}\det(B_{12})  \\
	&\qquad + \cdots + a_{in}(-1)^{1+n}\det(B_{1n})\Big) \\
	&= (-1)^{i-1}\Big(a_{i1}(-1)^(1+1)\det(A_{i1}) + a_{i2}(-1)^{1+2}\det(A_{i2}) \\
	&\qquad + \cdots + a_{in}(-1)^{1+n}\det(A_{in})\Big) \\
	&= a_{i1}(-1)^(i+1)\det(A_{i1}) + a_{i2}(-1)^{i+2}\det(A_{i2}) \\
	&\qquad + \cdots + a_{in}(-1)^{i+n}\det(A_{in}) \\
	&= a_{i1}C_{i1} + a_{i2}C_{i2} + \cdots + a_{in}C_{in}.
\end{align*}


\csection{The LU Factorization of a Matrix}

There are many instances where we have a number of systems to solve of the form $A \vx = \vb$, all with the same coefficient matrix. The system may evolve over time so that we do not know the constant vectors $\vb$ in the system all at once, but only determine them as time progresses. Each time we obtain a new vector $\vb$, we have to apply the same row operations to reduce the coefficient matrix to solve the new system. This is time repetitive and time consuming. Instead, we can keep track of the row operations in one row reduction and save ourselves a significant amount of time. One way of doing this is the $LU$-factorization (or decomposition). 

To illustrate, suppose we can write the matrix $A$ as a product $A = LU$,
where 
\[L = \left[ \begin{array}{rccc} 1&0&0&0\\-1&1&0&0 \\0&1&1&0\\1&0&0&1\end{array}  \right] \ \ \text{ and } \ \ U = \left[ \begin{array}{ccrr} 1&0&1&0\\0&1&3&-2 \\0&0&0&3\\0&0&0&0\end{array}  \right].\]
Let $\vb = [3 \ 1 \ 1 \ 3]^{\tr}$ and $\vx = [x_1 \ x_2 \ x_3 \ x_4]^{\tr}$, and consider the linear system $A \vx = \vb$. If $A \vx = \vb$, then $LU \vx = \vb$. We can solve this system without applying row operations as follows. Let $U\vx = \vz$, where $\vz = [z_1 \ z_2 \ z_3 \ z_4]^{\tr}$. We can solve $L\vz = \vb$ by using forward substitution.

The equation $L \vz = \vb$ is equivalent to the system
\begin{alignat*}{5}
{}z_1   \	&{}  \	&{}      \	&{} \	&{}     \	&{} \   &{}       &= 3 \\
{-}z_1  \	&{+} \	&{}z_2   \	&{} \ 	&{}     \	&{}	\   &{}       &= 1 \\
{}      \	&{}  \	&{}z_2   \	&{+} \ 	&{}z_3  \   &{}	\ 	&{}       &= 1 \\
{}      \	&{}  \	&{}      \	&{}  \ 	&{}     \   &{}	\ 	&{}z_4    &= 3. \\
\end{alignat*}
The first equation shows that $z_1=3$. Substituting into the second equation gives us $z_2 = 4$. Using this information in the third equation yields $z_3 = -3$, and then the fourth equation shows that $z_4 = 0$. To return to the original system, since $U\vx = \vz$, we now solve this system to find the solution vector $\vx$. In this case, since $U$ is upper triangular, we use back substitution. The equation $U\vx = \vz$ is equivalent to the system
\begin{alignat*}{5}
{}x_1   \	&{}  \	&{}      \	&{+} \	&{}x_3   \	&{}    \   &{}       &= &{}&3 \\
{}      \	&{} \	&{}x_2   \	&{+} \ 	&{3}x_3  \	&{-}	\  &{2}x_4   &= &{}&4 \\
{}      \	&{}  \	&{}      \	&{}  \ 	&{}      \   &{}	\  &{3}x_4   &= &{-}&3. \\
\end{alignat*}
Note that the third column of $U$ is not a pivot column, so $x_3$ is a free variable. The last equation shows that $x_4=-1$. Substituting into the second equation and solving for $x_2$ yields $x_2 = 2-3x_3$. The first equation then gives us $x_1 = 3-x_3$. So the general solution 
\[\vx = \left[ \begin{array}{r} 3\\2\\0\\-1 \end{array} \right] +  \left[ \begin{array}{r} -1\\-3\\1\\0 \end{array} \right]x_3\]
to $A \vx = \vb$ can be found through $L$ and $U$ via forward and backward substitution. If we can find a factorization of a matrix $A$ into a lower triangular matrix $L$ and an upper triangular matrix $U$, then $A= LU$ is called an $LU$-\emph{factorization}\index{$LU$ factorization} or $LU$-\emph{decomposition}.  


We can use elementary matrices to obtain a factorization of certain matrices into products of lower triangular (the``L" in LU) and upper triangular (the ``U" in LU) matrices. We illustrate with an example. Let
\[A = \left[ \begin{array}{rccr} 1&0&1&0\\-1&1&2&-2 \\0&1&3&1\\1&0&1&0\end{array}  \right].\]
Our goal is to find an upper triangular matrix $U$ and a lower triangular matrix $L$ so that $A = LU$. We begin by row reducing $A$ to an upper triangular matrix, keeping track of the elementary matrices used to perform the row operations. We start by replacing the entries below the $(1,1)$ entry in $A$ with zeros. The elementary matrices that perform these operations are
\[E_1 = \left[ \begin{array}{cccc} 1&0&0&0\\1&1&0&0 \\0&0&1&0\\0&0&0&1\end{array}  \right] \ \ \text{ and } \ \ E_2 = \left[ \begin{array}{rccc} 1&0&0&0\\0&1&0&0 \\0&0&1&0\\-1&0&0&1\end{array}  \right],\]
and
\[E_2E_1A = \left[ \begin{array}{ccrr} 1&0&1&0\\0&1&3&-2 \\0&1&3&1\\0&0&0&0\end{array}  \right].\]
We next zero out the entries below the $(2,2)$ entry as
\[E_3E_2E_1A = \left[ \begin{array}{ccrr} 1&0&1&0\\0&1&3&-2 \\0&0&0&3\\0&0&0&0\end{array}  \right],\]
where
\[E_3 = \left[ \begin{array}{crcc} 1&0&0&0\\0&1&0&0 \\0&-1&1&0\\0&0&0&1\end{array}  \right].\]
The product $E_3E_2E_1A$ is an upper triangular matrix $U$. So we have
\[E_3E_2E_1A = U\]
and
\[A = E_1^{-1}E_2^{-1}E_3^{-1}U,\]
where
\[E_1^{-1}E_2^{-1}E_3^{-1} = \left[ \begin{array}{rccc} 1&0&0&0\\-1&1&0&0 \\0&1&1&0\\1&0&0&1\end{array}  \right]\]
is a lower triangular matrix $L$. So we have decomposed the matrix $A$ into a product $A = LU$, where $L$ is lower triangular and $U$ is upper triangular. Since every matrix is row equivalent to a matrix in row echelon form, we can always find an upper triangular matrix $U$ in this way. However, we may not always obtain a corresponding lower triangular matrix, as the next example illustrates. 

Suppose we change the problem slightly and consider the matrix
\[B = \left[ \begin{array}{rccr} 1&0&1&0\\-1&1&2&-2 \\0&1&3&1\\1&0&0&1\end{array}  \right].\]
Using the same elementary matrices $E_1$, $E_2$, and $E_3$ as earlier, we have
\[E_3E_2E_1B = \left[ \begin{array}{ccrr} 1&0&1&0\\0&1&3&-2 \\0&0&0&3\\0&0&-1&1\end{array}  \right].\]
To reduce $B$ to row-echelon form now requires a row interchange. Letting
\[E_4 = \left[ \begin{array}{crcc} 1&0&0&0\\0&1&0&0 \\0&0&0&1\\0&0&1&0\end{array}  \right]\]
brings us to
\[E_4E_3E_2E_1B = \left[ \begin{array}{ccrr} 1&0&1&0\\0&1&3&-2 \\0&0&-1&1\\0&0&0&3\end{array}  \right].\]
So in this case we have $U = E_4E_3E_2E_1B$, but
\[E_1^{-1}E_2^{-1}E_3^{-1}E_4^{-1} = \left[ \begin{array}{rccc} 1&0&0&0\\-1&1&0&0 \\0&1&0&1\\1&0&1&0\end{array}  \right]\]
is not lower triangular. The difference in this latter example is that we needed a row swap to obtain the upper triangular form. 

\csection{Examples}

\ExampleIntro

\begin{example} ~
	\ba
	\item If $A$, $B$ are $n\times n$ matrices with $\det(A)=3$ and $\det(B)=2$, evaluate the following determinant values. Briefly justify.\\
		\begin{enumerate}[i.]
		\item $\det(A^{-1})$

		\item $\det(ABA^{\mathsf{T}})$

		\item $\det(A^3(BA)^{-1}(AB)^2)$
		
		\end{enumerate}
		
	\item If the determinant of $\left[ \begin{array}{ccc} a&b&c\\d&e&f\\g&h&i\end{array} \right]$ is $m$, find the determinant of each of the following matrices.

		\begin{enumerate}[i.]
		\item $\left[ \begin{array}{ccc} a&b&c\\2d&2e&2f\\g&h&i\end{array} \right]$

		\item $\left[ \begin{array}{ccc} d&e&f\\g&h&i\\a&b&c\end{array} \right]$

		\item $\left[ \begin{array}{ccc} a&b&c\\g-2d&h-2e&i-2f\\a+d&b+e&c+f \end{array} \right]$
		\end{enumerate}
		
	\ea


\ExampleSolution
	\ba
	\item Assume that $\det(A)=3$ and $\det(B)=2$. 
		\begin{enumerate}[i.]
		\item Since $\det(A) \neq 0$, we know that $A$ is invertible. Since $1 = \det(I_n) = \det(AA^{-1}) = \det(A) \det(A^{-1})$, it follows that $\det(A^{-1}) = \frac{1}{\det(A)} = \frac{1}{3}$. 
		
		\item We know that $\det(A^{\tr}) = \det(A)$, so 
\begin{align*}
 \det(ABA^\tr) &= \det(A) \det(B) \det(A^{\tr}) \\
 	&= \det(A) \det(B) \det(A) \\
	&= (3)(2)(3) \\
	&= 18.
\end{align*}

		\item  Using properties of determinants gives us 
\begin{align*}
\det(A^3(BA)^{-1}(AB)^2) &= \det(A^3)\det((BA)^{-1}) \det((AB)^2) \\
	&= (\det(A))^3 \left(\frac{1}{\det(AB)}\right) (\det(AB))^2 \\
	&= 27 \left(\frac{1}{\det(A) \det(B)} \right) (\det(A)\det(B))^2 \\
	&= \frac{(27)(6^2)}{6} \\
	&= 162.
\end{align*}
		\end{enumerate}
	
	\item Assume that $\det\left(\left[ \begin{array}{ccc} a&b&c\\d&e&f\\g&h&i\end{array} \right] \right) = m$. 
		\begin{enumerate}[i.]
		\item Multiplying a row by a scalar multiples the determinant by that scalar, so 
\[\det\left( \left[ \begin{array}{ccc} a&b&c\\2d&2e&2f\\g&h&i\end{array} \right] \right) = 2m.\]

		\item Interchanging two rows multiples the determinant by $-1$. It takes two row swaps in the original matrix to obtain this one, so 
\[\det\left( \left[ \begin{array}{ccc} d&e&f\\g&h&i\\a&b&c\end{array} \right] \right) = (-1)^2m = m.\]

		\item Adding a multiple of a row to another does not change the determinant of the matrix. Since there is a row swap needed to get this matrix from the original we have  
\[\det\left(\left[ \begin{array}{ccc} a&b&c\\g-2d&h-2e&i-2f\\a+d&b+e&c+f \end{array} \right]\right) = -m.\]

		\end{enumerate}

	\ea
	
\end{example}

\begin{example} Let $A = \left[ \begin{array}{ccr} 2&8&0\\2&2&-3\\1&2&7 \end{array} \right]$.
	\ba
	\item Find an LU factorization for $A$.
	
	\item Use the LU factorization with forward substitution and back substitution to solve the system $A \vx = [18 \ 3 \ 12]^{\tr}$. 
	
	\ea

\ExampleSolution
\ba
\item We row reduce $A$ to an upper triangular matrix by applying elementary matrices. First notice that if $E_1 = \left[ \begin{array}{rcc} 1&0&0 \\-1&1&0 \\ 0&0&1 \end{array} \right]$, then 
\[E_1 A = \left[ \begin{array}{crr} 2&8&0 \\ 0&-6&-3 \\ 1&2&7 \end{array} \right].\]
Letting $E_2 =  \left[\renewcommand{\arraystretch}{1.4} \begin{array}{rcc} 1&0&0 \\0&1&0 \\ -\frac{1}{2}&0&1 \end{array} \right]$ gives us
\[E_2E_1A = \left[ \begin{array}{crr} 2&8&0 \\ 0&-6&-3 \\ 0&-2&7 \end{array} \right].\]
Finally, when $E_3 = \left[\renewcommand{\arraystretch}{1.4} \begin{array}{crc} 1&0&0 \\0&1&0 \\ 0&-\frac{1}{3}&1 \end{array} \right]$ we have 
\[U=E_3E_2E_1A = \left[ \begin{array}{crr} 2&8&0 \\ 0&-6&-3 \\ 0&0&8 \end{array} \right].\]
This gives us $E_3E_2E_1A = U$, so we can take
\[L = E_1^{-1}E_2^{-1}E_3^{-1} =  \left[\renewcommand{\arraystretch}{1.4} \begin{array}{ccc} 1&0&0 \\1&1&0 \\ \frac{1}{2}&\frac{1}{3}&1 \end{array} \right].\]

	\item  To solve the system $A \vx = \vb$, where $\vb = [18 \ 3 \ 12]^{\tr}$, we use the LU factorization of $A$ and solve $LU \vx = \vb$. Let $\vx = [x_1 \ x_2 \ x_3]^{\tr}$ and let $\vz = [z_1 \ z_2 \ z_3]^{\tr}$ with $U \vx = \vz$ so that $L \vz = L(U\vx) = A\vx = \vb$. First we solve $L \vz = [18 \ 3 \ 12]^{\tr}$ to find $\vz$ using forward substitution. The first row of $L$ shows that $z_1 = 18$ and the second row that $z_1 + z_2 = 3$. So $z_2 = -15$. The third row of $L$ gives us $\frac{1}{2}z_1 + \frac{1}{3}z_2 + z_3 = 12$, so $z_3 = 12 - 9 + 5 = 8$. Now to find $\vx$ we solve $U \vx = \vz$ using back substitution. The third row of $U$ tells us that $8x_3 = 8$ or that $x_3 = 1$. The second row of $U$ shows that $-6x_2-3x_3 = -15$ or $x_2 =2$. Finally, the first row of $U$ gives us $2x_1+8x_2 = 18$, or $x_1 = 1$. So the solution to $A \vx = \vb$ is $\vx = [1 \ 2 \ 1]^{\tr}$. 
	
	\ea

\end{example}


\csection{Summary}

\begin{itemize}
\item The elementary row operations have the following effects on the determinant:
\ba \item If we multiply a row of a matrix by a constant $k$, then the determinant is multiplied by $k$.
\item If we swap two rows of a matrix, then the determinant changes sign.
\item If we add a multiple of a row of a matrix to another, the determinant does not change.
\ea
\item Each of the elementary row operations can be achieved by multiplication by elementary matrices. To obtain the elementary matrix corresponding to an elementary row operation, we perform the operation on the identity matrix.
\item Let $A$ be an $n\times n$ invertible matrix. For any $\vb$ in $\R^n$, the solution $\vx$ of $A\vx=\vb$ has entries
\[ x_i =\frac{\det(A_i(\vb))}{\det(A)} \]
where $A_i(\vb)$ represents the matrix formed by replacing $i$th column of $A$ with $\vb$.
\item Let $A$ be an invertible $n\times n$ matrix. Then 
\[ A^{-1} = \frac{1}{\det(A)} \text{adj } A \]
where the $\text{adj } A$ matrix, the \emph{adjugate of $A$}, is defined as the matrix whose $ij$-th entry is $C_{ji}$, the $ji$-th cofactor of $A$.
\item For a $2\times 2$ matrix $A$, the area of the image of the unit square under the transformation $T(\vx)=A\vx$ is equal to $|\det(A)|$, which is also equal to the area of the parallelogram defined by the columns of $A$. 
\item For a $3\times 3$ matrix $A$, the volume of the image of the unit cube under the transformation $T(\vx)=A\vx$ is equal to $|\det(A)|$, which is also equal to the volume of the parallelepiped defined by the columns of $A$. 
\item An $LU$ factorization of a square matrix $A$ consists of a lower triangular matrix $L$ and an upper triangular matrix $U$ so that $A = LU$. 
\item A square matrix $A$ has an $LU$ factorization if we can use row operations without row interchanges to row reduce $A$ to an upper triangular matrix $U$. In this situation the elementary matrices that perform the row operations produce a lower triangular matrix $L$ so that $A = LU$. If $A$ cannot be reduced to an upper triangular matrix $U$ without row interchanges, then we can factor $A$ in the form $PLU$, where $L$ is a lower triangular matrix, $U$ is an upper triangular matrix, and $P$ is obtained from the identity matrix by appropriate row interchanges.  
\item There are many instances where we have a number of systems to solve of the form $A \vx = \vb$, all with the same coefficient matrix but where the vectors $\vb$ can change. With an $LU$ factorization, we can keep track of the row operations in one row reduction and save ourselves a significant amount of time when solving these systems. 
\end{itemize}

\csection{Exercises}

\be
\item \label{ex:4_f_det_multiple} Find a formula for $\det(rA)$ in terms of $r$ and $\det(A)$, where $A$ is an $n\times n$ matrix and $r$ is a scalar. Explain why your formula is valid.

\item Find $\det(A)$ by hand using elementary row operations where
\[ A= \left[ \begin{array}{rrrr} 1&2&-1&3\\ -1&-2&3&-1\\ -2&-1&2&-3\\ 1&8&-3&8 \end{array} \right] \, .\]

\item Consider the matrix $A= \left[ \begin{array}{rrrr} 4&-1&-1&-1 \\ -1&4&-1&-1\\ -1&-1&4&-1\\ -1&-1&-1&4 \end{array} \right]$. We will find $\det(A)$ using elementary row operations. (This matrix arises in graph theory, and its determinant gives the number of spanning trees in the complete graph with 5 vertices. This number is also equal to the number of labeled trees with 5 vertices.)

\ba 
\item Add rows $R_2$, $R_3$ and $R_4$ to the first row in that order.



\item Then add the new $R_1$ to rows $R_2$, $R_3$ and $R_4$ to get a triangular matrix $B$.


\item Find the determinant of $B$. Then use $\det(B)$ and properties of how elementary row operations affect determinants to find $\det(A)$.



\item Generalize your work to find the determinant of the $n\times n$ matrix
\[ A= \left[ \begin{array}{rrrrrr} n&-1&-1&\cdots &-1&-1 \\ -1&n&-1&\cdots &-1&-1\\ \vdots &\vdots &\vdots &\cdots & \vdots & \vdots\\ -1&-1&-1& \cdots&-1&n \end{array} \right] \, .\]

\ea

\item For which matrices $A$, if any, is $\det(A)=-\det(-A)$? Justify your answer. 

\item Find the inverse $A^{-1}$ of $A=\left[ \begin{array}{ccc} 1&0&1 \\ 0&1&0\\2&0&1\end{array} \right]$ using the adjugate matrix.

\item For an invertible $n\times n$ matrix $A$, what is the relationship between $\det(A)$ and $\det(\text{adj }A)$? Justify your result. 

\item Let $A = \left[ \begin{array}{ccc} a&b&1\\c&d&2\\e&f&3 \end{array} \right]$, and assume that $\det(A) = 2$. Determine the determinants of each of the following.
\ba
\item $B= \left[ \begin{array}{ccc} a&b&1\\3c&3d&6\\e+a&f+b&4 \end{array} \right] $

\item $C =  \left[ \begin{array}{ccr} 2e&2f&6 \\2c-2e&2d-2f&-2\\2a&2b&2 \end{array} \right]$

\ea

\item Find the area of the parallelogram with one vertex at the origin and adjacent vertices at $(1,2)$ and $(a,b)$. For which $(a,b)$ is the area 0? When does this happen geometrically?

\item Find the volume of the parallelepiped with one vertex at the origin and three adjacent vertices at $(3,2,0)$, $(1,1,1)$ and $(1,3,c)$ where $c$ is unknown. For which $c$, is the volume 0? When does this happen geometrically?

\item Find an $LU$ factorization of each of the following matrices $A$. Use the $LU$ factorization to solve the system $A \vx = \vb$ for the given vector $\vb$.
	\ba
	\item $A = \left[ \begin{array}{rcc} 2&1&3 \\ -1&0&1 \\ 2&1&5 \end{array} \right]$, $\vb = \left[ \begin{array}{c} 1 \\ 2 \\ 1 \end{array} \right]$
	
	\item $A = \left[ \begin{array}{rcc} 1&1&0 \\ -1&1&1 \\ 0&2&1 \end{array} \right]$, $\vb = \left[ \begin{array}{c} 1 \\ 2 \\ 1 \end{array} \right]$
	
	\item $A = \left[ \begin{array}{rcrc} 1&1&1&0 \\ 1&0&1&1 \\ -1&1&0&0 \\ 0&1&-1&0 \end{array} \right]$, $\vb = \left[ \begin{array}{c} 3 \\ 0 \\ 2 \\ 0 \end{array} \right]$
	
	\ea

\item Let $A = \left[ \begin{array}{cc} 1&2\\2&1 \end{array} \right]$.
	\ba
	\item Find an $LU$ decomposition of $A$.
	\item Find a different factorization of $A$ into a product $L'U'$ where $L'$ is a lower triangular matrix different from $L$ and $U'$ is an upper triangular matrix different from $U$. Conclude that the $LU$ decomposition of a matrix is not unique. 
	\ea

\item Let $A = \left[ \begin{array}{rcc} 1&2&3\\-1&2&0 \\ 2&0&0\end{array} \right]$.
	\ba
	\item Find an $LU$ decomposition of $A$.
	\item Find an $LU$ decomposition of $A$ in which the diagonal entries of $D$ are all 1. (Hint: Continue row reducing.)
	\item Find an upper triangular matrix $L$ whose diagonal entries are all 1, a lower triangular matrix $U$ whose diagonal entries are all 1, and a diagonal matrix $D$ such that $A = LDU$.
	\ea
	
		
\item Label each of the following statements as True or False. Provide justification for your response.
\ba
\item \textbf{True/False} If two rows are equal in $A$, then $\det(A)=0$.

\item \textbf{True/False} If $A$ is a square matrix and $R$ is a row echelon form of $A$, then $\det(A) = \det(R)$. 

\item \textbf{True/False} If a matrix $A$ is invertible, then 0 is not an eigenvalue of $A$.

\item \textbf{True/False} If $A$ is a $2\times 2$ matrix for which the image of the unit square under the transformation $T(\vx)=A\vx$ has zero area, then $A$ is non-invertible.

\item \textbf{True/False} Row operations do not change the determinant of a square matrix. 

\item \textbf{True/False} If $A_{ij}$ is the matrix obtained from a square matrix $A = [a_{ij}]$ by deleting the $i$th row and $jth$ column of $A$, then 
\begin{align*}
a_{i1}(-1)^{i+1}&\det(A_{i1}) + a_{i2}(-1)^{i+2}\det(A_{i2}) + \cdots \\
	&\qquad + a_{in}(-1)^{i+n}\det(A_{in}) \\
	&=  a_{1j}(-1)^{j+1}\det(A_{1j}) + a_{2i}(-1)^{j+2}\det(A_{2i}) + \cdots \\
	&\qquad + a_{nj}(-1)^{j+n}\det(A_{nj})
\end{align*}
for any $i$ and $j$ between $1$ and $n$.  

\item \textbf{True/False} If $A$ is an invertible matrix, then $\det\left(A^{\tr}A\right) > 0$.  

\ea

\ee
