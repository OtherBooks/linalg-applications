\achapter{35}{Inner Product Spaces} \label{sec:inner_products}

\vspace*{-17 pt}
\framebox{
\parbox{\dimexpr\linewidth-3\fboxsep-3\fboxrule}
{\begin{fqs}
\item What is an inner product space?
\item What is an orthogonal set in an inner product space? 
\item What is an orthogonal basis for an inner product space?  
\item How do we find the coordinate vector for a vector in an inner product space relative to an orthogonal basis for the space? 
\item What is the projection of a vector orthogonal to a subspace and why are such orthogonal projections important?
\end{fqs}}}% \hspace*{3 pt}}

\vspace*{13 pt}

\csection{Application: Fourier Series}

In calculus, a Taylor polynomial for a function $f$ is a polynomial approximation that fits $f$ well around the center of the approximation. For this reason, Taylor polynomials are good \emph{local} approximations, but they are not in general good global approximations. In particular, if a function $f$ has periodic behavior it is impossible to model $f$ well globally with polynomials that have infinite limits at infinity. For these kinds of functions, trigonometric polynomials are better choices. Trigonometric polynomials lead us to Fourier series, and we will investigate how inner products allow us to use trigonometric polynomials to model musical tones later in this section. 


\csection{Introduction}

In Section \ref{sec:dot_product} we were introduced to inner products in $\R^n$. The concept of an inner product can be extended to vector spaces, as we will see in this section. This will allow us to measure lengths and angles between vectors and define orthogonality in certain vector spaces. 

Recall that an inner product on $\R^n$ assigns to each pair of vectors $\vu$ and $\vv$ the scalar $\langle \vu, \vv \rangle$. Thus, an inner product on $\R^n$ defines a mapping from $\R^n \times \R^n$ to $\R$. Recall also that an inner product on $\R^n$ is commutative, distributes over vector addition, and respects scalar multiplication, and the inner product of a vector in $\R^n$  by itself is always non-negative and is equal to 0 only when the vector is the zero vector. We will investigate this ideas in vector spaces in Preview Activity \ref{pa:6_c}.  

\begin{pa} \label{pa:6_c} Consider the vector space $\pol_2$ of polynomials of degree less than or equal to $2$ with real coefficients. Define a mapping from $\pol_2 \times \pol_2$ to $\R$ by
\[\langle p(t), q(t) \rangle = \int_0^1 p(t)q(t) \, dt.\]
 
\be
\item Calculate $\langle 1, t \rangle$. 
	
\item If $p(t)$ and $q(t)$ are in $\pol_2$, is it true that 
	\[ \langle p(t) , q(t) \rangle = \langle q(t), p(t) \rangle?\]
	Verify your answer.
	
\item If $p(t)$, $q(t)$, and $r(t)$ are in $\pol_2$, is it true that 
	\[\langle p(t)+q(t), r(t) \rangle = \langle p(t), r(t) \rangle + \langle q(t), r(t) \rangle?\]
	Verify your answer.
	
\item If $p(t)$ and $q(t)$ are in $\pol_2$ and $c$ is a scalar, is it true that 
\[\langle cp(t) , q(t) \rangle = c\langle p(t) , q(t) \rangle?\]
Verify your answer. 

\item If $p(t)$ is in $\pol_2$, must it be the case that $\langle p(t) , p(t) \rangle \geq 0$? When is $\langle p(t) , p(t) \rangle = 0$?
 
\ee

\end{pa}

 
\csection{Inner Product Spaces}

As we saw in Preview Activity \ref{pa:6_c}, we can define a mapping from $\pol_2 \times \pol_2$ to $\R$ that has the same properties of inner products on $\R^n$. So we can extend the definition of inner product to arbitrary vector spaces.  

\begin{definition} \label{def:6_c_inner_product}  An \textbf{inner product}\index{inner product} $\langle \ , \ \rangle$ on a vector space $V$ is a mapping from $V \times V \to \R$ satisfying
\begin{enumerate}
\item $\langle \vu , \vv \rangle = \langle \vv , \vu \rangle$ for all $\vu$ and $\vv$ in $V$,
\item $\langle \vu + \vv , \vw \rangle = \langle \vu , \vw \rangle + \langle \vv , \vw \rangle$ for all $\vu$, $\vv$, and $\vw$ in $V$,
\item $\langle c\vu , \vv \rangle = c\langle \vu , \vv \rangle$ for all $\vu$, $\vv$ in $V$ and all scalars $c$,
\item $\langle \vu , \vu \rangle \geq 0$ for all $\vu$ in $V$ and $\langle \vu , \vu \rangle = 0$ if and only if $\vu = \vzero$.
\end{enumerate}
An \textbf{inner product space}\index{inner product space} is a vector space on which an inner product is defined.
\end{definition} 

 \begin{activity} Consider the mapping $\langle \ , \ \rangle $ from $\pol_1 \times \pol_1$ to $\R$ defined by 
\[\langle p(t), q(t) \rangle = p'(0)q'(0).\]
\ba
\item Show that this mapping satisfies the second property of an inner product.

\item Although this mapping satisfies the first three properties of an inner product, show that this mapping does not satisfy the fourth property and so is not an inner product. 
\ea

\end{activity}


Since inner products in vector spaces are defined in the same way as inner products in $\R^n$, they will satisfy the same properties. Some of these properties are summarized in the following theorem.

\begin{theorem} \label{thm:6_c_inner_product_properties} Let $\langle \ , \ \rangle$ be an inner product on $\R^n$ and let $\vu, \vv$, and $\vw$ be vectors in $\R^n$ and $c$ a scalar. Then
\begin{enumerate}
	\item $\langle \vzero , \vv \rangle = \langle \vv , \vzero \rangle = 0$
	\item $\langle \vu , c\vv \rangle = c\langle \vu , \vv \rangle$
	\item $\langle \vu , \vv+\vw \rangle = \langle \vu , \vv \rangle + \langle \vu , \vw \rangle$
\item $\langle \vu - \vv, \vw \rangle = \langle \vu , \vw \rangle - \langle \vv , \vw \rangle$
\end{enumerate}
\end{theorem}

One special inner product is indicated in Preview Activity \ref{pa:6_c}. Recall that $C[a,b]$ is the vector space of continuous functions on the closed interval $[a,b]$. Let $\langle \ ,  \ \rangle$ map from $C[a,b] \times C[a,b]$ to $\R$ be defined by 
\[\langle f(x), g(x) \rangle = \int_a^b f(x)g(x) \, dx.\]
The verification that this mapping is an inner product is left to the exercises. 

\csection{The Length of a Vector}

We can use inner products to define the length of any vector in an inner product space and the distance between two vectors in an inner product space. The idea comes right from the relationship between lengths of vectors in $\R^n$ and inner products on $\R^n$ (compare to Definition \ref{def:6_a_length_Rn}). 

\begin{definition} Let $\vv$ be a vector in an inner product space. The \textbf{length}\index{length of a vector in an inner product space} of $\vv$ is the real number
\[||\vv|| = \sqrt{\langle \vv, \vv \rangle}.\]
\end{definition}

The length of a vector in a vector space is also called \emph{magnitude} or \emph{norm}. Just as with inner products on $\R^n$ we can use the notion of length to define unit vectors in inner product spaces (compare to Definition \ref{def:6_a_unit_vector}).  

\begin{definition} A vector $\vv$ in an inner product space is a \textbf{unit vector}\index{unit vector in an inner product space} if $||\vv|| = 1$.
\end{definition}

We can find a unit vector in the direction of a nonzero vector $\vv$ in an inner product space by dividing by the norm of the vector. That is, the vector $\ds \frac{\vv}{||\vv||}$ is a unit vector in the direction of the vector $\vv$, provided that $\vv$ is not zero. 

We define the distance between vectors $\vu$ and $\vv$ in an inner product space  in the same way we defined distance using the dot product (compare to Definition \ref{def:6_a_distance}).

\begin{definition} Let $\vu$ and $\vv$ be vectors in an inner product space. The \textbf{distance between}\index{distance between vectors in an inner product space} $\vu$ and $\vv$ is the length of the difference $\vu - \vv$ or
\[|| \vu - \vv ||.\]
\end{definition}


\begin{activity} \label{act:6_c_Frobenius} The \emph{trace} (see Definition \ref{def:trace}) of an $n \times n$ matrix $A = [a_{ij}]$ is the sum of the diagonal entries of $A$. That is,
\[\trace(A) = a_{11} + a_{22} + \cdots + a_{nn}.\]
If $A$ and $B$ are in the space $\M_{n \times n}$ of $n \times n$ matrices with real entries, we define the mapping $\langle  \ , \ \rangle$ from $\M_{n \times n} \times \M_{n \times n}$ to $\R$ by  
\[\langle A, B \rangle = \trace\left(AB^{\tr}\right).\]
This mapping is an inner product on the space $\M_{n \times n}$ called the \emph{Frobenius} inner product (details are in the exercises). Let $A = \left[ \begin{array}{cc} 1&0\\2&2 \end{array} \right]$ and $B = \left[ \begin{array}{rc} -3&4\\-2&1 \end{array} \right]$ in $\M_{2 \times 2}$
\ba
\item Find the length of the vectors $A$ and $B$ using the Frobenius inner product. 
 
 \item Find the distance between $A$ and $B$ using the Frobenius inner product. 
 
 \ea

\end{activity}




\csection{Orthogonality in Inner Product Spaces}

We defined orthogonality in $\R^n$ using inner products in $\R^n$ (see Definition \ref{def:6_a_orthogonal_dot_product}) and the angle between vectors. We can extend those ideas to any inner product space. 

If $\vu$ and $\vv$ are nonzero vectors in an inner product space, then the \textbf{angle} $\theta$ \textbf{between}\index{angle between vectors} $\vu$ \textbf{and} $\vv$ is such that 
\[\cos(\theta) = \frac{\langle \vu, \vv \rangle}{||\vu||\, ||\vv||}.\]
and $0\leq \theta \leq \pi$. This angle is well-defined due to the Cauchy-Schwarz inequality $|\langle \vu, \vv \rangle| \leq ||\vu||\, ||\vv||$ whose proof is left to the exercises.

With the angle between vectors in mind, we can define orthogonal vectors in an inner product space. 

\begin{definition} Vectors $\vu$ and $\vv$ in an inner product space are \textbf{orthogonal}\index{orthogonal vectors in inner product spaces} if
\[\langle \vu, \vv \rangle = 0.\]
\end{definition}
 
Note that this defines the zero vector to be orthogonal to every vector.

\begin{activity} In this activity we use the Frobenius inner product (see Activity \ref{act:6_c_Frobenius}). Let $A = \left[ \begin{array}{cc} 0&1\\1&0 \end{array} \right]$ and $B = \left[ \begin{array}{cc} 0&1\\0&0 \end{array} \right]$ in $\M_{2 \times 2}$.
	\ba
	\item Find a nonzero vector in $\M_{2 \times 2}$ orthogonal to $A$.
	
	
	\item Find the angle between $A$ and $B$. 

	\ea
\end{activity}

Using orthogonality we can generalize the notions of orthogonal sets and bases, orthonormal bases and orthogonal complements we defined in $\R^n$ to all inner product spaces in a natural way.

\csection{Orthogonal and Orthonormal Bases in Inner Product Spaces}

As we did with inner products in $\R^n$, we define an orthogonal set to be one in which all of the vectors in the set are orthogonal to each other  (compare to Definition \ref{def:6_b_orthogonal_set_Rn}).

\begin{definition} A subset $S$ of an inner product space for which $\langle \vu, \vv \rangle = 0$ for all $\vu \neq \vv$ in $S$ is called an \textbf{orthogonal set}\index{orthogonal set in an inner product space}. 
\end{definition}

As in $\R^n$, an orthogonal set of nonzero vectors is always linearly independent. The proof is similar to that of Theorem  \ref{thm:6_b_Orth_li} and is left to the exercises. 

\begin{theorem} \label{thm:6_c_Orth_li_ips} Let $\{\vv_1, \vv_2, \ldots, \vv_m\}$ be a set of nonzero orthogonal vectors in an inner product space. Then the vectors $\vv_1$, $\vv_2$, $\ldots$, $\vv_m$ are linearly independent.
\end{theorem}


A basis that is also an orthogonal set is given a special name (compare to Definition \ref{def:6_b_orthogonal_basis_Rn}).

\begin{definition} An \textbf{orthogonal basis}\index{orthogonal basis in an inner product space} $\CB$ for a subspace $W$ of an inner product space is a basis of $W$ that is also an orthogonal set.
\end{definition}

Using inner products in $\R^n$, we saw that the representation of a vector as a linear combination of vectors in an orthogonal basis was quite elegant. The same is true in any inner product space. To see this, let $\CB = \{\vv_1, \vv_2, \ldots, \vv_m\}$ be an orthogonal basis for a subspace $W$ of an inner product space and let $\vx$ be any vector in $W$. We know that
\[\vx = x_1\vv_1 + x_2\vv_2 + \cdots + x_m \vv_m\]
for some scalars $x_1$, $x_2$, $\ldots$, $x_m$. If $1\leq k\leq m$, then, using inner product properties and the orthogonality of the vectors $\vv_i$, we have
\[\langle \vv_k, \vx \rangle = x_1 \langle \vv_k, \vv_1 \rangle + x_2 \langle \vv_k, \vv_2 \rangle + \cdots + x_m \langle \vv_k, \vv_m \rangle = x_k \langle \vv_k, \vv_k \rangle.\]
So
\[x_k = \ds \frac{\langle \vx, \vv_k \rangle}{\langle \vv_k,  \vv_k \rangle}.\]
Thus, we can calculate each weight individually with two simple inner product calculations. 

%In other words, the coordinate vector $[\vx]_{\CB}$ of $\vx$ in an inner product space with orthogonal basis $\CB = \{\vv_1, \vv_2, \ldots, \vv_m\}$ is given by 
%\[[\vx]_{\CB} = \left[ \begin{array}{c} \frac{\langle \vx, \vv_1 \rangle}{\langle \vv_1, \vv_1 \rangle} \\ \frac{\langle \vx, \vv_2 \rangle}{\langle \vv_2, \vv_2 \rangle} \\ \vdots \\ \frac{\langle \vx, \vv_m \rangle}{\langle \vv_m, \vv_m \rangle} \end{array} \right].\]

We summarize this discussion in the next theorem (compare to Theorem \ref{thm:6_b_orth_dcomp}).

\begin{theorem} \label{thm:6_c_orth_dcomp} Let $\CB = \{\vv_1, \vv_2, \ldots, \vv_m\}$ be an orthogonal basis for a subspace of an inner product space. Let $\vx$ be a vector in $W$. Then
\begin{equation}
\vx = \ds \frac{\langle \vx, \vv_1 \rangle}{\langle \vv_1, \vv_1 \rangle} \vv_1 +  \frac{\langle \vx, \vv_2 \rangle}{\langle \vv_2, \vv_2 \rangle} \vv_2 + \cdots + \frac{\langle \vx, \vv_m \rangle}{\langle \vv_m, \vv_m \rangle} \vv_m. \label{eq:6_c_orth_decomp_ips}
\end{equation}
\end{theorem}


\begin{activity} Let $p_1(t) = 1-t$, $p_2(t) = -2+4t+4t^2$, and $p_3(t) = 7-41t+40t^2$ be vectors in the inner product space $\pol_2$ with inner product defined by $\langle p(t), q(t) \rangle = \int_0^1 p(t)q(t) \ dt$. Let $\CB = \{p_1(t), p_2(t), p_3(t)\}$. You may assume that $\CB$ is an orthogonal basis for $\pol_2$. Let $z(t) = 4-2t^2$. Find the weight $x_3$ so that $z(t) = x_1p_1(t) + x_2 p_2(t) + x_3 p_3(t)$. Use technology as appropriate to evaluate any integrals. 

\end{activity}


The decomposition (\ref{eq:6_c_orth_decomp_ips}) is even simpler if $\langle \vv_k, \vv_k \rangle = 1$ for each $k$. Recall that
\[\langle \vv, \vv \rangle = || \vv ||^2,\]
so the condition $\langle \vv, \vv \rangle  = 1$ implies that the vector $\vv$ has norm 1. As with inner products in $\R^n$, an orthogonal basis with this additional condition is given a special name (compare to Definition \ref{def:6_b_orthonormal_basis}).

\begin{definition} An \textbf{orthonormal basis}\index{orthonormal basis in an inner product space} $\CB = \{\vv_1, \vv_2, \ldots, \vv_m\}$ for a subspace $W$ of an inner product space is an orthogonal basis such that $|| \vv_k || = 1$ for $1\leq k\leq m$.
\end{definition}

If $\CB = \{\vv_1, \vv_2, \ldots, \vv_m\}$ is an orthonormal basis for a subspace $W$ of an inner product space and $\vx$ is a vector in $W$, then  (\ref{eq:6_c_orth_decomp_ips}) becomes
\begin{equation}
\vx = \langle \vx, \vv_1 \rangle \vv_1 +  \langle \vx, \vv_2 \rangle \vv_2 + \cdots + \langle \vx, \vv_m \rangle \vv_m. \label{eq:6_c_orthnorm_decomp_ips}
\end{equation}

Recall that we can construct an orthonormal basis from an orthogonal basis by dividing each basis vector by its magnitude.


\csection{Orthogonal Projections onto Subspaces}

In Section \ref{sec:gram_schmidt} we saw how to project a vector $\vv$ in $\R^n$ onto a subspace $W$ of $\R^n$. The same process works for vectors in any inner product space.

\begin{definition} Let $W$ be a subspace of an inner product space $V$ and let $\CB = \{\vw_1, \vw_2, \ldots, \vw_m\}$ be an orthogonal basis for $W$. For a vector $\vv$ in $V$, the \textbf{orthogonal projection of $\vv$ onto $W$}\index{orthogonal projection onto a subspace} is the vector
\[\proj_W \vv =  \frac{\langle \vv, \vw_1 \rangle }{\langle \vw_1, \vw_1 \rangle } \vw_1 + \frac{\langle \vv, \vw_2 \rangle }{\langle \vw_2, \vw_2 \rangle }  \vw_2 + \cdots + \frac{\langle \vv, \vw_m \rangle}{\langle \vw_m, \vw_m \rangle} \vw_m.\] 
The \textbf{projection of $\vv$ orthogonal to $W$}\index{projection orthogonal to a subspace} is the vector
\[\proj_{W^{\perp}} \vv = \vv - \proj_W \vv.\]
\end{definition}

The notation $\proj_{W^{\perp}} \vv$ indicates that we expect this vector to be orthogonal to every vector in $W$. 

\begin{activity} \label{act:6_c_orth_projection} In Section \ref{sec:gram_schmidt} we showed that in the inner product space $\R^n$ using the dot product as inner product, if $W$ is a subspace of $\R^n$ and $\vv$ is in $\R^n$, then $\proj_{W^{\perp}} \vv$ is orthogonal to every vector in $W$. In this activity we verify that same fact in an inner product space. That is, assume that $\CB = \{\vw_1, \vw_2, \ldots, \vw_m\}$ is an orthogonal basis for a subspace $W$ of an inner product space $V$ and $\vv$ is a vector in $V$. Follow the indicated steps to show that $\proj_{W^{\perp}} \vv$ is orthogonal to every vector in $\CB$. 
\ba
\item Let $\vz$ be the projection of $\vv$ onto $W$. Write $\vz$ in terms of the basis vectors in $\CB$. 

\item The vector $\vv - \vz$ is the projection of $\vv$ orthogonal to $W$. Let $k$ be between $1$ and $m$. Use the result of part (a) to show that $\vv - \vz$ is orthogonal to $\vw_k$. Exercise  \ref{ex:6_c_orth_complement_basis} then shows that $\vv - \vz$ is orthogonal to every vector in $W$. 

%\item Explain how the result of (b) shows that $\vv - \vz$ is orthogonal to every vector on $W$. 

\ea

\end{activity}

Activity \ref{act:6_c_orth_projection} shows that the vector $\vv - \vw$ is orthogonal to vector for $W$. So, in fact, $\proj_{W^\perp} \vv$ is the projection of $\vv$ onto the orthogonal complement of $W$, which will be defined shortly. 
 
 \csection{Best Approximations in Inner Product Spaces}

We have seen, e.g., linear regression to fit a line to a set of data, that we often want to find a vector in a subspace that ``best" approximates a given vector in a vector space. As in $\R^n$, the projection of a vector onto a subspace has this important property.  That is, $\proj_W \vv$ is the vector in $W$ closest to $\vv$ and therefore the best approximation of $\vv$ by a vector in $W$. To see that this is true in any inner product space, we first need a generalization of the Pythagorean Theorem that holds in inner product spaces.

\begin{theorem}[Generalized Pythagorean Theorem] \label{thm:6_c_PT} Let $\vu$ and $\vv$ be orthogonal vectors in an inner product space $V$. Then
\[||\vu - \vv||^2 = ||\vu||^2 + ||\vv||^2.\]
\end{theorem}

\begin{proof} Let $\vu$ and $\vv$ be orthogonal vectors in an inner product space $V$. Then
\begin{align*}
|| \vu - \vv ||^2 &= \langle \vu-\vv, \vu-\vv \rangle \\
	&= \langle \vu,\vu \rangle - 2 \langle \vu,\vv \rangle + \langle \vv, \vv \rangle \\
	&= \langle \vu,\vu \rangle - 2 (0) + \langle \vv, \vv \rangle \\
	&= ||\vu||^2 + ||\vv||^2.
\end{align*}
\end{proof}
Note that replacing $\vv$ with $-\vv$ in the theorem also shows that $||\vu + \vv||^2 = ||\vu||^2 + ||\vv||^2$ if $\vu$ and $\vv$ are orthogonal.

Now we will prove that the projection of a vector $\vu$ onto a subspace $W$ of an inner product space $V$ is the best approximation in $W$ to the vector $\vu$.

\begin{theorem} \label{thm:6_c_best_approx} Let $W$ be a subspace of an inner product space $V$ and let $\vu$ be a vector in $V$. Then
\[||\vu - \proj_W \vu || < || \vu - \vx ||\]
for every vector $\vx$ in $W$ different from $\proj_W \vu$.
\end{theorem}

\begin{proof} Let $W$ be a subspace of an inner product space $V$ and let $\vu$ be a vector in $V$. Let $\vx$ be a vector in $W$. Now
\[\vu - \vx = (\vu-\proj_W \vu) + (\proj_W \vu - \vx).\]
Since both $\proj_W \vu$ and $\vx$ are in $W$, we know that $\proj_W \vu - \vx$ is in $W$. Since $\proj_{W^{\perp}} \vu = \vu-\proj_W \vu$ is orthogonal to every vector in $W$, we have that $\vu-\proj_W \vu$ is orthogonal to $\proj_W \vu - \vx$. We can now use the Generalized Pythagorean Theorem to conclude that
\[||\vu - \vx||^2 = ||\vu-\proj_W \vu||^2 + ||\proj_W \vu - \vx||^2.\]
Since $\vx \neq \proj_W \vu$, it follows that $||\proj_W \vu - \vx||^2 > 0$ and
\[|| \vu - \vx ||^2 > ||\vu - \proj_W \vu ||^2.\]
Since norms are nonnegative, we can conclude that $||\vu - \proj_W \vu || < || \vu - \vx ||$ as desired.
\end{proof}

Theorem \ref{thm:6_c_best_approx} shows that the distance from $\proj_W \vv$ to $\vv$ is less than the distance from any other vector in $W$ to $\vv$. So $\proj_W \vv$ is the best approximation to $\vv$ of all the vectors in $W$.

In $\R^n$ using the dot product as inner product, if $\vv = [v_1 \ v_2 \ v_3 \ \ldots \ v_n]^{\tr}$ and $\proj_W \vv =  [w_1 \ w_2 \ w_3 \ \ldots \ w_n]^{\tr}$, then the square of the error in approximating $\vv$ by $\proj_W \vv$ is given by
\[|| \vv - \proj_W \vv ||^2 = \sum_{i=1}^n (v_i - w_i)^2.\]
So $\proj_W \vv$ minimizes this sum of squares over all vectors in $W$. As a result, we call $\proj_W \vv$ the \emph{least squares approximation} to $\vv$.


\begin{activity} The set $\CB = \{1, t-\frac{1}{2}, t^3-\frac{9}{10}t+\frac{1}{5}\}$ is an orthogonal basis for a subspace $W$ of the inner product space $\pol_3$ using the inner product $\langle p(t), q(t) \rangle = \int_0^1 p(t)q(t) \ dt$. Find the polynomial in $W$ that is closest to the polynomial $r(t) = t^2$ and give a numeric estimate of how good this approximation is.


\end{activity}


\csection{Orthogonal Complements}

If we have a set of vectors $S$ in an inner product space $V$, we can define the orthogonal complement of $S$ as we did in $\R^n$ (see \ref{def:6_a_orth_complement}).

\begin{definition} The \textbf{orthogonal complement}\index{orthogonal complement in inner product spaces} of a subset $S$ of an inner product space $V$ is the set
\[S^{\perp} = \{\vv \in V : \langle \vv, \vu \rangle = 0 \text{ for all } \vu \in S\}.\]
\end{definition}

As we saw in $\R^n$, to show that a vector is in the orthogonal complement of a subspace, it is enough to show that the vector is orthogonal to every vector in a basis for that subspace. The same is true in any inner product space. The proof is left to the exercises. 

\begin{theorem} \label{thm:6_e_ip_orth_complement_basis} Let $\CB = \{\vw_1, \vw_2, \ldots, \vw_m\}$ be a basis for a subspace $W$ of an inner product space $V$. A vector $\vv$ in $V$ is orthogonal to every vector in $W$ if and only if $\vv$ is orthogonal to every vector in $\CB$.
\end{theorem}


\begin{activity} Consider $\pol_2$ with the inner product $\langle p(t), q(t) \rangle = \int_0^1 p(t)q(t) \ dt$.
\ba 
\item Find $\langle p(t), 1-t \rangle$ where $p(t)=a+bt+ct^2$ is in $\pol_2$.

\item Describe as best you can the orthogonal complement of $\Span\{1-t\}$ in $\pol_2$. Is $p(t)=1-2t-2t^2$ in this orthogonal complement? Is $p(t)=1+t-t^2$?
\ea

\end{activity}

As was the case in $\R^n$, give a subspace $W$ of an inner product space $V$, any vector in $V$ can be written uniquely as a sum of a vector in $W$ and a vector in $W^{\perp}$. 

\begin{activity} \label{act:6_c_decompose_ips} Let $V$ be an inner product space of dimension $n$, and let $W$ be a subspace of $V$. Let $\vx$ be any vector in $V$. We will demonstrate that $\vx$ can be written uniquely as a sum of a vector in $W$ and a vector in $W^{\perp}$. 
	\ba
	\item Explain why $\proj_W \vx$ is in $W$.
	
	\item Explain why $\proj_{W^{\perp}} \vx$ is in $W^{\perp}$. 

	\item Explain why $\vx$ can be written as a sum of vectors, one in $W$ and one in $W^{\perp}$. 

	\item Now we demonstrate the uniqueness of this decomposition. Suppose $\vx = \vw+\vw_1$ and $\vx = \vu+\vu_1$, where $\vw$ and $\vu$ are in $W$ and $\vw_1$ and $\vu_1$ are in $W^{\perp}$. Show that $\vw=\vu$ and $\vw_1 = \vu_1$, so that the representation of $\vx$ as a sum of a vector in $W$ and a vector in $W^{\perp}$ is unique. (Hint: What is $W \cap W^{\perp}$?)


	\ea
	
\end{activity}

We summarize the result of Activity \ref{act:6_c_decompose_ips}.

\begin{theorem} \label{thm:6_c_decompose_ips} Let $V$ be a finite dimensional inner product space, and let $W$ be a subspace of $V$. Any vector in $V$ can be written in a unique way as a sum of a vector in $W$ and a vector in $W^{\perp}$. 
\end{theorem}

Theorem \ref{thm:6_c_decompose_ips} is useful in many applications. For example, to compress an image using wavelets, we store the image as a collection of data, then rewrite the data using a succession of subspaces and their orthogonal complements.  This new representation allows us to visualize the data in a way that compression is possible. 

\csection{Examples}

\ExampleIntro

\begin{example} Let $V = \pol_3$ be the inner product space with inner product 
\[\langle p(t), q(t) \rangle = \int_{-1}^1 p(t)q(t) \ dt.\]
Let $p_1(t) = 1+t$, $p_2(t) = 1-3t$, $p_3(t) = 3t-5t^3$, and $p_4(t) = 1-3t^2$. 
\ba
\item Show that the set $\CB = \{p_1(t), p_2(t), p_3(t), p_4(t)\}$ is an orthogonal basis for $V$.

\item  Use  \ref{thm:6_c_orth_dcomp} to write the polynomial $q(t) = t^2+t^3$ as a linear combination of the basis vectors in $\CB$. 

\ea

\ExampleSolution All calculations are done by hand or with a computer algebra system, so we leave those details to the reader. 
\ba
\item If we show that the set $\CB$ is an orthogonal set, then Theorem \ref{thm:6_c_Orth_li_ips} shows that $\CB$ is linearly independent. Since $\dim(\pol_3) = 4$, the linearly independent set $\CB$ that contains four vectors must be a basis for $\pol_3$. 

To determine if the set $\CB$ is an orthogonal set, we must calculate the inner products of pairs of distinct vectors in $\CB$. Since $\langle 1+t,1-3t \rangle = 0$, $\langle 1+t,3t-5t^3 \rangle = 0$, $\langle 1+t,1-3t^2 \rangle = 0$, $\langle 1-3t,3t-5t^3 \rangle = 0$, $\langle 1-3t,1-3t^2 \rangle = 0$, and $\langle 3t-5t^3,1-3t^2 \rangle = 0$, we conclude that $\CB$ is an orthogonal basis for $\pol_3$. 

\item  We can write the polynomial $q(t) = 1+t+t^2+t^3$ as a linear combination of the basis vectors in $\CB$ as follows:
\begin{align*}
q(t) = \frac{\langle q(t),p_1(t) \rangle }{\langle p_1(t),p_1(t) \rangle} p_1(t) &+ \frac{\langle q(t),p_2(t) \rangle }{\langle p_2(t),p_2(t) \rangle} p_2(t) \\
	&+ \frac{\langle q(t),p_3(t) \rangle }{\langle p_3(t),p_3(t) \rangle} p_3(t) + \frac{\langle q(t),p_4(t) \rangle }{\langle p_4(t),p_4(t) \rangle} p_4(t).
\end{align*}
Now 
\[\begin{array}{ccc}
\langle q(t), p_1(t) \rangle = \frac{16}{15}, &\langle q(t), p_2(t) \rangle = -\frac{8}{15}, &\langle q(t), p_3(t) \rangle = -\frac{8}{35}, \\
\langle q(t), p_4(t) \rangle = -\frac{8}{15}, &\langle p_1(t), p_1(t)\rangle  = \frac{8}{3}, &\langle p_2(t), p_2(t) \rangle = 8, \\
\langle p_3(t), p_3(t) \rangle = \frac{8}{7}, &\langle p_4(t), p_4(t) \rangle = \frac{8}{5} & 
\end{array}\]
so
\begin{align*}
q(t) &= \frac{ \frac{16}{15} }{ \frac{8}{3} } p_1(t) - \frac{ \frac{8}{15} }{ 8 } p_2(t) -  \frac{ \frac{8}{35} }{ \frac{8}{7} } p_3(t) -  \frac{ \frac{8}{15} }{ \frac{8}{5} } p_4(t) \\
	&\frac{2}{5} p_1(t) - \frac{1}{15}p_2(t) - \frac{1}{5} p_3(t) - \frac{1}{3} p_4(t).
\end{align*}

\ea

\end{example}


\begin{example} Let $V$ be the inner product space $\R^4$ with inner product defined by 
\[\langle [u_1 \ u_2 \ u_3 \ u_4]^{\tr}, [v_1 \ v_2 \ v_3 \ v_4]^{\tr} \rangle = u_1v_1 + 2u_2v_2 + 3u_3v_3 + 4u_4v_4.\]
\ba
\item Let $W$ be the plane spanned by $[-1 \ 1\ 0 \ 1]^{\tr}$ and $[6 \ 1 \ 7 \ 1]^{\tr}$ in $V$. Find the vector in $W$ that is closest to the vector $[2 \ 0 \ -1 \ 3]^{\tr}$. Exactly how close is your best approximation to the vector $[2 \ 0 \ -1 \ 3]^{\tr}$?  

\item Express the vector $[2 \ 0 \ -1 \ 3]^{\tr}$ as the sum of a vector in $W$ and a vector orthogonal to $W$.

\ea

\ExampleSolution 
\ba
\item The vector we're looking for is the projection of $[2 \ 0 \ -1 \ 3]^{\tr}$ onto the plane. A spanning set for the plane is $\CB = \{[-1 \ 1 \ 0 \ 1]^{\tr}, [6 \ 1 \ 7 \ 1]^{\tr}\}$. Neither vector in $\CB$ is a scalar multiple of the other, so $\CB$ is a basis for the plane. Since
\[\langle [-1 \ 1 \ 0 \ 1]^{\tr}, [6 \ 1 \ 7 \ 1]^{\tr} \rangle = -6 + 2 + 0 + 4 =  0,\]
the set $\CB$ is an orthogonal basis for the plane. 

The projection of the vector $\vv = [2 \ 0 \ -1 \ 3]^{\tr}$ onto the plane spanned by $\vw_1 =  [-1 \ 1 \ 0 \ 1]^{\tr}$ and $\vw_2 = [6 \ 1 \ 7 \ 1]^{\tr}$ is given by 
\begin{align*}
\frac{\langle \vv, \vw_1\rangle}{\langle \vw_1, \vw_1\rangle } \vw_1 + \frac{\langle \vv, \vw_2 \rangle }{\langle \vw_2, \vw_2 \rangle}\vw_2 &= \frac{10}{7} [-1 \ 1\ 0 \ 1]^{\tr} + \frac{3}{189}  [6 \ 1 \ 7 \ 1]^{\tr} \\
	&= \frac{1}{189}[-252 \ 273 \ 21 \ 273]{\tr} \\
	&= \left[ -\frac{4}{3} \ \frac{13}{9} \ \frac{1}{9} \ \frac{13}{9} \right].
\end{align*}
To measure how close close $\left[ -\frac{4}{3} \ \frac{13}{9} \ \frac{1}{9} \ \frac{13}{9} \right]$ is to $[2 \ 0 \ -1 \ 3]^{\tr}$, we calculate
\begin{align*}
\left|\left| \left[ -\frac{4}{3} \ \frac{13}{9} \ \frac{1}{9} \ \frac{13}{9} \right]   - [2 \ 0 \ -1 \ 3]^{\tr} \right|\right| &= \left|\left| \left[-\frac{10}{3} \ \frac{13}{9} \ \frac{10}{9} \ -\frac{14}{9} \right] \right| \right| \\
%	&= \sqrt{\left\langle \left[-\frac{10}{3} \ \frac{13}{9} \ \frac{10}{9} \ -\frac{14}{9} \right], \left[-\frac{10}{3} \ \frac{13}{9} \ \frac{10}{9} \ -\frac{14}{9} \right] \right\rangle} \\
	&= \sqrt{ \frac{100}{9} + \frac{338}{81} + \frac{300}{81} + \frac{784}{81}} \\
	&= \frac{1}{9} \sqrt{2322} \\
	&\approx 5.35.
\end{align*} 

\item If $\vv = [2 \ 0 \ -1 \ 3]^{\tr}$, then $\proj_{W} \vv$ is in $W$ and 
\[\proj_{W^{\perp}} \vv = \vv - \proj_{W} \vv =  \left[-\frac{10}{3} \ \frac{13}{9} \ \frac{10}{9} \ -\frac{14}{9} \right]\]
is in $W^{\perp}$, and $\vv = \proj_{W} \vv + \proj_{W^{\perp}} \vv$. 

\ea

\end{example}

\csection{Summary}
\begin{itemize}
\item An inner product $\langle \ , \ \rangle$ on a vector space $V$ is a mapping from $V \times V \to \R$ satisfying
	\begin{enumerate}
	\item $\langle \vu , \vv \rangle = \langle \vv , \vu \rangle$ for all $\vu$ and $\vv$ in $V$,
	\item $\langle \vu + \vv , \vw \rangle = \langle \vu , \vw \rangle + \langle \vv , \vw \rangle$ for all $\vu$, $\vv$, and $\vw$ in $V$,
	\item $\langle c\vu , \vv \rangle = c\langle \vu , \vv \rangle$ for all $\vu$, $\vv$ in $V$ and $c \in \R$,
	\item $\langle \vu , \vu \rangle \geq 0$ for all $\vu$ in $V$ and $\langle \vu , \vu \rangle = 0$ if and only if $\vu = \vzero$.
	\end{enumerate}
\item An inner product space is a pair $V$, $\langle \ , \ \rangle$ where $V$ is a vector space and $\langle \ , \ \rangle$ is an inner product on $V$.
\item The length of a vector $\vv$ in an inner product space $V$ is defined to be the real number $||\vv|| = \sqrt{\langle \vv, \vv \rangle}$.
\item The distance between two vectors $\vu$ and $\vv$ in an inner product space $V$ is the scalar $|| \vu - \vv ||$.
\item The angle $\theta$ between two vectors $\vu$ and $\vv$ is the angle which satisfies $0\leq \theta \leq \pi$ and 
\[ \cos(\theta) = \frac{\langle \vu, \vv \rangle}{||\vu|| ||\vv||} \,.\]
\item Two vectors $\vu$ and $\vv$ in an inner product space $V$ are orthogonal if $\langle \vu, \vv \rangle = 0$.
\item A subset $S$ of an inner product space is an orthogonal set if  $\langle \vu, \vv \rangle = 0$ for all $\vu \neq \vv$ in $S$.
\item A basis for a subspace of an inner product space is an orthogonal basis if the basis is also an orthogonal set. 
\item Let $\CB = \{\vv_1, \vv_2, \ldots, \vv_m\}$ be an orthogonal basis for a subspace of an inner product space $V$. Let $\vx$ be a vector in $W$. Then
\[\vx = \sum_{i=1}^m c_i \vv_i,\]
where 
\[c_i = \frac{\langle \vx, \vv_i \rangle}{\langle \vv_i, \vv_i \rangle}\]
for each $i$. 
\item An orthogonal basis $\CB = \{\vv_1, \vv_2, \ldots, \vv_m\}$ for a subspace $W$ of an inner product space $V$ is an orthonormal basis if $|| \vv_k || = 1$ for each $k$ from 1 to $m$.
\item If $\CB = \{\vw_1, \vw_2, \ldots, \vw_m\}$ is an orthogonal basis for $V$ and $\vx \in V$, then
\[[\vx]_{\CB} = \left[ \begin{array}{c} \frac{\langle \vx, \vw_1 \rangle}{\langle \vw_1, \vw_1 \rangle} \\ \frac{\langle \vx, \vw_2 \rangle}{\langle \vw_2, \vw_2 \rangle} \\ \vdots \\ \frac{\langle \vx, \vw_m \rangle}{\langle \vw_m, \vw_m \rangle} \end{array} \right]. \]
\item The projection of the vector $\vv$ in an inner product space $V$ onto a subspace $W$ of $V$ is the vector
\[\proj_W \vv =  \frac{\langle \vv, \vw_1 \rangle }{\langle \vw_1, \vw_1 \rangle } \vw_1 + \frac{\langle \vv, \vw_2 \rangle }{\langle \vw_2, \vw_2 \rangle }  \vw_2 + \cdots + \frac{\langle \vv, \vw_m \rangle}{\langle \vw_m, \vw_m \rangle} \vw_m,\]
 where $\{\vw_1, \vw_2, \ldots, \vw_m\}$ is an orthogonal basis of $W$. Projections are important in that $\proj_W \vv$ is the best approximation of the vector $\vv$ by a vector in $W$ in the least squares sense.  
\item With $W$ as in (a), the projection of $\vv$ orthogonal to $W$ is the vector
\[\proj_{W^{\perp}} \vv = \vv - \proj_W \vv.\]
The norm of $\proj_{W^{\perp}} \vv$ provides a measure of how well $\proj_W \vv$ approximates the vector $\vv$. 
\item The orthogonal complement of a subset $S$ of an inner product space $V$ is the set
\[S^{\perp} = \{\vv \in V : \langle \vv, \vu \rangle = 0 \text{ for all } \vu \in S\}.\]
\end{itemize} 


\csection{Exercises}
\be

\item \label{ex:6_c_Cab} Let $C[a,b]$ be the set of all continuous real valued functions on the interval $[a,b]$. If $f$ is in $C[a,b]$, we can extend $f$ to a continuous function from $\R$ to $\R$ by letting $F$ be the function defined by 
\[F(x) = \begin{cases} f(a) &\text{if } x < a \\ f(x) &\text{if } a \leq x \leq b \\ f(b) & \text{if } b < x \end{cases}.\]
In this way we can view $C[a,b]$ as a subset of $\F$, the vector space of all functions from $\R$ to $\R$. Verify that $C[a,b]$ is a vector space. 


\item \label{ex:6_c_inner_products} Use the definition of an inner product to determine which of the following defines an inner product on the indicated space. Verify your answers.
    \ba

    \item $\langle \vu, \vv \rangle = u_1v_1 - u_2v_1 - u_1v_2 + 3u_2v_2$ for $\vu = [u_1 \ u_2]^{\tr}$ and $\vv = [v_1 \ v_2]^{\tr}$ in $\R^2$

    \item $\langle f, g \rangle = \int_a^b f(x)g(x) \ dx$ for $f,g \in C[a,b]$ (where $C[a,b]$ is the vector space of all continuous functions on the interval $[a,b]$)

	
	\item $\langle f, g \rangle = f'(0)g'(0)$ for $f,g \in D(-1,1)$ (where $D(a,b)$ is the vector space of all differentiable functions on the interval $(a,b)$)
	
	
	
	\item $\langle \vu, \vv \rangle = (A\vu) \cdot (A\vv)$ for $\vu, \vv \in \R^n$ and $A$ an invertible $n \times n$ matrix
	
	
	

    \ea


\item We can sometimes visualize an inner product in $\R^2$ or $\R^3$ (or other spaces) by describing the unit circle $S^1$, where
\[S^1 = \{ \vv \in V : || \vv || = 1\}\]
in that inner product space. For example, in the inner product space $\R^2$ with the dot product as inner product, the unit circle is just our standard unit circle. Inner products, however, can distort this familiar picture of the unit circle.  Describe the points on the unit circle $S^1$ in the inner product space $\R^2$ with inner product $\langle [u_1 \ u_2], [v_1 \ v_2] \rangle = 2u_1v_1 + 3u_2v_2$ using the following steps.
	\ba
	\item Let $\vx = [x \ y] \in \R^2$. Set up an equation in $x$ and $y$ that is equivalent to the vector equation $|| \vx || = 1$.
	
	
	
	\item Describe the graph of the equation you found in $\R^2$. It should have a familiar form. Draw a picture to illustrate. What do you think of calling this graph a ``circle"?
	
	
	
	\ea
	

\item \label{ex:6_c_ip_1} Define $\langle \ ,  \ \rangle$ on $\R^2$ by $\langle [u_1 \ u_2]^\tr , [v_1 \ v_2]^\tr \rangle = 4u_1v_1+2u_2v_2$.	
	\ba
	\item Show that $\langle \ ,  \ \rangle$ is an inner product.
	\item The inner product $\langle \ ,  \ \rangle$ can be represented as a matrix transformation $\langle \vu, \vv \rangle = \vu^{\tr} A \vv$, where $\vu$ and $\vv$ are written as column vectors. Find a matrix $A$ that represents this inner product. 
	\ea

\item \label{ex:6_c_ip_2} This exercise is a generalization of Exercise \ref{ex:6_c_ip_1}. Define $\langle \ ,  \ \rangle$ on $\R^n$ by 
\[\langle [u_1 \ u_2 \ \cdots \ u_n]^{\tr},  [v_1 \ v_2 \ \cdots \ v_n]^{\tr} \rangle = a_1u_1v_1+a_2u_2v_2+ \cdots + a_nu_nv_n\]
for some positive scalars $a_1$, $a_2$, $\ldots$, $a_n$. 
	\ba
	\item Show that $\langle \ ,  \ \rangle$ is an inner product.
	\item The inner product $\langle \ ,  \ \rangle$ can be represented as a matrix transformation $\langle \vu, \vv \rangle = \vu^{\tr} A \vv$, where $\vu$ and $\vv$ are written as column vectors. Find a matrix $A$ that represents this inner product. 
	\ea

\item Is the sum of two inner products on an inner product space $V$ an inner product on $V$? If yes, prove it. If no, provide a counterexample. (By the sum of inner products we mean a function $\langle \ , \ \rangle$ satisfying 
	\[\langle \vu, \vv \rangle = \langle \vu, \vv \rangle_1 + \langle \vu, \vv \rangle_2\]
for all $\vu$ and $\vv$ in $V$, where $\langle \ , \ \rangle_1$ and $\langle \ , \ \rangle_2$ are inner products on $V$.) 

\item 
	\ba
	\item Does $\langle \vu, \vv \rangle = \vu^{\tr}A\vv$ define an inner product on $\R^n$ for every $n \times n$ matrix $A$? Verify your answer.  
	\item If your answer to part (a) is no, are there any types of matrices for which $\langle \vu, \vv \rangle = \vu^{\tr}A\vv$ defines an inner product? (Hint: See Exercises \ref{ex:6_c_ip_1} and \ref{ex:6_c_ip_2}.)
	\ea

\item \label{ex:6_c_trace} The trace of an $n \times n$ matrix $A = [a_{ij}]$ has some useful properties. 
	\ba
	\item Show that $\trace(A+B) = \trace(A) + \trace(B)$ for any $n \times n$ matrices $A$ and $B$.
	\item Show that $\trace(cA) = c \trace(A)$ for any $n \times n$ matrix $A$ and any scalar $c$. 
	\item Show that $\trace\left(A^{\tr}\right) = \trace(A)$ for any $n \times n$ matrix.
	\ea

\item Let $V$ be an inner product space and $\vu, \vv$ be two vectors in $V$. 
\ba 
\item Check that if $\vv=\vzero$, the Cauchy-Schwarz inequality 
\[ |\langle \vu, \vv \rangle | \leq ||\vu || ||\vv || \]
holds.

\item Assume $\vv \neq \vzero$. Let $\lambda= \langle \vu, \vv \rangle / ||\vv||^2 $ and $\vw=\vu - \lambda \vv$. Use the fact that $||\vw||^2 \geq 0$ to conclude the Cauchy-Schwarz inequality in this case.
\ea

\item \label{ex:6_c_Frobenius} The \emph{Frobenius inner product} is defined as  
	\[\langle A, B \rangle = \trace\left(AB^{\tr}\right).\]
for $n \times n$ matrices $A$ and $B$. Verify that $\langle A, B \rangle$ defines an inner product on $\M_{n \times n}$. 

	
\item Let $A = [a_{ij}]$ and $B = [b_{ij}]$ be two $n \times n$ matrices. 
	\ba
	\item Show that if $n=2$, then the Frobenius inner product (see Exercise \ref{ex:6_c_Frobenius}) of $A$ and $B$ is 
	\[\langle A, B \rangle = a_{11}b_{11} + a_{12}b_{12} + a_{21}b_{21} + a_{22}b_{22}.\]
	
	\item Extend part (a) to the general case. That is, show that for an arbitrary $n$, 
	\[\langle A, B \rangle = \sum_{i=1}^n \sum_{j=1}^n a_{ij}b_{ij}.\]
	
	\item Compare the Frobenius inner product to the scalar product of two vectors. 
	
	\ea

	
\item  Let $\CB = \{[1 \ 1 \ 1]^{\tr}, [1 \ -1 \ 0]^{\tr}\}$ and let $W = \Span \ \CB$ in $\R^3$. 
   \ba
    \item Show that $\CB$ is an orthogonal basis for $W$, using the dot product as inner product. 

    \item Explain why the vector $\vv = [0 \ 2 \ 2]^{\tr}$ is not in $W$.
    
    \item Find the vector in $W$ that is closest to $\vv$. How close is this vector to $\vv$?
    
    \ea 
    
\item  Let $\R^3$ be the inner product space with inner product
\[\langle [u_1 \ u_2 \ u_3]^{\tr}, [v_1 \ v_2 \ v_3]^{\tr}\rangle = u_1v_1 + 2u_2v_2 + u_3v_3.\]
Let $\CB = \{[1 \ 1 \ 1]^{\tr}, [1 \ -1 \ 1]^{\tr}\}$ and let $W = \Span \ \CB$ in $\R^3$. 
   \ba
    \item Show that $\CB$ is an orthogonal basis for $W$, using the given inner product. 
    
    \item Explain why the vector $\vv = [0 \ 2 \ 2]^{\tr}$ is not in $W$.
    
    \item Find the vector in $W$ that is closest to $\vv$. How close is this vector to $\vv$?
    
    \ea 
    
\item  Let $\pol_2$ be the inner product space with inner product
\[\langle p(t), q(t) \rangle = \int_0^1 p(t)q(t) \ dt.\]

Let $\CB = \{1, 1-2t\}$ and let $W = \Span \ \CB$ in $\pol_2$. 
   \ba
    \item Show that $\CB$ is an orthogonal basis for $W$, using the given inner product. 
    
    \item Explain why the polynomial $q(t) = t^2$ is not in $W$.
    
    \item Find the vector in $W$ that is closest to $q(t)$. How close is this vector to $q(t)$?
    
    \ea 
    
\item Prove the remaining properties of Theorem \ref{thm:6_c_inner_product_properties}. That is, if $\langle \ , \ \rangle$ is an inner product on a vector space $V$ and $\vu, \vv$, and $\vw$ are vectors in $V$ and $c$ is any scalar, then
	\be
	\item $\langle \vzero , \vv \rangle = \langle \vv , \vzero \rangle = 0$
	\item $\langle \vu , c\vv \rangle = c\langle \vu , \vv \rangle$
	\item $\langle \vv+\vw , \vu \rangle = \langle \vv , \vu \rangle + \langle \vw , \vu \rangle$
\item $\langle \vu - \vv, \vw \rangle = \langle \vw, \vu - \vv \rangle= \langle \vu , \vw \rangle - \langle \vv , \vw \rangle= \langle \vw, \vu \rangle - \langle \vw, \vv \rangle$
	\ee


\item \label{ex:6_c_orth_complement_basis} Prove the following theorem referenced in Activity \ref{act:6_c_orth_projection}. (Hint: Refer to Theorem \ref{thm:6_a_orth_complement_basis}.)

\begin{theorem} \label{thm:6_a_orth_complement_basis} Let $\CB = \{\vw_1, \vw_2, \ldots, \vw_m\}$ be a basis for a subspace $W$ of an inner product space $V$. A vector $\vv$ in $V$ is orthogonal to every vector in $W$ if and only if $\vv$ is orthogonal to every vector in $\CB$.
\end{theorem}


\item Prove Theorem \ref{thm:6_c_Orth_li_ips}. (Hint: Compare to Theorem \ref{thm:6_b_Orth_li}.)

\item \label{ex_6_c_all_ips} Let $V$ be a vector space with basis $\{\vv_1, \vv_2, \ldots, \vv_n\}$. Define $\langle \ , \ \rangle$ as follows:
\[\langle \vu, \vw \rangle = \sum_{i=1}^n u_iw_i\]
if $\vu = \sum_{i=1}^n u_i \vv_i$ and $\vw = \sum_{i=1}^n w_i \vv_i$ in $V$. (Since the representation of a vector as a linear combination of basis elements is unique, this mapping is well-defined.) Show that $\langle \ , \ \rangle$ is an inner product on $V$ and conclude that any finite dimensional vector space can be made into an inner product space. 

\item Label each of the following statements as True or False. Provide justification for your response.
	\ba
	\item \textbf{True/False} An inner product on a vector space $V$ is a function from $V$ to the real numbers. 
	\item \textbf{True/False} If $\langle \ , \ \rangle$ is an inner product on a vector space $V$, and if $\vv$ is a vector in $V$, then the set $W = \{\vx \in V : \langle \vx,\vv \rangle = 0\}$ is a subspace of $V$.  
	\item \textbf{True/False} There is exactly one inner product on each inner product space.
	\item \textbf{True/False} If $\vx$, $\vy$, and $\vz$ are vectors in an inner product space with $\langle \vx, \vz \rangle = \langle \vy, \vz \rangle$, then $\vx = \vy$. 
	\item \textbf{True/False} If $\langle \vx, \vy \rangle = 0$ for all vectors $\vy$ in an inner product space $V$, then $\vx = \vzero$. 
	\item \textbf{True/False} If $\vu$ and $\vv$ are vectors in an inner product space and the distance from $\vu$ to $\vv$ is the same as the distance from $\vu$ to $-\vv$, then $\vu$ and $\vv$ are orthogonal.
	\item \textbf{True/False} If $W$ is a subspace of an inner product space and a vector $\vv$ is orthogonal to every vector in a basis of $W$, then $\vv$ is in $W^{\perp}$.
	\item \textbf{True/False} If $\{\vv_1, \vv_2, \vv_3\}$ is an orthogonal basis for an inner product space $V$, then so is $\{c\vv_1, \vv_2, \vv_3\}$ for any nonzero scalar $c$. 
	\item \textbf{True/False} An inner product $\langle \vu, \vv \rangle$ in an inner product space $V$ results in another vector in $V$.
	\item \textbf{True/False} An inner product in an inner product space $V$ is a function that maps pairs of vectors in $V$ to the set of non-negative real numbers. 
	\item \textbf{True/False} The vector space of all $n \times n$ matrices can be made into an inner product space.  
	\item \textbf{True/False} Any non-zero multiple of an inner product on space $V$ is also an inner product on $V$.
  \item \textbf{True/False} Every set of $k$ non-zero orthogonal vectors in a vector space $V$ of dimension $k$ is a basis for $V$. 
	\item \textbf{True/False} For any finite-dimensional inner product space $V$ and a subspace $W$ of $V$, $W$ is a subspace of $(W^\perp)^\perp$.
	\item \textbf{True/False} If $W$ is a subspace of an inner product space, then $W\cap W^\perp = \{\vzero\}$.
	\ea    
	  

\ee

\csection{Project: Fourier Series and Musical Tones}

Joseph Fourier first studied trigonometric polynomials to understand the flow of heat in metallic plates and rods. The resulting series, called Fourier series, now have applications in a variety of areas including  electrical engineering, vibration  analysis, acoustics, optics, signal processing, image processing, geology, quantum mechanics, and many more. For our purposes, we will focus on synthesized music.

Pure musical tones are periodic sine waves. Simple electronic circuits can be designed to generate alternating current. Alternating current is current that is periodic, and hence is described by a combination of $\sin(kx)$ and $\cos(kx)$ for integer values of $k$. To synthesize an instrument like a violin, we can project the instrument's tones onto trigonometric polynomials -- and then we can produce them electronically. As we will see, these projections are least squares approximations onto certain vector spaces.
%http://www.falstad.com/fourier/
%http://www.phy.ntnu.edu.tw/ntnujava/index.php?topic=17 (better, but can't load the website)
The website \url{http://www.falstad.com/fourier/} provides a tool for hearing sounds digitally created by certain functions. For example, you can listen to the sound generated by a sawtooth function $f$ of the form
\[f(x) = \begin{cases}
x &\text{ if } -\pi < x \leq \pi, \\
f(x-2\pi), 	& \text{ if } \pi < x, \\	
f(x+2\pi), 	& \text{ if } x \leq -\pi.
\end{cases}\]
Try out some of the tones on this website (click on the Sound button to hear the tones). You can also alter the tones by clicking on any one of the white dots and moving it up or down.  and play with the buttons.  We will learn much about what this website does in this project.
%\url{http://www.falstad.com/fourier/}

Pure tones are periodic and so are modeled by trigonometric functions. In general, trigonometric polynomials can be used to produce good approximations to periodic phenomena. A \emph{trigonometric polynomial}\index{trigonometric polynomial} is an object of the form
\begin{align*}
c_0 + c_1 \cos(x) + d_1\sin(x) &+ c_2\cos(2x) + d_2\sin(2x) + \cdots \\
	&+ c_n \cos(nx) + d_n \sin(nx) + \cdots,
\end{align*}
where the $c_i$ and $d_j$ are real constants. With judicious choices of these constants, we can approximate periodic and other behavior with trigonometric polynomials. The first step for us will be to understand the relationships between the summands of these trigonometric polynomials in the inner product space $C[-\pi, \pi]$\footnote{With suitable adjustments, we can work over any interval that is convenient, but for the sake of simplicity in this project, we will restrict ourselves to the interval $[-\pi, \pi]$.} of continuous functions from $[\pi, \pi]$ to $\R$ with the inner product
\begin{equation}
\langle f,g \rangle = \frac{1}{\pi} \int_{-\pi}^{\pi} f(x)g(x) \ dx. \label{eq:ip}
\end{equation}

Our first order of business is to verify that (\ref{eq:ip}) is, in fact, an inner product. 

\begin{pactivity} \label{act:Fourier_inner_product} Let $C[a,b]$ be the set of continuous real-valued functions on the interval $[a,b]$. In Exercise \ref{ex:6_c_Cab} in Section \ref{sec:inner_products} we are asked to show that $C[a,b]$ is a vector space, while Exercise \ref{ex:6_c_inner_products} in Section \ref{sec:inner_products} asks us to show that $\langle f,g \rangle = \int_a^b f(x)g(x) \, dx$ defines an inner product on $C[a,b]$. However, \ref{eq:ip} is slightly different than this inner product. Show that any positive scalar multiple of an inner product is an inner product, and conclude that (\ref{eq:ip}) defines an inner product on $C[-\pi,\pi]$. (We will see why we introduce the factor of $\frac{1}{\pi}$ later.)


\end{pactivity}

Now we return to our inner product space $C[-\pi,\pi]$ with inner product (\ref{eq:ip}). Given a function $g$ in $C[-\pi,\pi]$, we approximate $g$ using only a finite number of the terms in a trigonometric polynomial. Let $W_n$ be the subspace of $C[-\pi,\pi]$ spanned by the functions
\[1, \cos(x), \sin(x), \cos(2x), \sin(2x), \ldots, \cos(nx), \sin(nx).\]
One thing we need to know is the dimension of $W_n$.

\label{ex:W1}

\begin{pactivity} \label{act:Fourier_W1} We start with the initial case of $W_1$.
	\ba
	\item Show directly that the functions $1$, $\cos(x)$, and $\sin(x)$ are orthogonal.


	\item What is the dimension of $W_1$? Explain.
	
	\ea

\end{pactivity}

Now we need to see if what happened in Project Activity \ref{act:Fourier_W1}  happens in general. A few tables of integrals and some basic facts from trigonometry can help.

\begin{pactivity} \label{act:Fourier_Wn} A table of integrals shows the following for $k \neq m$ (up to a constant):
\begin{align}
\int \cos(mx)\cos(kx) \ dx &= \frac{1}{2} \left( \frac{\sin((k-m)x)}{k-m} + \frac{\sin((k+m)x)}{k+m} \right)  \label{eq:cos_cos_int} \\
\int \sin(mx)\sin(kx) \ dx &= \frac{1}{2} \left( \frac{\sin((k-m)x)}{k-m} - \frac{\sin((k+m)x)}{k+m} \right)   \label{eq:sin_sin_int} \\
\int \cos(mx)\sin(kx) \ dx &= \frac{1}{2} \left( \frac{\cos((m-k)x)}{m-k} - \frac{\sin((m+k)x)}{m+k} \right)  \label{eq:cos_sin_int1} \\
\int \cos(mx)\sin(mx) \ dx &= -\frac{1}{2m} \cos^2(mx)  \label{eq:cos_sin_int2}
\end{align}
	\ba
	\item Use (\ref{eq:cos_cos_int}) to show that $\cos(mx)$ and $\cos(kx)$ are orthogonal in $C[-\pi,\pi]$ if $k \neq m$. 


	\item Use (\ref{eq:sin_sin_int}) to show that $\sin(mx)$ and $\sin(kx)$ are orthogonal in $C[-\pi,\pi]$ if $k \neq m$.

	
	\item Use (\ref{eq:cos_sin_int1}) to show that $\cos(mx)$ and $\sin(kx)$ are orthogonal in $C[-\pi,\pi]$ if $k \neq m$.

	
	\item Use (\ref{eq:cos_sin_int2}) to show that $\cos(mx)$ and $\sin(mx)$ are orthogonal in $C[-\pi,\pi]$.

	
	\item What is $\dim(W_n)$? Explain.
	
	\ea
\end{pactivity}

Once we have an orthogonal basis for $W_n$, we might want to create an orthonormal basis for $W_n$. Throughout the remainder of this project, unless otherwise specified, you should use a table of integrals or any appropriate technological tool to find integrals for any functions you need. 

\begin{pactivity} \label{act:Fourier_basis} Show that the set
\[\CB_n = \left\{\frac{1}{\sqrt{2}}, \cos(x), \cos(2x), \ldots, \cos(nx), \sin(x), \sin(2x), \ldots, \sin(nx)\right\}\]
is an orthonormal basis for $W_n$. Use the fact that the norm of a vector $\vv$ in an inner product space with inner product $\langle \ , \ \rangle$ is defined to be $\sqrt{\langle \vv, \vv \rangle}$. (This is where the factor of $\frac{1}{\pi}$ will be helpful.)

\end{pactivity}

Now we need to recall how to find the best approximation to a vector by a vector in a subspace, and apply that idea to approximate an arbitrary function $g$ with a trigonometric polynomial in $W_n$. Recall that the best approximation of a function $g$ in $C[-\pi,\pi]$ is the projection of $g$ onto $W_n$. If we have an orthonormal basis $\{h_0, h_1, h_2, \ldots, h_{2n}\}$ of $W_n$, then the projection of $g$ onto $W_n$ is
\[\proj_{W_n} g = \langle g,h_0 \rangle h_0 + \langle g,h_1 \rangle h_1 + \langle g,h_2 \rangle h_2 + \ldots + \langle g,h_{2n} \rangle h_{2n}.\]
With this idea, we can find formulas for the coefficients when we project an arbitrary function onto $W_n$.


\begin{pactivity} \label{act:Fourier_projection} If $g$ is an arbitrary function in $C[-\pi,\pi]$, we will write the projection of $g$ onto $W_n$ as
\begin{align*}
a_0\left(\frac{1}{\sqrt{2}}\right) + a_1 \cos(x) + b_1\sin(x) + a_2 \cos(2x) &+ b_2 \sin(2x) + \cdots \\
	&+ a_n \cos(nx) + b_n \sin(nx).
\end{align*}
The $a_i$ and $b_j$ are the \emph{Fourier coefficients}\index{Fourier coefficients} for $f$. The expression $a_n\cos(nx)+b_n\sin(nx)$ is called the $n$th \emph{harmonic} of $g$. The first harmonic is called the \emph{fundamental frequency}. The human ear cannot hear tones whose frequencies exceed 20000 Hz, so we only hear finitely many harmonics (the projections onto $W_n$ for some $n$).
	\ba
	\item Show that
\begin{equation}
a_0 =  \frac{1}{\sqrt{2}\pi}\int_{-\pi}^{\pi} g(x) \ dx. \label{eq:a0}
\end{equation}
Explain why $\frac{a_0}{\sqrt{2}}$ gives the average value of $g$ on $[-\pi,\pi]$. You may want to go back and review average value from calculus. This is saying that the best constant approximation of $g$ on $[-\pi,\pi]$ is its average value, which makes sense.

	
	\item Show that for $m \geq 1$,
\begin{equation}
a_m =  \frac{1}{\pi}\int_{-\pi}^{\pi} g(x) \cos(mx) \ dx. \label{eq:am}
\end{equation}


	\item Show that for $m \geq 1$,
\begin{equation}
b_m =  \frac{1}{\pi}\int_{-\pi}^{\pi} g(x) \sin(mx) \ dx. \label{eq:bm}
\end{equation}


	\ea
\end{pactivity}

Let us return to the sawtooth function defined earlier and find its Fourier coefficients.

\begin{pactivity} \label{act:Fourier_sawtooth} Let $f$ be defined by $f(x) = x$ on $[-\pi,\pi]$ and repeated periodically afterwards with period $2\pi$. Let $p_n$ be the projection of $f$ onto $W_n$.
	\ba
	\item Evaluate the integrals to find the projection $p_1$.

	\item  Use appropriate technology to find the projections $p_{10}$, $p_{20}$, and $p_{30}$ for the sawtooth function $f$. Draw pictures of these approximations against $f$ and explain what you see.  	
	
	\item Now we find formulas for all the Fourier coefficients. Use the fact that $x \cos(mx)$ is an odd function to explain why $a_m = 0$ for each $m$. Then show that $b_m = (-1)^{m+1}\frac{2}{m}$ for each $m$.

		
	\item Go back to the website \url{http://www.falstad.com/fourier/} and replay the sawtooth tone. Explain what the white buttons represent. 

	
	\ea
	
\end{pactivity}



\begin{pactivity} \label{act:Fourier_square_sums} This activity is not connected to the idea of musical tones, so can be safely ignored if so desired. We conclude with a derivation of a very fascinating formula that you may have seen for $\ds \sum_{n=1}^{\infty} \frac{1}{n^2}$. To do so, we need to analyze the error in approximating a function $g$ with a function in $W_n$.

Let $p_n$ be the projection of $g$ onto $W_n$. Notice that $p_n$ is also in $W_{n+1}$. It is beyond the scope of this project, but in ``nice" situations we have $||g - p_n|| \to 0$ as $n \to \infty$. Now $g - p_n$ is orthogonal to $p_n$, so the Pythagorean theorem shows that
\[||g-p_n||^2 + ||p_n||^2 = ||g||^2.\]
Since $||g-p_n||^2 \to 0$ as $n \to \infty$, we can conclude that
\begin{equation}
\lim_{n \to \infty} ||p_n||^2 = ||g||^2. \label{eq:Fourier_norm}
\end{equation}
We use these ideas to derive a formula for $\ds \sum_{n=1}^{\infty} \frac{1}{n^2}$.
	\ba
	\item Use the fact that $\CB_n$ is an orthonormal basis to show that
\[||p_n||^2 = a_0^2 +a_1^2 + b_1^2 + \cdots + a_n^2 + b_n^2.\]
Conclude that
\begin{equation}
||g||^2 = a_0^2 +a_1^2 + b_1^2 + \cdots + a_n^2 + b_n^2 + \cdots . \label{eq:norms}
\end{equation}


	\item For the remainder of this activity, let $f$ be the sawtooth function defined by $f(x) = x$ on $[-\pi,\pi]$ and repeated periodically afterwards. We determined the Fourier coefficients $a_i$ and $b_j$ of this function in Project Activity \ref{act:Fourier_sawtooth}.
		\begin{enumerate}
		\item[i.] Show that
\[a_0^2 +a_1^2 + b_1^2 + \cdots + a_n^2 + b_n^2 + \cdots = 4\sum_{n=1}^{\infty} \frac{1}{n^2}.\]

		\item[ii.] Calculate $||f||^2$ using the inner product and compare to (\ref{eq:norms}) to find a surprising formula for $\sum_{n=1}^{\infty} \frac{1}{n^2}$.

		
		\end{enumerate}
			
	\ea
\end{pactivity} 	


